[2024-08-01 09:10:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/config.json
[2024-08-01 09:10:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_stage1
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_sequence_stage1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-01 09:10:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_sequence_stage1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_sequence_stage1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-01 09:11:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1
[2024-08-01 09:11:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-01 09:11:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 113): INFO number of params: 901480
[2024-08-01 09:11:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1, ignoring auto resume
[2024-08-01 09:11:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth for fine-tuning......
[2024-08-01 09:11:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-01 09:11:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth'
[2024-08-01 09:11:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 14.762 (14.762)	Loss 0.5176 (0.5176)	Acc@1 93.555 (93.555)	Acc@5 98.242 (98.242)	Mem 2322MB
[2024-08-01 09:11:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.656 Acc@5 97.668
[2024-08-01 09:11:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 09:11:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 168): INFO Start training
[2024-08-01 09:11:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:39:56 lr 0.000000	 wd 0.0500	time 12.4686 (12.4686)	loss 1.5409 (1.5409)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 16679MB
[2024-08-01 09:12:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:02 lr 0.000000	 wd 0.0500	time 0.3065 (0.4256)	loss 1.2990 (1.2208)	grad_norm 0.3522 (nan)	loss_scale 16384.0000 (27901.4653)	mem 16679MB
[2024-08-01 09:12:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:13:57 lr 0.000000	 wd 0.0500	time 0.3076 (0.3637)	loss 1.0378 (1.2060)	grad_norm 0.3409 (nan)	loss_scale 16384.0000 (22171.3831)	mem 16679MB
[2024-08-01 09:13:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:12:40 lr 0.000000	 wd 0.0500	time 0.3135 (0.3455)	loss 0.9112 (1.1685)	grad_norm 0.3575 (nan)	loss_scale 16384.0000 (20248.6645)	mem 16679MB
[2024-08-01 09:13:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:11:46 lr 0.000001	 wd 0.0500	time 0.3359 (0.3363)	loss 0.9785 (1.1757)	grad_norm 0.3506 (nan)	loss_scale 8192.0000 (17405.4464)	mem 16679MB
[2024-08-01 09:14:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:10:59 lr 0.000001	 wd 0.0500	time 0.2883 (0.3295)	loss 1.1533 (1.1805)	grad_norm 0.3220 (nan)	loss_scale 8192.0000 (15566.4351)	mem 16679MB
[2024-08-01 09:14:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:10:19 lr 0.000001	 wd 0.0500	time 0.2968 (0.3255)	loss 1.3208 (1.1805)	grad_norm 0.3328 (nan)	loss_scale 8192.0000 (14339.4077)	mem 16679MB
[2024-08-01 09:15:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:09:44 lr 0.000001	 wd 0.0500	time 0.2889 (0.3245)	loss 1.2536 (1.1826)	grad_norm 0.3545 (nan)	loss_scale 8192.0000 (13462.4593)	mem 16679MB
[2024-08-01 09:15:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:08 lr 0.000001	 wd 0.0500	time 0.2883 (0.3221)	loss 1.0719 (1.1822)	grad_norm 0.3277 (nan)	loss_scale 4096.0000 (12559.0212)	mem 16679MB
[2024-08-01 09:16:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:08:32 lr 0.000001	 wd 0.0500	time 0.3299 (0.3201)	loss 1.3704 (1.1819)	grad_norm 0.3434 (nan)	loss_scale 4096.0000 (11619.7292)	mem 16679MB
[2024-08-01 09:16:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:07:58 lr 0.000002	 wd 0.0500	time 0.3041 (0.3186)	loss 1.4620 (1.1796)	grad_norm 0.3469 (nan)	loss_scale 4096.0000 (10868.1079)	mem 16679MB
[2024-08-01 09:17:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:26 lr 0.000002	 wd 0.0500	time 0.2847 (0.3183)	loss 1.2518 (1.1807)	grad_norm 0.3509 (nan)	loss_scale 4096.0000 (10253.0209)	mem 16679MB
[2024-08-01 09:18:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:06:53 lr 0.000002	 wd 0.0500	time 0.2866 (0.3174)	loss 0.9383 (1.1804)	grad_norm 0.3377 (nan)	loss_scale 4096.0000 (9740.3630)	mem 16679MB
[2024-08-01 09:18:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:20 lr 0.000002	 wd 0.0500	time 0.3022 (0.3166)	loss 1.2401 (1.1821)	grad_norm 0.3409 (nan)	loss_scale 4096.0000 (9306.5150)	mem 16679MB
[2024-08-01 09:19:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:05:48 lr 0.000002	 wd 0.0500	time 0.3237 (0.3159)	loss 1.4581 (1.1839)	grad_norm 0.3376 (nan)	loss_scale 4096.0000 (8934.6010)	mem 16679MB
[2024-08-01 09:19:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:16 lr 0.000002	 wd 0.0500	time 0.2949 (0.3161)	loss 1.1851 (1.1879)	grad_norm 0.3602 (nan)	loss_scale 4096.0000 (8612.2425)	mem 16679MB
[2024-08-01 09:20:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:04:44 lr 0.000003	 wd 0.0500	time 0.3084 (0.3157)	loss 1.2202 (1.1850)	grad_norm 0.3351 (nan)	loss_scale 4096.0000 (8330.1537)	mem 16679MB
[2024-08-01 09:20:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:12 lr 0.000003	 wd 0.0500	time 0.2930 (0.3151)	loss 0.9011 (1.1844)	grad_norm 0.3814 (nan)	loss_scale 4096.0000 (8081.2322)	mem 16679MB
[2024-08-01 09:21:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:40 lr 0.000003	 wd 0.0500	time 0.2917 (0.3148)	loss 1.1383 (1.1846)	grad_norm 0.3340 (nan)	loss_scale 4096.0000 (7859.9534)	mem 16679MB
[2024-08-01 09:21:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:09 lr 0.000003	 wd 0.0500	time 0.2907 (0.3148)	loss 1.4196 (1.1842)	grad_norm 0.3376 (nan)	loss_scale 4096.0000 (7661.9548)	mem 16679MB
[2024-08-01 09:22:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:37 lr 0.000003	 wd 0.0500	time 0.2965 (0.3144)	loss 0.8582 (1.1832)	grad_norm 0.3390 (nan)	loss_scale 4096.0000 (7483.7461)	mem 16679MB
[2024-08-01 09:22:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:06 lr 0.000003	 wd 0.0500	time 0.3125 (0.3141)	loss 1.0795 (1.1820)	grad_norm 0.3456 (nan)	loss_scale 4096.0000 (7322.5017)	mem 16679MB
[2024-08-01 09:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:34 lr 0.000004	 wd 0.0500	time 0.3630 (0.3139)	loss 1.5384 (1.1816)	grad_norm 0.3414 (nan)	loss_scale 4096.0000 (7175.9091)	mem 16679MB
[2024-08-01 09:23:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:03 lr 0.000004	 wd 0.0500	time 0.3159 (0.3138)	loss 1.3541 (1.1818)	grad_norm 0.3598 (nan)	loss_scale 4096.0000 (7042.0582)	mem 16679MB
[2024-08-01 09:24:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:31 lr 0.000004	 wd 0.0500	time 0.3149 (0.3136)	loss 1.3270 (1.1817)	grad_norm 0.3369 (nan)	loss_scale 4096.0000 (6919.3569)	mem 16679MB
[2024-08-01 09:24:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2828 (0.3128)	loss 0.8931 (1.1798)	grad_norm 0.3413 (nan)	loss_scale 4096.0000 (6806.4678)	mem 16679MB
[2024-08-01 09:24:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 0 training takes 0:13:05
[2024-08-01 09:24:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_0.pth saving......
[2024-08-01 09:24:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_0.pth saved !!!
[2024-08-01 09:24:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 10.214 (10.214)	Loss 0.5366 (0.5366)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 09:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.652 Acc@5 97.660
[2024-08-01 09:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 09:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.65%
[2024-08-01 09:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 09:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 09:25:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:37:55 lr 0.000004	 wd 0.0500	time 10.9816 (10.9816)	loss 1.1244 (1.1244)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:25:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:16:48 lr 0.000004	 wd 0.0500	time 0.3019 (0.4199)	loss 1.1955 (1.1629)	grad_norm 0.3466 (0.3529)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:26:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:13:53 lr 0.000004	 wd 0.0500	time 0.3080 (0.3623)	loss 0.7777 (1.1795)	grad_norm 0.3471 (0.3615)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:26:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:12:34 lr 0.000004	 wd 0.0500	time 0.3008 (0.3429)	loss 0.8916 (1.1791)	grad_norm 0.3439 (0.3577)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:27:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:11:40 lr 0.000005	 wd 0.0500	time 0.3166 (0.3332)	loss 0.9924 (1.1942)	grad_norm 0.3434 (0.3556)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:27:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:10:56 lr 0.000005	 wd 0.0500	time 0.3135 (0.3280)	loss 1.4692 (1.1913)	grad_norm 0.3357 (0.3545)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:28:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:10:16 lr 0.000005	 wd 0.0500	time 0.2887 (0.3240)	loss 1.5106 (1.1902)	grad_norm 0.3337 (0.3532)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:28:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:09:38 lr 0.000005	 wd 0.0500	time 0.3005 (0.3213)	loss 0.8849 (1.1868)	grad_norm 0.3971 (0.3525)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:29:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:03 lr 0.000005	 wd 0.0500	time 0.3595 (0.3196)	loss 1.4469 (1.1860)	grad_norm 0.6803 (0.3528)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:30:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:08:31 lr 0.000005	 wd 0.0500	time 0.3120 (0.3190)	loss 1.1916 (1.1850)	grad_norm 0.3661 (0.3531)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:30:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:07:57 lr 0.000006	 wd 0.0500	time 0.3071 (0.3182)	loss 1.3787 (1.1837)	grad_norm 0.3623 (0.3526)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:31:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:24 lr 0.000006	 wd 0.0500	time 0.2922 (0.3171)	loss 1.4365 (1.1822)	grad_norm 0.3335 (0.3539)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:31:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:06:51 lr 0.000006	 wd 0.0500	time 0.3256 (0.3162)	loss 0.9684 (1.1799)	grad_norm 0.3416 (0.3537)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:32:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:19 lr 0.000006	 wd 0.0500	time 0.2980 (0.3156)	loss 1.5459 (1.1809)	grad_norm 0.3550 (0.3533)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:32:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:05:47 lr 0.000006	 wd 0.0500	time 0.3214 (0.3150)	loss 0.9636 (1.1809)	grad_norm 0.3638 (0.3528)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:33:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:15 lr 0.000006	 wd 0.0500	time 0.2904 (0.3145)	loss 1.4356 (1.1817)	grad_norm 0.3339 (0.3540)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:33:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:43 lr 0.000007	 wd 0.0500	time 0.3105 (0.3141)	loss 1.3105 (1.1834)	grad_norm 0.3445 (0.3534)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:34:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:11 lr 0.000007	 wd 0.0500	time 0.2878 (0.3139)	loss 1.0028 (1.1837)	grad_norm 0.3690 (0.3537)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:34:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:40 lr 0.000007	 wd 0.0500	time 0.3121 (0.3136)	loss 1.7170 (1.1830)	grad_norm 0.3417 (0.3538)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:35:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:08 lr 0.000007	 wd 0.0500	time 0.2855 (0.3134)	loss 1.4455 (1.1842)	grad_norm 0.3554 (0.3537)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:35:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:37 lr 0.000007	 wd 0.0500	time 0.3060 (0.3130)	loss 1.4205 (1.1847)	grad_norm 0.3688 (0.3533)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:36:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:05 lr 0.000007	 wd 0.0500	time 0.3094 (0.3130)	loss 1.5148 (1.1868)	grad_norm 0.3765 (0.3533)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:36:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.3104 (0.3130)	loss 1.2386 (1.1873)	grad_norm 0.3320 (0.3531)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:37:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.3304 (0.3127)	loss 0.9812 (1.1896)	grad_norm 0.3380 (0.3531)	loss_scale 8192.0000 (4185.0048)	mem 16679MB
[2024-08-01 09:37:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:31 lr 0.000008	 wd 0.0500	time 0.2974 (0.3128)	loss 1.2265 (1.1896)	grad_norm 0.3375 (0.3528)	loss_scale 8192.0000 (4351.8934)	mem 16679MB
[2024-08-01 09:38:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2852 (0.3121)	loss 0.9176 (1.1898)	grad_norm 0.3408 (0.3526)	loss_scale 8192.0000 (4505.4362)	mem 16679MB
[2024-08-01 09:38:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 1 training takes 0:13:03
[2024-08-01 09:38:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.929 (11.929)	Loss 0.5303 (0.5303)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 09:38:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.634 Acc@5 97.682
[2024-08-01 09:38:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-08-01 09:38:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.65%
[2024-08-01 09:38:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:02:35 lr 0.000008	 wd 0.0500	time 11.5728 (11.5728)	loss 1.2552 (1.2552)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 09:39:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:16:45 lr 0.000008	 wd 0.0500	time 0.3057 (0.4186)	loss 1.4404 (1.1941)	grad_norm 0.3406 (0.3552)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 09:39:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:13:50 lr 0.000008	 wd 0.0500	time 0.2973 (0.3608)	loss 1.4204 (1.1848)	grad_norm 0.3319 (0.3497)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 09:40:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:12:31 lr 0.000008	 wd 0.0500	time 0.2983 (0.3413)	loss 1.1919 (1.1749)	grad_norm 0.3530 (nan)	loss_scale 4096.0000 (7429.9535)	mem 16679MB
[2024-08-01 09:40:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:11:38 lr 0.000009	 wd 0.0500	time 0.3177 (0.3321)	loss 1.4388 (1.1779)	grad_norm 0.3284 (nan)	loss_scale 4096.0000 (6598.5436)	mem 16679MB
[2024-08-01 09:41:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:10:54 lr 0.000009	 wd 0.0500	time 0.3086 (0.3270)	loss 1.4438 (1.1734)	grad_norm 0.3781 (nan)	loss_scale 4096.0000 (6099.0339)	mem 16679MB
[2024-08-01 09:41:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:10:15 lr 0.000009	 wd 0.0500	time 0.3491 (0.3234)	loss 1.2923 (1.1744)	grad_norm 0.3529 (nan)	loss_scale 4096.0000 (5765.7504)	mem 16679MB
[2024-08-01 09:42:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:09:38 lr 0.000009	 wd 0.0500	time 0.3204 (0.3208)	loss 1.3352 (1.1794)	grad_norm 0.3592 (nan)	loss_scale 4096.0000 (5527.5549)	mem 16679MB
[2024-08-01 09:42:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:02 lr 0.000009	 wd 0.0500	time 0.3095 (0.3187)	loss 1.4028 (1.1825)	grad_norm 0.3364 (nan)	loss_scale 4096.0000 (5348.8340)	mem 16679MB
[2024-08-01 09:43:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:08:28 lr 0.000009	 wd 0.0500	time 0.3106 (0.3171)	loss 0.9441 (1.1875)	grad_norm 0.3507 (nan)	loss_scale 4096.0000 (5209.7847)	mem 16679MB
[2024-08-01 09:44:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:07:54 lr 0.000010	 wd 0.0500	time 0.3207 (0.3160)	loss 0.8876 (1.1846)	grad_norm 0.3541 (nan)	loss_scale 4096.0000 (5098.5175)	mem 16679MB
[2024-08-01 09:44:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:21 lr 0.000010	 wd 0.0500	time 0.3268 (0.3151)	loss 1.3816 (1.1821)	grad_norm 0.3838 (nan)	loss_scale 4096.0000 (5007.4623)	mem 16679MB
[2024-08-01 09:45:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:06:49 lr 0.000010	 wd 0.0500	time 0.3114 (0.3149)	loss 0.9029 (1.1825)	grad_norm 0.3639 (nan)	loss_scale 4096.0000 (4931.5704)	mem 16679MB
[2024-08-01 09:45:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:17 lr 0.000010	 wd 0.0500	time 0.3132 (0.3143)	loss 1.4196 (1.1835)	grad_norm 0.3527 (nan)	loss_scale 4096.0000 (4867.3451)	mem 16679MB
[2024-08-01 09:46:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:05:45 lr 0.000010	 wd 0.0500	time 0.3322 (0.3138)	loss 1.2356 (1.1852)	grad_norm 0.3388 (nan)	loss_scale 4096.0000 (4812.2884)	mem 16679MB
[2024-08-01 09:46:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:13 lr 0.000010	 wd 0.0500	time 0.2987 (0.3133)	loss 1.3495 (1.1857)	grad_norm 0.3488 (nan)	loss_scale 4096.0000 (4764.5676)	mem 16679MB
[2024-08-01 09:47:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:04:42 lr 0.000011	 wd 0.0500	time 0.3077 (0.3131)	loss 0.9786 (1.1862)	grad_norm 0.3313 (nan)	loss_scale 4096.0000 (4722.8082)	mem 16679MB
[2024-08-01 09:47:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:11 lr 0.000011	 wd 0.0500	time 0.2961 (0.3130)	loss 1.2984 (1.1866)	grad_norm 0.3313 (nan)	loss_scale 4096.0000 (4685.9588)	mem 16679MB
[2024-08-01 09:48:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:39 lr 0.000011	 wd 0.0500	time 0.2956 (0.3128)	loss 1.0780 (1.1873)	grad_norm 0.3433 (nan)	loss_scale 4096.0000 (4653.2016)	mem 16679MB
[2024-08-01 09:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:08 lr 0.000011	 wd 0.0500	time 0.3535 (0.3127)	loss 1.3641 (1.1871)	grad_norm 0.3650 (nan)	loss_scale 4096.0000 (4623.8906)	mem 16679MB
[2024-08-01 09:49:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:37 lr 0.000011	 wd 0.0500	time 0.3120 (0.3135)	loss 1.1433 (1.1872)	grad_norm 0.3389 (nan)	loss_scale 4096.0000 (4597.5092)	mem 16679MB
[2024-08-01 09:49:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:06 lr 0.000011	 wd 0.0500	time 0.3170 (0.3136)	loss 1.0323 (1.1857)	grad_norm 0.3378 (nan)	loss_scale 4096.0000 (4573.6392)	mem 16679MB
[2024-08-01 09:50:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:34 lr 0.000012	 wd 0.0500	time 0.3094 (0.3135)	loss 1.4007 (1.1855)	grad_norm 0.3550 (nan)	loss_scale 4096.0000 (4551.9382)	mem 16679MB
[2024-08-01 09:50:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:03 lr 0.000012	 wd 0.0500	time 0.2949 (0.3134)	loss 1.2333 (1.1856)	grad_norm 0.3399 (nan)	loss_scale 4096.0000 (4532.1234)	mem 16679MB
[2024-08-01 09:51:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:31 lr 0.000012	 wd 0.0500	time 0.3019 (0.3136)	loss 0.8053 (1.1864)	grad_norm 0.3463 (nan)	loss_scale 4096.0000 (4513.9592)	mem 16679MB
[2024-08-01 09:51:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2857 (0.3131)	loss 1.2225 (1.1867)	grad_norm 0.3262 (nan)	loss_scale 4096.0000 (4497.2475)	mem 16679MB
[2024-08-01 09:51:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 2 training takes 0:13:05
[2024-08-01 09:52:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.175 (11.175)	Loss 0.5127 (0.5127)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 09:52:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.664 Acc@5 97.688
[2024-08-01 09:52:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 09:52:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.66%
[2024-08-01 09:52:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 09:52:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 09:52:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:45:51 lr 0.000012	 wd 0.0500	time 11.1715 (11.1715)	loss 0.7570 (0.7570)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:53:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:16:25 lr 0.000012	 wd 0.0500	time 0.2927 (0.4104)	loss 1.2415 (1.2231)	grad_norm 0.3444 (0.3521)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:53:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:13:47 lr 0.000012	 wd 0.0500	time 0.2879 (0.3596)	loss 1.5360 (1.1978)	grad_norm 0.3490 (0.3554)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:12:32 lr 0.000012	 wd 0.0500	time 0.3078 (0.3418)	loss 1.4539 (1.1945)	grad_norm 0.3310 (0.3520)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:54:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:11:40 lr 0.000013	 wd 0.0500	time 0.3040 (0.3334)	loss 1.4323 (1.1867)	grad_norm 0.3460 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:55:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:10:55 lr 0.000013	 wd 0.0500	time 0.3195 (0.3276)	loss 1.0023 (1.1867)	grad_norm 0.3549 (0.3496)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:55:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:10:16 lr 0.000013	 wd 0.0500	time 0.2870 (0.3239)	loss 1.0627 (1.1813)	grad_norm 0.3550 (0.3491)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:56:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:09:39 lr 0.000013	 wd 0.0500	time 0.2930 (0.3216)	loss 1.5320 (1.1788)	grad_norm 0.3410 (0.3497)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:04 lr 0.000013	 wd 0.0500	time 0.2867 (0.3198)	loss 0.7844 (1.1745)	grad_norm 0.3471 (0.3497)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:57:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:08:30 lr 0.000013	 wd 0.0500	time 0.3239 (0.3186)	loss 1.5776 (1.1768)	grad_norm 0.3543 (0.3488)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:57:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:07:56 lr 0.000014	 wd 0.0500	time 0.2911 (0.3173)	loss 1.3074 (1.1775)	grad_norm 0.3209 (0.3492)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:58:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:23 lr 0.000014	 wd 0.0500	time 0.3028 (0.3161)	loss 1.0014 (1.1782)	grad_norm 0.3759 (0.3495)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:58:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:06:51 lr 0.000014	 wd 0.0500	time 0.3416 (0.3157)	loss 1.1656 (1.1778)	grad_norm 0.3408 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:59:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:18 lr 0.000014	 wd 0.0500	time 0.2931 (0.3152)	loss 1.3400 (1.1771)	grad_norm 0.3321 (0.3499)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 09:59:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:05:46 lr 0.000014	 wd 0.0500	time 0.3159 (0.3148)	loss 1.1358 (1.1781)	grad_norm 0.3296 (0.3495)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:00:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:15 lr 0.000014	 wd 0.0500	time 0.3324 (0.3144)	loss 1.4265 (1.1764)	grad_norm 0.3436 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:00:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:43 lr 0.000015	 wd 0.0500	time 0.3049 (0.3142)	loss 0.7999 (1.1749)	grad_norm 0.3447 (nan)	loss_scale 2048.0000 (3968.0800)	mem 16679MB
[2024-08-01 10:01:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:12 lr 0.000015	 wd 0.0500	time 0.3186 (0.3143)	loss 1.0059 (1.1773)	grad_norm 0.3249 (nan)	loss_scale 2048.0000 (3855.2005)	mem 16679MB
[2024-08-01 10:01:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:40 lr 0.000015	 wd 0.0500	time 0.3764 (0.3144)	loss 1.1589 (1.1785)	grad_norm 0.3652 (nan)	loss_scale 2048.0000 (3754.8562)	mem 16679MB
[2024-08-01 10:02:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:09 lr 0.000015	 wd 0.0500	time 0.3156 (0.3144)	loss 1.2806 (1.1777)	grad_norm 0.3368 (nan)	loss_scale 2048.0000 (3665.0689)	mem 16679MB
[2024-08-01 10:02:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:37 lr 0.000015	 wd 0.0500	time 0.3454 (0.3143)	loss 1.3412 (1.1782)	grad_norm 0.3319 (nan)	loss_scale 2048.0000 (3584.2559)	mem 16679MB
[2024-08-01 10:03:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:06 lr 0.000015	 wd 0.0500	time 0.3043 (0.3142)	loss 0.9925 (1.1787)	grad_norm 0.4301 (nan)	loss_scale 2048.0000 (3511.1356)	mem 16679MB
[2024-08-01 10:03:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:34 lr 0.000016	 wd 0.0500	time 0.2935 (0.3140)	loss 0.8109 (1.1779)	grad_norm 0.3371 (nan)	loss_scale 2048.0000 (3444.6597)	mem 16679MB
[2024-08-01 10:04:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:03 lr 0.000016	 wd 0.0500	time 0.3465 (0.3138)	loss 0.9254 (1.1781)	grad_norm 0.3525 (nan)	loss_scale 2048.0000 (3383.9618)	mem 16679MB
[2024-08-01 10:04:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:31 lr 0.000016	 wd 0.0500	time 0.3159 (0.3137)	loss 1.4237 (1.1798)	grad_norm 0.3704 (nan)	loss_scale 2048.0000 (3328.3199)	mem 16679MB
[2024-08-01 10:05:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2842 (0.3130)	loss 1.3177 (1.1796)	grad_norm 0.3492 (nan)	loss_scale 2048.0000 (3277.1275)	mem 16679MB
[2024-08-01 10:05:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 3 training takes 0:13:05
[2024-08-01 10:05:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.058 (12.058)	Loss 0.5264 (0.5264)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 10:05:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.646 Acc@5 97.674
[2024-08-01 10:05:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-08-01 10:05:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.66%
[2024-08-01 10:06:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:47:54 lr 0.000016	 wd 0.0500	time 11.2208 (11.2208)	loss 1.2750 (1.2750)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:06:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:16:39 lr 0.000016	 wd 0.0500	time 0.3113 (0.4162)	loss 0.9625 (1.1933)	grad_norm 0.3499 (0.3542)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:07:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:13:53 lr 0.000016	 wd 0.0500	time 0.2977 (0.3620)	loss 0.8721 (1.1820)	grad_norm 0.3474 (0.3727)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:07:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:12:35 lr 0.000016	 wd 0.0500	time 0.2969 (0.3429)	loss 0.8153 (1.1788)	grad_norm 0.3596 (0.3638)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:08:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:11:41 lr 0.000017	 wd 0.0500	time 0.3224 (0.3337)	loss 1.3148 (1.1740)	grad_norm 0.3408 (0.3600)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:08:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:10:57 lr 0.000017	 wd 0.0500	time 0.2921 (0.3282)	loss 1.2662 (1.1746)	grad_norm 0.3803 (0.3596)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:09:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:17 lr 0.000017	 wd 0.0500	time 0.3055 (0.3246)	loss 1.1840 (1.1794)	grad_norm 0.3544 (0.3577)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:09:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:09:39 lr 0.000017	 wd 0.0500	time 0.2951 (0.3218)	loss 0.8513 (1.1799)	grad_norm 0.3509 (0.3601)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:10:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:04 lr 0.000017	 wd 0.0500	time 0.3083 (0.3200)	loss 0.7925 (1.1824)	grad_norm 0.3535 (0.3588)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:10:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:08:30 lr 0.000017	 wd 0.0500	time 0.3061 (0.3186)	loss 0.8317 (1.1837)	grad_norm 0.3418 (0.3578)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:11:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:07:56 lr 0.000018	 wd 0.0500	time 0.2922 (0.3173)	loss 1.6012 (1.1838)	grad_norm 0.3279 (0.3566)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:23 lr 0.000018	 wd 0.0500	time 0.3271 (0.3167)	loss 1.4794 (1.1846)	grad_norm 0.3352 (0.3557)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:12:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:06:52 lr 0.000018	 wd 0.0500	time 0.2911 (0.3166)	loss 1.2915 (1.1835)	grad_norm 0.3525 (0.3565)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:12:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:20 lr 0.000018	 wd 0.0500	time 0.2914 (0.3165)	loss 1.0232 (1.1842)	grad_norm 0.3680 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:13:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:05:47 lr 0.000018	 wd 0.0500	time 0.3216 (0.3158)	loss 1.4338 (1.1841)	grad_norm 0.3531 (0.3560)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:13:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:15 lr 0.000018	 wd 0.0500	time 0.2968 (0.3152)	loss 1.2222 (1.1844)	grad_norm 0.3367 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:14:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:04:44 lr 0.000019	 wd 0.0500	time 0.2879 (0.3150)	loss 1.4771 (1.1831)	grad_norm 0.3523 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:14:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:12 lr 0.000019	 wd 0.0500	time 0.3078 (0.3146)	loss 1.1801 (1.1837)	grad_norm 0.3524 (0.3545)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:15:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:40 lr 0.000019	 wd 0.0500	time 0.2890 (0.3144)	loss 1.5442 (1.1858)	grad_norm 0.3399 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:15:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:09 lr 0.000019	 wd 0.0500	time 0.3106 (0.3154)	loss 1.4243 (1.1861)	grad_norm 0.3443 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:16:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:38 lr 0.000019	 wd 0.0500	time 0.2883 (0.3151)	loss 0.8672 (1.1839)	grad_norm 0.3495 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:16:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:06 lr 0.000019	 wd 0.0500	time 0.3257 (0.3151)	loss 1.0224 (1.1827)	grad_norm 0.3396 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:17:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:35 lr 0.000020	 wd 0.0500	time 0.2865 (0.3147)	loss 0.8937 (1.1824)	grad_norm 0.3257 (0.3544)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:03 lr 0.000020	 wd 0.0500	time 0.2958 (0.3145)	loss 0.8028 (1.1817)	grad_norm 0.3484 (0.3546)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:18:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.2996 (0.3142)	loss 0.7973 (1.1815)	grad_norm 0.3207 (0.3545)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:18:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2856 (0.3136)	loss 1.2520 (1.1822)	grad_norm 0.3576 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:18:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 4 training takes 0:13:06
[2024-08-01 10:19:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.932 (11.932)	Loss 0.5161 (0.5161)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 10:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.666 Acc@5 97.680
[2024-08-01 10:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 10:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.67%
[2024-08-01 10:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 10:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 10:19:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:42:45 lr 0.000020	 wd 0.0500	time 11.0971 (11.0971)	loss 1.4138 (1.4138)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:20:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:16:34 lr 0.000020	 wd 0.0500	time 0.2885 (0.4142)	loss 0.9974 (1.2124)	grad_norm 0.3532 (0.3491)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:20:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:13:52 lr 0.000020	 wd 0.0500	time 0.2912 (0.3615)	loss 1.3618 (1.1822)	grad_norm 0.3485 (0.3576)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:21:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:12:34 lr 0.000020	 wd 0.0500	time 0.2947 (0.3426)	loss 1.0175 (1.1709)	grad_norm 0.3361 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:21:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:11:39 lr 0.000020	 wd 0.0500	time 0.3132 (0.3330)	loss 1.2791 (1.1800)	grad_norm 0.3568 (0.3560)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 10:22:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:10:56 lr 0.000020	 wd 0.0500	time 0.2863 (0.3278)	loss 1.0099 (1.1786)	grad_norm 0.3592 (0.3547)	loss_scale 4096.0000 (2064.3513)	mem 16679MB
[2024-08-01 10:22:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:17 lr 0.000020	 wd 0.0500	time 0.2917 (0.3245)	loss 1.0518 (1.1801)	grad_norm 0.3599 (0.3537)	loss_scale 4096.0000 (2402.3960)	mem 16679MB
[2024-08-01 10:23:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:09:39 lr 0.000020	 wd 0.0500	time 0.3552 (0.3216)	loss 0.9231 (1.1785)	grad_norm 0.3324 (0.3528)	loss_scale 4096.0000 (2643.9943)	mem 16679MB
[2024-08-01 10:23:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:04 lr 0.000020	 wd 0.0500	time 0.3046 (0.3199)	loss 1.5405 (1.1777)	grad_norm 0.3408 (0.3517)	loss_scale 4096.0000 (2825.2684)	mem 16679MB
[2024-08-01 10:24:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:08:30 lr 0.000020	 wd 0.0500	time 0.3219 (0.3184)	loss 1.0700 (1.1766)	grad_norm 0.3355 (0.3522)	loss_scale 4096.0000 (2966.3041)	mem 16679MB
[2024-08-01 10:24:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:07:56 lr 0.000020	 wd 0.0500	time 0.2921 (0.3173)	loss 1.2546 (1.1748)	grad_norm 0.3212 (0.3513)	loss_scale 4096.0000 (3079.1608)	mem 16679MB
[2024-08-01 10:25:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:23 lr 0.000020	 wd 0.0500	time 0.2998 (0.3166)	loss 1.1996 (1.1759)	grad_norm 0.3521 (0.3523)	loss_scale 4096.0000 (3171.5168)	mem 16679MB
[2024-08-01 10:25:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:06:51 lr 0.000020	 wd 0.0500	time 0.2936 (0.3158)	loss 1.4329 (1.1745)	grad_norm 0.3349 (0.3525)	loss_scale 4096.0000 (3248.4929)	mem 16679MB
[2024-08-01 10:26:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:18 lr 0.000020	 wd 0.0500	time 0.2882 (0.3153)	loss 0.9997 (1.1753)	grad_norm 0.3450 (0.3527)	loss_scale 4096.0000 (3313.6357)	mem 16679MB
[2024-08-01 10:26:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:05:47 lr 0.000020	 wd 0.0500	time 0.3038 (0.3151)	loss 1.4480 (1.1790)	grad_norm 0.3549 (0.3523)	loss_scale 4096.0000 (3369.4789)	mem 16679MB
[2024-08-01 10:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:15 lr 0.000020	 wd 0.0500	time 0.3093 (0.3149)	loss 0.8560 (1.1802)	grad_norm 0.3660 (0.3521)	loss_scale 4096.0000 (3417.8814)	mem 16679MB
[2024-08-01 10:27:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:43 lr 0.000020	 wd 0.0500	time 0.3308 (0.3145)	loss 1.4293 (1.1828)	grad_norm 0.3476 (0.3519)	loss_scale 4096.0000 (3460.2374)	mem 16679MB
[2024-08-01 10:28:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:12 lr 0.000020	 wd 0.0500	time 0.2905 (0.3144)	loss 0.9670 (1.1831)	grad_norm 0.3282 (0.3521)	loss_scale 4096.0000 (3497.6132)	mem 16679MB
[2024-08-01 10:28:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:40 lr 0.000020	 wd 0.0500	time 0.2939 (0.3143)	loss 0.7761 (1.1810)	grad_norm 0.3490 (0.3520)	loss_scale 4096.0000 (3530.8384)	mem 16679MB
[2024-08-01 10:29:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:09 lr 0.000020	 wd 0.0500	time 0.3144 (0.3143)	loss 1.1460 (1.1805)	grad_norm 0.3356 (0.3524)	loss_scale 4096.0000 (3560.5681)	mem 16679MB
[2024-08-01 10:29:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:37 lr 0.000020	 wd 0.0500	time 0.3101 (0.3143)	loss 1.5831 (1.1821)	grad_norm 0.3874 (0.3521)	loss_scale 4096.0000 (3587.3263)	mem 16679MB
[2024-08-01 10:30:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:06 lr 0.000020	 wd 0.0500	time 0.3260 (0.3143)	loss 1.4969 (1.1817)	grad_norm 0.3468 (0.3524)	loss_scale 4096.0000 (3611.5374)	mem 16679MB
[2024-08-01 10:30:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:34 lr 0.000020	 wd 0.0500	time 0.2976 (0.3144)	loss 1.2378 (1.1830)	grad_norm 0.3337 (0.3522)	loss_scale 4096.0000 (3633.5484)	mem 16679MB
[2024-08-01 10:31:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:03 lr 0.000020	 wd 0.0500	time 0.3163 (0.3143)	loss 1.1453 (1.1815)	grad_norm 0.3559 (0.3522)	loss_scale 4096.0000 (3653.6462)	mem 16679MB
[2024-08-01 10:32:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.2985 (0.3145)	loss 0.7932 (1.1809)	grad_norm 0.3561 (0.3520)	loss_scale 4096.0000 (3672.0700)	mem 16679MB
[2024-08-01 10:32:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2842 (0.3138)	loss 1.3147 (1.1821)	grad_norm 0.3846 (0.3518)	loss_scale 4096.0000 (3689.0204)	mem 16679MB
[2024-08-01 10:32:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 5 training takes 0:13:07
[2024-08-01 10:32:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.268 (11.268)	Loss 0.5117 (0.5117)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 10:33:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.666 Acc@5 97.698
[2024-08-01 10:33:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 10:33:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.67%
[2024-08-01 10:33:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 10:33:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 10:33:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:18:49 lr 0.000020	 wd 0.0500	time 11.9624 (11.9624)	loss 1.2182 (1.2182)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:33:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:16:49 lr 0.000020	 wd 0.0500	time 0.3247 (0.4202)	loss 0.8978 (1.2041)	grad_norm 0.3679 (0.3444)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:34:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:13:53 lr 0.000020	 wd 0.0500	time 0.2904 (0.3622)	loss 0.8733 (1.1945)	grad_norm 0.3181 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:34:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:12:51 lr 0.000020	 wd 0.0500	time 0.3016 (0.3503)	loss 0.9577 (1.1862)	grad_norm 0.3576 (0.3602)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:35:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:11:51 lr 0.000020	 wd 0.0500	time 0.3044 (0.3385)	loss 0.8028 (1.1885)	grad_norm 0.3710 (0.3581)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:35:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:05 lr 0.000020	 wd 0.0500	time 0.3136 (0.3322)	loss 1.2262 (1.1888)	grad_norm 0.3378 (0.3550)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:36:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:10:24 lr 0.000020	 wd 0.0500	time 0.3360 (0.3281)	loss 0.8710 (1.1800)	grad_norm 0.3365 (0.3555)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:36:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:09:45 lr 0.000020	 wd 0.0500	time 0.2941 (0.3250)	loss 1.4946 (1.1826)	grad_norm 0.3186 (0.3535)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:37:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:09 lr 0.000020	 wd 0.0500	time 0.3118 (0.3231)	loss 0.7294 (1.1871)	grad_norm 0.3269 (0.3524)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:37:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:08:34 lr 0.000020	 wd 0.0500	time 0.2894 (0.3214)	loss 1.4967 (1.1834)	grad_norm 0.3298 (0.3518)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:38:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:00 lr 0.000020	 wd 0.0500	time 0.2961 (0.3199)	loss 1.0579 (1.1832)	grad_norm 0.3260 (0.3519)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:38:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:27 lr 0.000020	 wd 0.0500	time 0.2963 (0.3190)	loss 0.8720 (1.1797)	grad_norm 0.3416 (0.3519)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:39:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:06:54 lr 0.000020	 wd 0.0500	time 0.2846 (0.3182)	loss 1.2590 (1.1767)	grad_norm 0.3725 (0.3517)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:39:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:22 lr 0.000020	 wd 0.0500	time 0.2965 (0.3179)	loss 1.1201 (1.1753)	grad_norm 0.3332 (0.3520)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:40:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:05:49 lr 0.000020	 wd 0.0500	time 0.3098 (0.3173)	loss 0.9511 (1.1754)	grad_norm 0.3427 (0.3516)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:40:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:17 lr 0.000020	 wd 0.0500	time 0.3260 (0.3167)	loss 0.7433 (1.1763)	grad_norm 0.3416 (0.3513)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:41:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:45 lr 0.000020	 wd 0.0500	time 0.2998 (0.3163)	loss 1.4559 (1.1737)	grad_norm 0.3281 (0.3509)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:42:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:13 lr 0.000020	 wd 0.0500	time 0.3453 (0.3161)	loss 1.3915 (1.1730)	grad_norm 0.3353 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:42:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:41 lr 0.000020	 wd 0.0500	time 0.3051 (0.3159)	loss 1.2235 (1.1735)	grad_norm 0.3350 (0.3514)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:43:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:09 lr 0.000020	 wd 0.0500	time 0.3015 (0.3155)	loss 1.3638 (1.1747)	grad_norm 0.3398 (0.3513)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 10:43:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:38 lr 0.000020	 wd 0.0500	time 0.3020 (0.3154)	loss 0.9935 (1.1755)	grad_norm 0.3462 (0.3519)	loss_scale 8192.0000 (4108.2819)	mem 16679MB
[2024-08-01 10:44:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:06 lr 0.000020	 wd 0.0500	time 0.3025 (0.3152)	loss 1.0040 (1.1757)	grad_norm 0.3615 (0.3518)	loss_scale 8192.0000 (4302.6521)	mem 16679MB
[2024-08-01 10:44:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:35 lr 0.000020	 wd 0.0500	time 0.2889 (0.3150)	loss 0.8158 (1.1750)	grad_norm 0.3717 (0.3518)	loss_scale 8192.0000 (4479.3603)	mem 16679MB
[2024-08-01 10:45:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:03 lr 0.000020	 wd 0.0500	time 0.2965 (0.3148)	loss 1.0449 (1.1776)	grad_norm 0.3623 (0.3527)	loss_scale 8192.0000 (4640.7093)	mem 16679MB
[2024-08-01 10:45:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.3061 (0.3151)	loss 1.4039 (1.1768)	grad_norm 0.3499 (0.3534)	loss_scale 8192.0000 (4788.6181)	mem 16679MB
[2024-08-01 10:46:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2841 (0.3144)	loss 1.3733 (1.1764)	grad_norm 0.4263 (0.3538)	loss_scale 8192.0000 (4924.6989)	mem 16679MB
[2024-08-01 10:46:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 6 training takes 0:13:08
[2024-08-01 10:46:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.212 (12.212)	Loss 0.5225 (0.5225)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 10:46:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.668 Acc@5 97.684
[2024-08-01 10:46:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 10:46:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.67%
[2024-08-01 10:46:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 10:46:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 10:46:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:27:32 lr 0.000020	 wd 0.0500	time 10.7326 (10.7326)	loss 0.9307 (0.9307)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:16:25 lr 0.000020	 wd 0.0500	time 0.2881 (0.4103)	loss 1.2123 (1.1603)	grad_norm 0.3399 (0.3619)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:47:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:13:50 lr 0.000020	 wd 0.0500	time 0.3003 (0.3608)	loss 1.1413 (1.1825)	grad_norm 0.3509 (0.3610)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:48:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:12:33 lr 0.000020	 wd 0.0500	time 0.2853 (0.3421)	loss 1.3197 (1.1773)	grad_norm 0.3407 (0.3591)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:48:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:11:38 lr 0.000020	 wd 0.0500	time 0.2976 (0.3323)	loss 0.8394 (1.1825)	grad_norm 0.3568 (0.3565)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:49:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:10:53 lr 0.000020	 wd 0.0500	time 0.2865 (0.3266)	loss 1.3580 (1.1795)	grad_norm 0.3289 (0.3545)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:49:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:14 lr 0.000020	 wd 0.0500	time 0.2935 (0.3232)	loss 1.3246 (1.1836)	grad_norm 0.3671 (0.3539)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:50:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:09:38 lr 0.000020	 wd 0.0500	time 0.2853 (0.3211)	loss 0.9921 (1.1783)	grad_norm 0.3314 (0.3536)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:50:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:03 lr 0.000020	 wd 0.0500	time 0.2890 (0.3196)	loss 0.8886 (1.1760)	grad_norm 0.3566 (0.3547)	loss_scale 8192.0000 (8192.0000)	mem 16679MB
[2024-08-01 10:51:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:08:29 lr 0.000020	 wd 0.0500	time 0.3157 (0.3182)	loss 1.4627 (1.1777)	grad_norm 0.3364 (nan)	loss_scale 4096.0000 (7801.0388)	mem 16679MB
[2024-08-01 10:51:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:07:56 lr 0.000020	 wd 0.0500	time 0.3020 (0.3173)	loss 0.8144 (1.1777)	grad_norm 0.3654 (nan)	loss_scale 4096.0000 (7430.9051)	mem 16679MB
[2024-08-01 10:52:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:23 lr 0.000020	 wd 0.0500	time 0.2959 (0.3163)	loss 1.3710 (1.1769)	grad_norm 0.3420 (nan)	loss_scale 4096.0000 (7128.0073)	mem 16679MB
[2024-08-01 10:53:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:06:51 lr 0.000020	 wd 0.0500	time 0.2904 (0.3157)	loss 1.4122 (1.1735)	grad_norm 0.3458 (nan)	loss_scale 4096.0000 (6875.5504)	mem 16679MB
[2024-08-01 10:53:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:18 lr 0.000020	 wd 0.0500	time 0.2941 (0.3151)	loss 1.3832 (1.1746)	grad_norm 0.3237 (nan)	loss_scale 4096.0000 (6661.9032)	mem 16679MB
[2024-08-01 10:54:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:05:46 lr 0.000019	 wd 0.0500	time 0.3204 (0.3146)	loss 1.2936 (1.1743)	grad_norm 0.3320 (nan)	loss_scale 4096.0000 (6478.7552)	mem 16679MB
[2024-08-01 10:54:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:14 lr 0.000019	 wd 0.0500	time 0.2946 (0.3142)	loss 0.8411 (1.1749)	grad_norm 0.3305 (nan)	loss_scale 4096.0000 (6320.0107)	mem 16679MB
[2024-08-01 10:55:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:43 lr 0.000019	 wd 0.0500	time 0.2931 (0.3139)	loss 1.1157 (1.1756)	grad_norm 0.3573 (nan)	loss_scale 4096.0000 (6181.0968)	mem 16679MB
[2024-08-01 10:55:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:11 lr 0.000019	 wd 0.0500	time 0.3171 (0.3137)	loss 1.3490 (1.1758)	grad_norm 0.3242 (nan)	loss_scale 4096.0000 (6058.5162)	mem 16679MB
[2024-08-01 10:56:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:40 lr 0.000019	 wd 0.0500	time 0.3450 (0.3136)	loss 0.7578 (1.1781)	grad_norm 0.3368 (nan)	loss_scale 4096.0000 (5949.5480)	mem 16679MB
[2024-08-01 10:56:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:08 lr 0.000019	 wd 0.0500	time 0.3031 (0.3133)	loss 1.1382 (1.1784)	grad_norm 0.3428 (nan)	loss_scale 4096.0000 (5852.0442)	mem 16679MB
[2024-08-01 10:57:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:37 lr 0.000019	 wd 0.0500	time 0.3609 (0.3134)	loss 1.1785 (1.1791)	grad_norm 0.3412 (nan)	loss_scale 4096.0000 (5764.2859)	mem 16679MB
[2024-08-01 10:57:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:05 lr 0.000019	 wd 0.0500	time 0.2947 (0.3133)	loss 1.4586 (1.1818)	grad_norm 0.3499 (nan)	loss_scale 4096.0000 (5684.8815)	mem 16679MB
[2024-08-01 10:58:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:34 lr 0.000019	 wd 0.0500	time 0.3046 (0.3132)	loss 1.2436 (1.1799)	grad_norm 0.3379 (nan)	loss_scale 4096.0000 (5612.6924)	mem 16679MB
[2024-08-01 10:58:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:03 lr 0.000019	 wd 0.0500	time 0.3098 (0.3131)	loss 1.4145 (1.1790)	grad_norm 0.3530 (nan)	loss_scale 4096.0000 (5546.7779)	mem 16679MB
[2024-08-01 10:59:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:31 lr 0.000019	 wd 0.0500	time 0.3354 (0.3130)	loss 1.2618 (1.1794)	grad_norm 0.3363 (nan)	loss_scale 4096.0000 (5486.3540)	mem 16679MB
[2024-08-01 10:59:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2880 (0.3124)	loss 1.5948 (1.1795)	grad_norm 0.3242 (nan)	loss_scale 4096.0000 (5430.7621)	mem 16679MB
[2024-08-01 10:59:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 7 training takes 0:13:03
[2024-08-01 10:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.545 (11.545)	Loss 0.5005 (0.5005)	Acc@1 93.750 (93.750)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 11:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.686 Acc@5 97.698
[2024-08-01 11:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 11:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.69%
[2024-08-01 11:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 11:00:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 11:00:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:47:39 lr 0.000019	 wd 0.0500	time 11.2150 (11.2150)	loss 1.2676 (1.2676)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:00:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:16:22 lr 0.000019	 wd 0.0500	time 0.2968 (0.4091)	loss 0.9816 (1.2224)	grad_norm 0.3269 (0.3463)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:01:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:13:39 lr 0.000019	 wd 0.0500	time 0.2969 (0.3561)	loss 0.8322 (1.2079)	grad_norm 0.3437 (0.3459)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:01:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:12:27 lr 0.000019	 wd 0.0500	time 0.3009 (0.3396)	loss 1.4199 (1.1987)	grad_norm 0.3444 (0.3500)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:02:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:11:34 lr 0.000019	 wd 0.0500	time 0.2870 (0.3305)	loss 0.8502 (1.1974)	grad_norm 0.3440 (0.3496)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:10:51 lr 0.000019	 wd 0.0500	time 0.2868 (0.3254)	loss 0.8150 (1.1861)	grad_norm 0.3378 (0.3493)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:03:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:10:12 lr 0.000019	 wd 0.0500	time 0.3334 (0.3222)	loss 0.9119 (1.1831)	grad_norm 0.3596 (0.3490)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:03:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:09:36 lr 0.000019	 wd 0.0500	time 0.3188 (0.3199)	loss 1.3565 (1.1836)	grad_norm 0.3516 (0.3494)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:04:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:01 lr 0.000019	 wd 0.0500	time 0.2952 (0.3182)	loss 1.4609 (1.1853)	grad_norm 0.3497 (0.3490)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:05:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:08:27 lr 0.000019	 wd 0.0500	time 0.3169 (0.3166)	loss 0.9547 (1.1803)	grad_norm 0.3358 (0.3483)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:05:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:07:53 lr 0.000019	 wd 0.0500	time 0.2892 (0.3156)	loss 1.3645 (1.1838)	grad_norm 0.3451 (0.3485)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:06:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:21 lr 0.000019	 wd 0.0500	time 0.2949 (0.3148)	loss 1.2148 (1.1818)	grad_norm 0.3273 (0.3506)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:06:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:06:49 lr 0.000019	 wd 0.0500	time 0.2938 (0.3142)	loss 1.1250 (1.1815)	grad_norm 0.4490 (0.3515)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:07:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:17 lr 0.000019	 wd 0.0500	time 0.3297 (0.3141)	loss 0.9012 (1.1809)	grad_norm 0.3523 (0.3515)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:07:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:05:45 lr 0.000019	 wd 0.0500	time 0.3002 (0.3135)	loss 1.2796 (1.1819)	grad_norm 0.3451 (nan)	loss_scale 2048.0000 (4078.4582)	mem 16679MB
[2024-08-01 11:08:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:13 lr 0.000019	 wd 0.0500	time 0.2921 (0.3133)	loss 1.1800 (1.1821)	grad_norm 0.3497 (nan)	loss_scale 2048.0000 (3943.1845)	mem 16679MB
[2024-08-01 11:08:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:42 lr 0.000019	 wd 0.0500	time 0.3129 (0.3132)	loss 1.3792 (1.1849)	grad_norm 0.3703 (nan)	loss_scale 2048.0000 (3824.8095)	mem 16679MB
[2024-08-01 11:09:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:11 lr 0.000019	 wd 0.0500	time 0.2949 (0.3132)	loss 0.7729 (1.1874)	grad_norm 0.3454 (nan)	loss_scale 2048.0000 (3720.3527)	mem 16679MB
[2024-08-01 11:09:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:39 lr 0.000019	 wd 0.0500	time 0.3081 (0.3133)	loss 0.7959 (1.1861)	grad_norm 0.3592 (nan)	loss_scale 2048.0000 (3627.4958)	mem 16679MB
[2024-08-01 11:10:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:08 lr 0.000019	 wd 0.0500	time 0.2904 (0.3132)	loss 1.1879 (1.1879)	grad_norm 0.3329 (nan)	loss_scale 2048.0000 (3544.4082)	mem 16679MB
[2024-08-01 11:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:37 lr 0.000019	 wd 0.0500	time 0.3103 (0.3133)	loss 0.8056 (1.1867)	grad_norm 0.3758 (nan)	loss_scale 2048.0000 (3469.6252)	mem 16679MB
[2024-08-01 11:11:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:05 lr 0.000019	 wd 0.0500	time 0.3139 (0.3134)	loss 0.8839 (1.1871)	grad_norm 0.3643 (nan)	loss_scale 2048.0000 (3401.9610)	mem 16679MB
[2024-08-01 11:11:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:34 lr 0.000019	 wd 0.0500	time 0.2997 (0.3132)	loss 1.1356 (1.1855)	grad_norm 0.3671 (nan)	loss_scale 2048.0000 (3340.4453)	mem 16679MB
[2024-08-01 11:12:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:03 lr 0.000019	 wd 0.0500	time 0.3064 (0.3131)	loss 1.3533 (1.1841)	grad_norm 0.3286 (nan)	loss_scale 2048.0000 (3284.2764)	mem 16679MB
[2024-08-01 11:12:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:32 lr 0.000019	 wd 0.0500	time 0.3327 (0.3138)	loss 1.1752 (1.1840)	grad_norm 0.3612 (nan)	loss_scale 2048.0000 (3232.7863)	mem 16679MB
[2024-08-01 11:13:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2843 (0.3134)	loss 0.9062 (1.1838)	grad_norm 0.3257 (nan)	loss_scale 2048.0000 (3185.4138)	mem 16679MB
[2024-08-01 11:13:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 8 training takes 0:13:06
[2024-08-01 11:13:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.265 (12.265)	Loss 0.5015 (0.5015)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 16679MB
[2024-08-01 11:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.720 Acc@5 97.700
[2024-08-01 11:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 11:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.72%
[2024-08-01 11:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 11:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 11:14:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:45:04 lr 0.000019	 wd 0.0500	time 11.1530 (11.1530)	loss 1.3607 (1.3607)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:14:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:16:32 lr 0.000019	 wd 0.0500	time 0.2929 (0.4131)	loss 1.2612 (1.1612)	grad_norm 0.3618 (0.3487)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:15:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:13:52 lr 0.000019	 wd 0.0500	time 0.3096 (0.3618)	loss 0.9245 (1.1852)	grad_norm 0.3370 (0.3559)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:15:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:12:36 lr 0.000019	 wd 0.0500	time 0.3132 (0.3433)	loss 1.5233 (1.1935)	grad_norm 0.3356 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:11:42 lr 0.000019	 wd 0.0500	time 0.3080 (0.3342)	loss 0.8555 (1.1768)	grad_norm 0.3357 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:16:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:10:58 lr 0.000019	 wd 0.0500	time 0.2912 (0.3289)	loss 0.7632 (1.1722)	grad_norm 0.3515 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:17:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:18 lr 0.000019	 wd 0.0500	time 0.3032 (0.3250)	loss 0.8406 (1.1764)	grad_norm 0.3357 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:17:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:09:40 lr 0.000019	 wd 0.0500	time 0.3086 (0.3224)	loss 1.4035 (1.1801)	grad_norm 0.3326 (0.3526)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:18:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:06 lr 0.000019	 wd 0.0500	time 0.3403 (0.3209)	loss 1.3852 (1.1813)	grad_norm 0.3281 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:08:32 lr 0.000019	 wd 0.0500	time 0.2918 (0.3199)	loss 1.3671 (1.1808)	grad_norm 0.3402 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:19:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:07:58 lr 0.000019	 wd 0.0500	time 0.3167 (0.3189)	loss 1.1731 (1.1816)	grad_norm 0.3300 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:25 lr 0.000018	 wd 0.0500	time 0.3155 (0.3178)	loss 1.3426 (1.1775)	grad_norm 0.3418 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:06:52 lr 0.000018	 wd 0.0500	time 0.3348 (0.3172)	loss 0.8428 (1.1764)	grad_norm 0.3739 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:20:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:22 lr 0.000018	 wd 0.0500	time 0.2900 (0.3179)	loss 1.2857 (1.1773)	grad_norm 0.3335 (0.3542)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:21:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:05:49 lr 0.000018	 wd 0.0500	time 0.3230 (0.3172)	loss 1.0711 (1.1764)	grad_norm 0.3297 (0.3535)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:21:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:17 lr 0.000018	 wd 0.0500	time 0.3266 (0.3167)	loss 0.8685 (1.1797)	grad_norm 0.3686 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:22:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:04:46 lr 0.000018	 wd 0.0500	time 0.3099 (0.3171)	loss 0.9292 (1.1779)	grad_norm 0.3401 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:22:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:13 lr 0.000018	 wd 0.0500	time 0.2859 (0.3167)	loss 1.4633 (1.1784)	grad_norm 0.3534 (0.3536)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:23:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:42 lr 0.000018	 wd 0.0500	time 0.3345 (0.3164)	loss 1.2766 (1.1771)	grad_norm 0.3987 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:23:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:10 lr 0.000018	 wd 0.0500	time 0.3173 (0.3160)	loss 1.4196 (1.1766)	grad_norm 0.3611 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:24:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:38 lr 0.000018	 wd 0.0500	time 0.2928 (0.3158)	loss 1.4093 (1.1772)	grad_norm 0.3174 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:24:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:06 lr 0.000018	 wd 0.0500	time 0.3583 (0.3155)	loss 1.4161 (1.1788)	grad_norm 0.3521 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:25:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:35 lr 0.000018	 wd 0.0500	time 0.2858 (0.3153)	loss 1.1761 (1.1799)	grad_norm 0.3504 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:25:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:03 lr 0.000018	 wd 0.0500	time 0.2984 (0.3152)	loss 0.7916 (1.1798)	grad_norm 0.3547 (0.3532)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:26:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:32 lr 0.000018	 wd 0.0500	time 0.3020 (0.3151)	loss 1.0924 (1.1801)	grad_norm 0.3942 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:26:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2841 (0.3145)	loss 1.2628 (1.1796)	grad_norm 0.3449 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:27:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 9 training takes 0:13:09
[2024-08-01 11:27:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.264 (12.264)	Loss 0.5122 (0.5122)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 11:27:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.740 Acc@5 97.710
[2024-08-01 11:27:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 11:27:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 11:27:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 11:27:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 11:27:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:49:49 lr 0.000018	 wd 0.0500	time 11.2667 (11.2667)	loss 1.6344 (1.6344)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:28:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:16:49 lr 0.000018	 wd 0.0500	time 0.2886 (0.4201)	loss 0.8816 (1.1902)	grad_norm 0.3755 (0.3619)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:28:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:00 lr 0.000018	 wd 0.0500	time 0.2862 (0.3652)	loss 1.4670 (1.1854)	grad_norm 0.3189 (0.3577)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:29:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:12:41 lr 0.000018	 wd 0.0500	time 0.2963 (0.3456)	loss 1.3302 (1.1806)	grad_norm 0.3590 (0.3545)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 11:29:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:11:47 lr 0.000018	 wd 0.0500	time 0.3049 (0.3365)	loss 0.9166 (1.1847)	grad_norm 0.3551 (0.3561)	loss_scale 4096.0000 (2129.7157)	mem 16679MB
[2024-08-01 11:30:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:01 lr 0.000018	 wd 0.0500	time 0.3058 (0.3306)	loss 0.7088 (1.1803)	grad_norm 0.3318 (0.3552)	loss_scale 4096.0000 (2522.1876)	mem 16679MB
[2024-08-01 11:30:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:10:20 lr 0.000018	 wd 0.0500	time 0.3196 (0.3265)	loss 1.1849 (1.1880)	grad_norm 0.3270 (0.3529)	loss_scale 4096.0000 (2784.0532)	mem 16679MB
[2024-08-01 11:31:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:09:43 lr 0.000018	 wd 0.0500	time 0.2946 (0.3236)	loss 1.2961 (1.1859)	grad_norm 0.3146 (0.3521)	loss_scale 4096.0000 (2971.2068)	mem 16679MB
[2024-08-01 11:31:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:07 lr 0.000018	 wd 0.0500	time 0.3038 (0.3215)	loss 1.2946 (1.1887)	grad_norm 0.3434 (0.3514)	loss_scale 4096.0000 (3111.6305)	mem 16679MB
[2024-08-01 11:32:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:08:32 lr 0.000018	 wd 0.0500	time 0.2972 (0.3199)	loss 1.4691 (1.1862)	grad_norm 0.3401 (0.3515)	loss_scale 4096.0000 (3220.8835)	mem 16679MB
[2024-08-01 11:32:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:01 lr 0.000018	 wd 0.0500	time 0.3010 (0.3205)	loss 1.2838 (1.1873)	grad_norm 0.3373 (0.3508)	loss_scale 4096.0000 (3308.3077)	mem 16679MB
[2024-08-01 11:33:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:27 lr 0.000018	 wd 0.0500	time 0.3012 (0.3193)	loss 1.0155 (1.1933)	grad_norm 0.3568 (0.3506)	loss_scale 4096.0000 (3379.8510)	mem 16679MB
[2024-08-01 11:33:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:06:54 lr 0.000018	 wd 0.0500	time 0.2979 (0.3182)	loss 1.4128 (1.1907)	grad_norm 0.3459 (0.3506)	loss_scale 4096.0000 (3439.4804)	mem 16679MB
[2024-08-01 11:34:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:21 lr 0.000018	 wd 0.0500	time 0.3012 (0.3176)	loss 1.3819 (1.1882)	grad_norm 0.3333 (0.3521)	loss_scale 4096.0000 (3489.9431)	mem 16679MB
[2024-08-01 11:34:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:05:49 lr 0.000018	 wd 0.0500	time 0.3593 (0.3169)	loss 1.3492 (1.1861)	grad_norm 0.3389 (0.3516)	loss_scale 4096.0000 (3533.2020)	mem 16679MB
[2024-08-01 11:35:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:17 lr 0.000018	 wd 0.0500	time 0.2937 (0.3165)	loss 1.1845 (1.1835)	grad_norm 0.3563 (0.3512)	loss_scale 4096.0000 (3570.6969)	mem 16679MB
[2024-08-01 11:35:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:04:44 lr 0.000018	 wd 0.0500	time 0.3140 (0.3159)	loss 1.1307 (1.1821)	grad_norm 0.3312 (0.3516)	loss_scale 4096.0000 (3603.5078)	mem 16679MB
[2024-08-01 11:36:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:13 lr 0.000018	 wd 0.0500	time 0.2919 (0.3155)	loss 1.2467 (1.1841)	grad_norm 0.3390 (0.3514)	loss_scale 4096.0000 (3632.4609)	mem 16679MB
[2024-08-01 11:36:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:41 lr 0.000018	 wd 0.0500	time 0.2912 (0.3153)	loss 1.1524 (1.1831)	grad_norm 0.3708 (0.3511)	loss_scale 4096.0000 (3658.1988)	mem 16679MB
[2024-08-01 11:37:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:09 lr 0.000018	 wd 0.0500	time 0.2974 (0.3152)	loss 1.1077 (1.1821)	grad_norm 0.3328 (0.3510)	loss_scale 4096.0000 (3681.2288)	mem 16679MB
[2024-08-01 11:37:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:38 lr 0.000017	 wd 0.0500	time 0.3080 (0.3152)	loss 1.3537 (1.1857)	grad_norm 0.3447 (0.3520)	loss_scale 4096.0000 (3701.9570)	mem 16679MB
[2024-08-01 11:38:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:06 lr 0.000017	 wd 0.0500	time 0.2883 (0.3151)	loss 1.2109 (1.1869)	grad_norm 0.3368 (0.3526)	loss_scale 4096.0000 (3720.7120)	mem 16679MB
[2024-08-01 11:39:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:35 lr 0.000017	 wd 0.0500	time 0.2926 (0.3150)	loss 1.3486 (1.1881)	grad_norm 0.3242 (0.3523)	loss_scale 4096.0000 (3737.7628)	mem 16679MB
[2024-08-01 11:39:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:03 lr 0.000017	 wd 0.0500	time 0.3426 (0.3149)	loss 1.3140 (1.1860)	grad_norm 0.3429 (0.3523)	loss_scale 4096.0000 (3753.3316)	mem 16679MB
[2024-08-01 11:40:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:32 lr 0.000017	 wd 0.0500	time 0.2958 (0.3147)	loss 1.3860 (1.1849)	grad_norm 0.3292 (0.3528)	loss_scale 4096.0000 (3767.6035)	mem 16679MB
[2024-08-01 11:40:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2839 (0.3140)	loss 0.9071 (1.1845)	grad_norm 0.3402 (0.3526)	loss_scale 4096.0000 (3780.7341)	mem 16679MB
[2024-08-01 11:40:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 10 training takes 0:13:08
[2024-08-01 11:40:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_10.pth saving......
[2024-08-01 11:40:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_10.pth saved !!!
[2024-08-01 11:40:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.064 (12.064)	Loss 0.5059 (0.5059)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 16679MB
[2024-08-01 11:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.736 Acc@5 97.734
[2024-08-01 11:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 11:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 11:41:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:43:37 lr 0.000017	 wd 0.0500	time 11.1181 (11.1181)	loss 0.9114 (0.9114)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:41:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:16:34 lr 0.000017	 wd 0.0500	time 0.3078 (0.4140)	loss 1.4025 (1.1699)	grad_norm 0.3342 (0.3469)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:42:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:13:52 lr 0.000017	 wd 0.0500	time 0.3484 (0.3618)	loss 1.3484 (1.1906)	grad_norm 0.3506 (0.3463)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:42:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:12:36 lr 0.000017	 wd 0.0500	time 0.3218 (0.3437)	loss 0.8716 (1.1803)	grad_norm 0.3345 (0.3457)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:43:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:11:43 lr 0.000017	 wd 0.0500	time 0.3077 (0.3347)	loss 1.4312 (1.1789)	grad_norm 0.3498 (0.3466)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:43:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:10:58 lr 0.000017	 wd 0.0500	time 0.2892 (0.3290)	loss 0.7470 (1.1783)	grad_norm 0.3210 (0.3473)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:10:18 lr 0.000017	 wd 0.0500	time 0.2971 (0.3250)	loss 1.4690 (1.1789)	grad_norm 0.3393 (0.3482)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:09:40 lr 0.000017	 wd 0.0500	time 0.2968 (0.3221)	loss 1.0152 (1.1742)	grad_norm 0.3662 (0.3505)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:05 lr 0.000017	 wd 0.0500	time 0.3044 (0.3206)	loss 1.5069 (1.1723)	grad_norm 0.3301 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:45:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:08:31 lr 0.000017	 wd 0.0500	time 0.3102 (0.3192)	loss 1.0208 (1.1752)	grad_norm 0.3559 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:46:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:07:57 lr 0.000017	 wd 0.0500	time 0.2880 (0.3181)	loss 1.1769 (1.1752)	grad_norm 0.3444 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:46:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:24 lr 0.000017	 wd 0.0500	time 0.2894 (0.3169)	loss 0.9170 (1.1738)	grad_norm 0.3484 (0.3497)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:47:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:06:51 lr 0.000017	 wd 0.0500	time 0.3346 (0.3160)	loss 0.9464 (1.1706)	grad_norm 0.3331 (0.3493)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:47:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:19 lr 0.000017	 wd 0.0500	time 0.3127 (0.3155)	loss 1.4052 (1.1709)	grad_norm 0.3786 (0.3495)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:48:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:05:47 lr 0.000017	 wd 0.0500	time 0.3183 (0.3150)	loss 0.8232 (1.1712)	grad_norm 0.3450 (0.3494)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:48:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:15 lr 0.000017	 wd 0.0500	time 0.2896 (0.3147)	loss 1.4058 (1.1720)	grad_norm 0.3462 (0.3492)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:04:43 lr 0.000017	 wd 0.0500	time 0.3295 (0.3146)	loss 1.2957 (1.1730)	grad_norm 0.3614 (0.3493)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:50:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:12 lr 0.000017	 wd 0.0500	time 0.3308 (0.3143)	loss 1.0888 (1.1722)	grad_norm 0.3413 (0.3491)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:50:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:40 lr 0.000017	 wd 0.0500	time 0.3120 (0.3142)	loss 0.8516 (1.1721)	grad_norm 0.3487 (0.3488)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:51:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:09 lr 0.000017	 wd 0.0500	time 0.3150 (0.3141)	loss 1.1439 (1.1712)	grad_norm 0.3592 (0.3490)	loss_scale 8192.0000 (4134.7838)	mem 16679MB
[2024-08-01 11:51:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:37 lr 0.000017	 wd 0.0500	time 0.2967 (0.3140)	loss 1.0690 (1.1714)	grad_norm 0.3148 (0.3490)	loss_scale 8192.0000 (4337.5432)	mem 16679MB
[2024-08-01 11:52:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:06 lr 0.000017	 wd 0.0500	time 0.3242 (0.3139)	loss 1.3671 (1.1706)	grad_norm 0.3608 (0.3489)	loss_scale 8192.0000 (4521.0014)	mem 16679MB
[2024-08-01 11:52:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:35 lr 0.000017	 wd 0.0500	time 0.3212 (0.3149)	loss 1.0002 (1.1725)	grad_norm 0.3536 (nan)	loss_scale 4096.0000 (4572.4089)	mem 16679MB
[2024-08-01 11:53:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:03 lr 0.000016	 wd 0.0500	time 0.3075 (0.3147)	loss 1.1840 (1.1706)	grad_norm 0.3505 (nan)	loss_scale 4096.0000 (4551.7045)	mem 16679MB
[2024-08-01 11:53:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:32 lr 0.000016	 wd 0.0500	time 0.3427 (0.3146)	loss 1.4081 (1.1719)	grad_norm 0.3447 (nan)	loss_scale 4096.0000 (4532.7247)	mem 16679MB
[2024-08-01 11:54:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2837 (0.3138)	loss 0.8611 (1.1701)	grad_norm 0.3153 (nan)	loss_scale 4096.0000 (4515.2627)	mem 16679MB
[2024-08-01 11:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 11 training takes 0:13:07
[2024-08-01 11:54:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.632 (12.632)	Loss 0.5039 (0.5039)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 11:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.736 Acc@5 97.710
[2024-08-01 11:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 11:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 11:54:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:17:19 lr 0.000016	 wd 0.0500	time 10.4874 (10.4874)	loss 0.9496 (0.9496)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:55:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:16:49 lr 0.000016	 wd 0.0500	time 0.2977 (0.4203)	loss 1.3177 (1.1738)	grad_norm 0.3324 (0.3488)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:55:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:13:55 lr 0.000016	 wd 0.0500	time 0.3058 (0.3629)	loss 1.3142 (1.1713)	grad_norm 0.3326 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:56:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:12:35 lr 0.000016	 wd 0.0500	time 0.3151 (0.3431)	loss 0.7574 (1.1838)	grad_norm 0.3551 (0.3474)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:56:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:11:43 lr 0.000016	 wd 0.0500	time 0.3019 (0.3345)	loss 1.2460 (1.1750)	grad_norm 0.3381 (0.3496)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:57:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:10:59 lr 0.000016	 wd 0.0500	time 0.2942 (0.3292)	loss 1.4527 (1.1748)	grad_norm 0.3524 (0.3590)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:57:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:10:18 lr 0.000016	 wd 0.0500	time 0.3121 (0.3252)	loss 1.4269 (1.1732)	grad_norm 0.3724 (0.3576)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:58:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:09:41 lr 0.000016	 wd 0.0500	time 0.2992 (0.3228)	loss 1.2259 (1.1713)	grad_norm 0.3410 (0.3558)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:58:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:06 lr 0.000016	 wd 0.0500	time 0.2897 (0.3211)	loss 1.3800 (1.1726)	grad_norm 0.3392 (0.3552)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 11:59:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:08:32 lr 0.000016	 wd 0.0500	time 0.3392 (0.3197)	loss 1.5583 (1.1702)	grad_norm 0.3415 (0.3542)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:00:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:07:59 lr 0.000016	 wd 0.0500	time 0.2943 (0.3191)	loss 1.3969 (1.1724)	grad_norm 0.3491 (0.3558)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:00:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:26 lr 0.000016	 wd 0.0500	time 0.3067 (0.3182)	loss 0.8260 (1.1739)	grad_norm 0.3591 (0.3549)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:01:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:06:53 lr 0.000016	 wd 0.0500	time 0.2992 (0.3173)	loss 1.4641 (1.1721)	grad_norm 0.3411 (0.3541)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:01:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:20 lr 0.000016	 wd 0.0500	time 0.3054 (0.3165)	loss 1.0600 (1.1731)	grad_norm 0.3266 (0.3538)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:02:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:05:48 lr 0.000016	 wd 0.0500	time 0.2874 (0.3159)	loss 1.3519 (1.1728)	grad_norm 0.3282 (0.3530)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:15 lr 0.000016	 wd 0.0500	time 0.2894 (0.3153)	loss 0.8164 (1.1702)	grad_norm 0.3587 (0.3532)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:03:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:04:44 lr 0.000016	 wd 0.0500	time 0.3162 (0.3150)	loss 1.3841 (1.1669)	grad_norm 0.3233 (0.3526)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:03:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:12 lr 0.000016	 wd 0.0500	time 0.3324 (0.3147)	loss 1.0817 (1.1691)	grad_norm 0.3588 (0.3523)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 12:04:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:40 lr 0.000016	 wd 0.0500	time 0.2870 (0.3144)	loss 0.8396 (1.1676)	grad_norm 0.3543 (nan)	loss_scale 2048.0000 (4086.9028)	mem 16679MB
[2024-08-01 12:04:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:09 lr 0.000016	 wd 0.0500	time 0.3170 (0.3143)	loss 1.3286 (1.1683)	grad_norm 0.3436 (nan)	loss_scale 2048.0000 (3979.6486)	mem 16679MB
[2024-08-01 12:05:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:37 lr 0.000016	 wd 0.0500	time 0.2963 (0.3142)	loss 1.0696 (1.1692)	grad_norm 0.3472 (nan)	loss_scale 2048.0000 (3883.1144)	mem 16679MB
[2024-08-01 12:05:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:06 lr 0.000016	 wd 0.0500	time 0.3196 (0.3141)	loss 0.8230 (1.1685)	grad_norm 0.3061 (nan)	loss_scale 2048.0000 (3795.7696)	mem 16679MB
[2024-08-01 12:06:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:34 lr 0.000016	 wd 0.0500	time 0.3161 (0.3139)	loss 0.8241 (1.1700)	grad_norm 0.3498 (nan)	loss_scale 2048.0000 (3716.3617)	mem 16679MB
[2024-08-01 12:06:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:03 lr 0.000015	 wd 0.0500	time 0.3180 (0.3138)	loss 1.2154 (1.1706)	grad_norm 0.3490 (nan)	loss_scale 2048.0000 (3643.8557)	mem 16679MB
[2024-08-01 12:07:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:31 lr 0.000015	 wd 0.0500	time 0.3430 (0.3136)	loss 1.3796 (1.1708)	grad_norm 0.3227 (nan)	loss_scale 2048.0000 (3577.3894)	mem 16679MB
[2024-08-01 12:07:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.2839 (0.3129)	loss 0.9497 (1.1701)	grad_norm 0.3473 (nan)	loss_scale 2048.0000 (3516.2383)	mem 16679MB
[2024-08-01 12:07:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 12 training takes 0:13:05
[2024-08-01 12:08:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.186 (12.186)	Loss 0.5137 (0.5137)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 12:08:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.690 Acc@5 97.716
[2024-08-01 12:08:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 12:08:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 12:08:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:14:02 lr 0.000015	 wd 0.0500	time 11.8474 (11.8474)	loss 1.3490 (1.3490)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:08:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:16:48 lr 0.000015	 wd 0.0500	time 0.3435 (0.4198)	loss 1.3737 (1.2355)	grad_norm 0.3394 (nan)	loss_scale 1024.0000 (1318.0198)	mem 16679MB
[2024-08-01 12:09:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:13:54 lr 0.000015	 wd 0.0500	time 0.3121 (0.3623)	loss 1.4140 (1.2228)	grad_norm 0.3306 (nan)	loss_scale 1024.0000 (1171.7413)	mem 16679MB
[2024-08-01 12:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:12:34 lr 0.000015	 wd 0.0500	time 0.2974 (0.3427)	loss 1.4024 (1.2230)	grad_norm 0.3367 (nan)	loss_scale 1024.0000 (1122.6578)	mem 16679MB
[2024-08-01 12:10:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:11:39 lr 0.000015	 wd 0.0500	time 0.2911 (0.3329)	loss 1.4768 (1.2032)	grad_norm 0.3501 (nan)	loss_scale 1024.0000 (1098.0549)	mem 16679MB
[2024-08-01 12:11:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:10:56 lr 0.000015	 wd 0.0500	time 0.2956 (0.3279)	loss 1.3101 (1.1965)	grad_norm 0.3650 (nan)	loss_scale 1024.0000 (1083.2735)	mem 16679MB
[2024-08-01 12:11:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:10:17 lr 0.000015	 wd 0.0500	time 0.2884 (0.3244)	loss 1.1977 (1.1905)	grad_norm 0.3380 (nan)	loss_scale 1024.0000 (1073.4110)	mem 16679MB
[2024-08-01 12:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:09:40 lr 0.000015	 wd 0.0500	time 0.2937 (0.3222)	loss 1.1993 (1.1813)	grad_norm 0.3819 (nan)	loss_scale 1024.0000 (1066.3623)	mem 16679MB
[2024-08-01 12:12:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:05 lr 0.000015	 wd 0.0500	time 0.3039 (0.3204)	loss 1.2466 (1.1805)	grad_norm 0.3427 (nan)	loss_scale 1024.0000 (1061.0737)	mem 16679MB
[2024-08-01 12:13:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:08:30 lr 0.000015	 wd 0.0500	time 0.2914 (0.3187)	loss 1.3260 (1.1811)	grad_norm 0.3495 (nan)	loss_scale 1024.0000 (1056.9589)	mem 16679MB
[2024-08-01 12:13:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:07:57 lr 0.000015	 wd 0.0500	time 0.2883 (0.3177)	loss 1.3941 (1.1799)	grad_norm 0.3322 (nan)	loss_scale 1024.0000 (1053.6663)	mem 16679MB
[2024-08-01 12:14:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:24 lr 0.000015	 wd 0.0500	time 0.3355 (0.3170)	loss 1.4073 (1.1759)	grad_norm 0.3427 (nan)	loss_scale 1024.0000 (1050.9718)	mem 16679MB
[2024-08-01 12:14:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:06:51 lr 0.000015	 wd 0.0500	time 0.3343 (0.3162)	loss 1.1934 (1.1800)	grad_norm 0.3571 (nan)	loss_scale 1024.0000 (1048.7261)	mem 16679MB
[2024-08-01 12:15:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:19 lr 0.000015	 wd 0.0500	time 0.3007 (0.3157)	loss 0.7354 (1.1786)	grad_norm 0.3597 (nan)	loss_scale 1024.0000 (1046.8255)	mem 16679MB
[2024-08-01 12:15:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:05:47 lr 0.000015	 wd 0.0500	time 0.2973 (0.3151)	loss 1.3832 (1.1772)	grad_norm 0.3544 (nan)	loss_scale 1024.0000 (1045.1963)	mem 16679MB
[2024-08-01 12:16:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:15 lr 0.000015	 wd 0.0500	time 0.3015 (0.3145)	loss 1.2340 (1.1779)	grad_norm 0.3480 (nan)	loss_scale 1024.0000 (1043.7841)	mem 16679MB
[2024-08-01 12:16:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:04:43 lr 0.000015	 wd 0.0500	time 0.2891 (0.3139)	loss 0.9323 (1.1768)	grad_norm 0.3472 (nan)	loss_scale 1024.0000 (1042.5484)	mem 16679MB
[2024-08-01 12:17:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:11 lr 0.000015	 wd 0.0500	time 0.3113 (0.3135)	loss 1.0502 (1.1780)	grad_norm 0.3346 (nan)	loss_scale 1024.0000 (1041.4580)	mem 16679MB
[2024-08-01 12:17:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:39 lr 0.000015	 wd 0.0500	time 0.2890 (0.3133)	loss 1.2576 (1.1784)	grad_norm 0.3392 (nan)	loss_scale 1024.0000 (1040.4886)	mem 16679MB
[2024-08-01 12:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:08 lr 0.000015	 wd 0.0500	time 0.3292 (0.3131)	loss 1.0945 (1.1785)	grad_norm 0.3443 (nan)	loss_scale 1024.0000 (1039.6213)	mem 16679MB
[2024-08-01 12:18:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:37 lr 0.000015	 wd 0.0500	time 0.3057 (0.3130)	loss 1.4047 (1.1783)	grad_norm 0.3221 (nan)	loss_scale 1024.0000 (1038.8406)	mem 16679MB
[2024-08-01 12:19:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:05 lr 0.000014	 wd 0.0500	time 0.3327 (0.3131)	loss 1.3806 (1.1784)	grad_norm 0.3432 (nan)	loss_scale 1024.0000 (1038.1342)	mem 16679MB
[2024-08-01 12:19:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:34 lr 0.000014	 wd 0.0500	time 0.2952 (0.3135)	loss 1.2138 (1.1770)	grad_norm 0.3360 (nan)	loss_scale 1024.0000 (1037.4920)	mem 16679MB
[2024-08-01 12:20:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:03 lr 0.000014	 wd 0.0500	time 0.3107 (0.3135)	loss 1.3686 (1.1759)	grad_norm 0.3574 (nan)	loss_scale 1024.0000 (1036.9057)	mem 16679MB
[2024-08-01 12:20:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:31 lr 0.000014	 wd 0.0500	time 0.3138 (0.3135)	loss 0.9492 (1.1787)	grad_norm 0.3192 (nan)	loss_scale 1024.0000 (1036.3682)	mem 16679MB
[2024-08-01 12:21:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.2911 (0.3129)	loss 0.7928 (1.1774)	grad_norm 0.3377 (nan)	loss_scale 1024.0000 (1035.8737)	mem 16679MB
[2024-08-01 12:21:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 13 training takes 0:13:05
[2024-08-01 12:21:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.860 (11.860)	Loss 0.4895 (0.4895)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 12:21:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.724 Acc@5 97.756
[2024-08-01 12:21:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 12:21:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 12:21:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:23:00 lr 0.000014	 wd 0.0500	time 10.6235 (10.6235)	loss 1.4296 (1.4296)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:22:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:16:39 lr 0.000014	 wd 0.0500	time 0.2927 (0.4161)	loss 1.1981 (1.1948)	grad_norm 0.3385 (0.3773)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:23:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:13:50 lr 0.000014	 wd 0.0500	time 0.3136 (0.3607)	loss 1.4864 (1.1936)	grad_norm 0.3293 (0.3615)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:23:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:12:33 lr 0.000014	 wd 0.0500	time 0.2910 (0.3423)	loss 0.9454 (1.1964)	grad_norm 0.4839 (0.3603)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:24:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:11:40 lr 0.000014	 wd 0.0500	time 0.3130 (0.3335)	loss 1.6065 (1.1957)	grad_norm 0.3486 (0.3572)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:24:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:10:57 lr 0.000014	 wd 0.0500	time 0.2909 (0.3284)	loss 0.9612 (1.1911)	grad_norm 0.3310 (0.3592)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:25:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:18 lr 0.000014	 wd 0.0500	time 0.2951 (0.3250)	loss 0.9774 (1.1900)	grad_norm 0.3546 (0.3584)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:25:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:09:42 lr 0.000014	 wd 0.0500	time 0.3089 (0.3231)	loss 1.1414 (1.1906)	grad_norm 0.3534 (0.3579)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:26:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:06 lr 0.000014	 wd 0.0500	time 0.3191 (0.3211)	loss 0.8097 (1.1845)	grad_norm 0.3345 (0.3562)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:26:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:08:31 lr 0.000014	 wd 0.0500	time 0.3332 (0.3195)	loss 0.8629 (1.1833)	grad_norm 0.3513 (0.3557)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:07:57 lr 0.000014	 wd 0.0500	time 0.2887 (0.3179)	loss 1.0053 (1.1807)	grad_norm 0.3424 (0.3552)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:27:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:24 lr 0.000014	 wd 0.0500	time 0.3102 (0.3169)	loss 1.3928 (1.1802)	grad_norm 0.3627 (0.3545)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:28:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:06:51 lr 0.000014	 wd 0.0500	time 0.2984 (0.3161)	loss 1.4891 (1.1798)	grad_norm 0.3925 (0.3542)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:28:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:19 lr 0.000014	 wd 0.0500	time 0.2859 (0.3154)	loss 1.2562 (1.1832)	grad_norm 0.3464 (0.3539)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:29:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:05:47 lr 0.000014	 wd 0.0500	time 0.2892 (0.3150)	loss 1.2591 (1.1828)	grad_norm 0.3459 (0.3540)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:29:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:15 lr 0.000014	 wd 0.0500	time 0.3067 (0.3147)	loss 1.4211 (1.1818)	grad_norm 0.3341 (0.3530)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 12:30:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:04:43 lr 0.000014	 wd 0.0500	time 0.3122 (0.3144)	loss 0.7700 (1.1806)	grad_norm 0.3415 (0.3527)	loss_scale 2048.0000 (1071.3304)	mem 16679MB
[2024-08-01 12:30:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:11 lr 0.000014	 wd 0.0500	time 0.2880 (0.3139)	loss 0.9499 (1.1795)	grad_norm 0.3349 (0.3533)	loss_scale 2048.0000 (1128.7478)	mem 16679MB
[2024-08-01 12:31:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:40 lr 0.000013	 wd 0.0500	time 0.2902 (0.3138)	loss 0.9161 (1.1801)	grad_norm 0.3925 (0.3531)	loss_scale 2048.0000 (1179.7890)	mem 16679MB
[2024-08-01 12:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:08 lr 0.000013	 wd 0.0500	time 0.3124 (0.3138)	loss 1.1865 (1.1804)	grad_norm 0.3663 (0.3526)	loss_scale 2048.0000 (1225.4603)	mem 16679MB
[2024-08-01 12:32:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:37 lr 0.000013	 wd 0.0500	time 0.2979 (0.3137)	loss 1.2397 (1.1797)	grad_norm 0.3406 (0.3524)	loss_scale 2048.0000 (1266.5667)	mem 16679MB
[2024-08-01 12:32:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:06 lr 0.000013	 wd 0.0500	time 0.3079 (0.3136)	loss 1.3778 (1.1791)	grad_norm 0.3444 (0.3542)	loss_scale 2048.0000 (1303.7601)	mem 16679MB
[2024-08-01 12:33:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:34 lr 0.000013	 wd 0.0500	time 0.2997 (0.3136)	loss 1.3259 (1.1782)	grad_norm 0.3434 (0.3539)	loss_scale 2048.0000 (1337.5738)	mem 16679MB
[2024-08-01 12:33:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:03 lr 0.000013	 wd 0.0500	time 0.3304 (0.3136)	loss 0.8503 (1.1770)	grad_norm 0.3389 (0.3534)	loss_scale 2048.0000 (1368.4485)	mem 16679MB
[2024-08-01 12:34:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:31 lr 0.000013	 wd 0.0500	time 0.2983 (0.3135)	loss 0.8769 (1.1762)	grad_norm 0.3620 (0.3532)	loss_scale 2048.0000 (1396.7514)	mem 16679MB
[2024-08-01 12:34:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2844 (0.3129)	loss 1.2781 (1.1750)	grad_norm 0.3363 (0.3531)	loss_scale 2048.0000 (1422.7909)	mem 16679MB
[2024-08-01 12:34:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 14 training takes 0:13:05
[2024-08-01 12:35:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.226 (11.226)	Loss 0.4805 (0.4805)	Acc@1 93.555 (93.555)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 12:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.742 Acc@5 97.780
[2024-08-01 12:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 12:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 12:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 12:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 12:35:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:35:46 lr 0.000013	 wd 0.0500	time 10.9298 (10.9298)	loss 1.2884 (1.2884)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:36:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:16:31 lr 0.000013	 wd 0.0500	time 0.3140 (0.4128)	loss 0.7953 (1.1823)	grad_norm 0.3466 (0.3598)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:36:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:13:46 lr 0.000013	 wd 0.0500	time 0.2929 (0.3591)	loss 1.4653 (1.1673)	grad_norm 0.3484 (0.3563)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:37:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:12:29 lr 0.000013	 wd 0.0500	time 0.2919 (0.3406)	loss 0.9267 (1.1666)	grad_norm 0.3444 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:37:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:11:36 lr 0.000013	 wd 0.0500	time 0.2916 (0.3314)	loss 0.8131 (1.1653)	grad_norm 0.3426 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:38:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:10:53 lr 0.000013	 wd 0.0500	time 0.2902 (0.3266)	loss 0.9609 (1.1767)	grad_norm 0.3546 (0.3567)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:38:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:10:15 lr 0.000013	 wd 0.0500	time 0.3520 (0.3234)	loss 1.4172 (1.1795)	grad_norm 0.3529 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:39:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:09:38 lr 0.000013	 wd 0.0500	time 0.3207 (0.3212)	loss 1.2566 (1.1794)	grad_norm 0.3449 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:39:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:03 lr 0.000013	 wd 0.0500	time 0.2887 (0.3193)	loss 1.4208 (1.1810)	grad_norm 0.3371 (0.3526)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:40:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:08:29 lr 0.000013	 wd 0.0500	time 0.3074 (0.3178)	loss 1.2939 (1.1807)	grad_norm 0.3459 (0.3519)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:40:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:07:55 lr 0.000013	 wd 0.0500	time 0.2976 (0.3165)	loss 0.8605 (1.1807)	grad_norm 0.3348 (0.3520)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:41:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:22 lr 0.000013	 wd 0.0500	time 0.2998 (0.3155)	loss 0.8794 (1.1815)	grad_norm 0.3356 (0.3514)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:41:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:06:50 lr 0.000013	 wd 0.0500	time 0.2901 (0.3153)	loss 0.8355 (1.1815)	grad_norm 0.3619 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:42:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:18 lr 0.000013	 wd 0.0500	time 0.2966 (0.3149)	loss 0.8820 (1.1826)	grad_norm 0.3393 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:42:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:05:46 lr 0.000012	 wd 0.0500	time 0.2986 (0.3144)	loss 0.9788 (1.1827)	grad_norm 0.3465 (0.3527)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:43:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:14 lr 0.000012	 wd 0.0500	time 0.2901 (0.3141)	loss 1.0277 (1.1788)	grad_norm 0.3453 (0.3527)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:43:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:04:42 lr 0.000012	 wd 0.0500	time 0.2848 (0.3136)	loss 1.0244 (1.1797)	grad_norm 0.3505 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:44:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:11 lr 0.000012	 wd 0.0500	time 0.2881 (0.3131)	loss 0.8376 (1.1798)	grad_norm 0.3511 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:44:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:39 lr 0.000012	 wd 0.0500	time 0.3109 (0.3130)	loss 1.6849 (1.1778)	grad_norm 0.4026 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:45:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:08 lr 0.000012	 wd 0.0500	time 0.2978 (0.3129)	loss 0.9909 (1.1760)	grad_norm 0.3683 (0.3552)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:45:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:36 lr 0.000012	 wd 0.0500	time 0.3344 (0.3127)	loss 0.8546 (1.1773)	grad_norm 0.3458 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:46:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:05 lr 0.000012	 wd 0.0500	time 0.2915 (0.3126)	loss 0.7970 (1.1761)	grad_norm 0.3437 (0.3562)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:34 lr 0.000012	 wd 0.0500	time 0.3082 (0.3125)	loss 1.0921 (1.1757)	grad_norm 0.3289 (0.3557)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:47:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:03 lr 0.000012	 wd 0.0500	time 0.3125 (0.3123)	loss 1.0395 (1.1759)	grad_norm 0.4595 (0.3557)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:47:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:31 lr 0.000012	 wd 0.0500	time 0.3117 (0.3123)	loss 1.4844 (1.1763)	grad_norm 0.3389 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:48:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2881 (0.3118)	loss 1.5381 (1.1769)	grad_norm 0.3541 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:48:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 15 training takes 0:13:02
[2024-08-01 12:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.520 (12.520)	Loss 0.5312 (0.5312)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 12:48:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.712 Acc@5 97.744
[2024-08-01 12:48:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 12:48:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 12:49:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:44:12 lr 0.000012	 wd 0.0500	time 11.1319 (11.1319)	loss 1.2201 (1.2201)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:49:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:16:36 lr 0.000012	 wd 0.0500	time 0.2894 (0.4150)	loss 1.0559 (1.1899)	grad_norm 0.3501 (0.4404)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:50:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:13:47 lr 0.000012	 wd 0.0500	time 0.3066 (0.3597)	loss 1.2792 (1.1787)	grad_norm 0.3408 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:50:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:12:40 lr 0.000012	 wd 0.0500	time 0.2892 (0.3453)	loss 1.0276 (1.1715)	grad_norm 0.3736 (0.3912)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:51:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:11:44 lr 0.000012	 wd 0.0500	time 0.3035 (0.3352)	loss 0.9196 (1.1595)	grad_norm 0.3391 (0.3795)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:10:59 lr 0.000012	 wd 0.0500	time 0.2959 (0.3297)	loss 1.5548 (1.1620)	grad_norm 0.3563 (0.3742)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 12:52:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:10:19 lr 0.000012	 wd 0.0500	time 0.2867 (0.3255)	loss 1.4558 (1.1628)	grad_norm 0.3196 (0.3708)	loss_scale 4096.0000 (2313.7970)	mem 16679MB
[2024-08-01 12:52:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:09:42 lr 0.000012	 wd 0.0500	time 0.3396 (0.3233)	loss 1.0759 (1.1654)	grad_norm 0.3404 (0.3677)	loss_scale 4096.0000 (2568.0342)	mem 16679MB
[2024-08-01 12:53:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:07 lr 0.000012	 wd 0.0500	time 0.3098 (0.3215)	loss 1.3401 (1.1669)	grad_norm 0.3536 (0.3664)	loss_scale 4096.0000 (2758.7915)	mem 16679MB
[2024-08-01 12:53:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:08:32 lr 0.000012	 wd 0.0500	time 0.3102 (0.3196)	loss 1.1711 (1.1678)	grad_norm 0.3450 (0.3665)	loss_scale 4096.0000 (2907.2053)	mem 16679MB
[2024-08-01 12:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:07:58 lr 0.000011	 wd 0.0500	time 0.3149 (0.3184)	loss 1.3603 (1.1677)	grad_norm 0.3402 (0.3646)	loss_scale 4096.0000 (3025.9660)	mem 16679MB
[2024-08-01 12:54:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:25 lr 0.000011	 wd 0.0500	time 0.3187 (0.3175)	loss 0.8473 (1.1718)	grad_norm 0.3690 (0.3633)	loss_scale 4096.0000 (3123.1535)	mem 16679MB
[2024-08-01 12:55:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:06:52 lr 0.000011	 wd 0.0500	time 0.2916 (0.3167)	loss 0.8741 (1.1696)	grad_norm 0.3385 (0.3620)	loss_scale 4096.0000 (3204.1565)	mem 16679MB
[2024-08-01 12:55:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:19 lr 0.000011	 wd 0.0500	time 0.3024 (0.3159)	loss 1.3080 (1.1725)	grad_norm 0.3360 (0.3611)	loss_scale 4096.0000 (3272.7071)	mem 16679MB
[2024-08-01 12:56:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:05:47 lr 0.000011	 wd 0.0500	time 0.3062 (0.3154)	loss 1.2568 (1.1724)	grad_norm 0.3740 (0.3604)	loss_scale 4096.0000 (3331.4718)	mem 16679MB
[2024-08-01 12:56:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:15 lr 0.000011	 wd 0.0500	time 0.2935 (0.3148)	loss 1.3143 (1.1748)	grad_norm 0.3319 (0.3594)	loss_scale 4096.0000 (3382.4064)	mem 16679MB
[2024-08-01 12:57:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:04:44 lr 0.000011	 wd 0.0500	time 0.3186 (0.3153)	loss 1.1342 (1.1754)	grad_norm 0.3622 (0.3588)	loss_scale 4096.0000 (3426.9781)	mem 16679MB
[2024-08-01 12:57:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:12 lr 0.000011	 wd 0.0500	time 0.2917 (0.3150)	loss 1.0921 (1.1745)	grad_norm 0.3551 (0.3581)	loss_scale 4096.0000 (3466.3092)	mem 16679MB
[2024-08-01 12:58:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:40 lr 0.000011	 wd 0.0500	time 0.3279 (0.3147)	loss 0.8670 (1.1733)	grad_norm 0.3192 (0.3579)	loss_scale 4096.0000 (3501.2726)	mem 16679MB
[2024-08-01 12:58:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:09 lr 0.000011	 wd 0.0500	time 0.3121 (0.3144)	loss 1.0395 (1.1740)	grad_norm 0.3317 (0.3575)	loss_scale 4096.0000 (3532.5576)	mem 16679MB
[2024-08-01 12:59:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:37 lr 0.000011	 wd 0.0500	time 0.2935 (0.3143)	loss 1.3641 (1.1747)	grad_norm 0.3246 (0.3569)	loss_scale 4096.0000 (3560.7156)	mem 16679MB
[2024-08-01 12:59:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:06 lr 0.000011	 wd 0.0500	time 0.3299 (0.3142)	loss 1.3672 (1.1755)	grad_norm 0.3287 (0.3562)	loss_scale 4096.0000 (3586.1932)	mem 16679MB
[2024-08-01 13:00:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:34 lr 0.000011	 wd 0.0500	time 0.3139 (0.3140)	loss 1.0214 (1.1750)	grad_norm 0.3469 (0.3563)	loss_scale 4096.0000 (3609.3557)	mem 16679MB
[2024-08-01 13:00:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:03 lr 0.000011	 wd 0.0500	time 0.2910 (0.3139)	loss 0.9061 (1.1732)	grad_norm 0.3558 (0.3558)	loss_scale 4096.0000 (3630.5050)	mem 16679MB
[2024-08-01 13:01:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:32 lr 0.000011	 wd 0.0500	time 0.3259 (0.3137)	loss 1.2419 (1.1723)	grad_norm 0.3344 (0.3553)	loss_scale 4096.0000 (3649.8925)	mem 16679MB
[2024-08-01 13:01:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2848 (0.3130)	loss 0.8610 (1.1726)	grad_norm 0.3554 (0.3553)	loss_scale 4096.0000 (3667.7297)	mem 16679MB
[2024-08-01 13:02:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 16 training takes 0:13:05
[2024-08-01 13:02:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.343 (11.343)	Loss 0.5024 (0.5024)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 13:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.730 Acc@5 97.740
[2024-08-01 13:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 13:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 13:02:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:01:09 lr 0.000011	 wd 0.0500	time 10.0997 (10.0997)	loss 1.4121 (1.4121)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:03:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:16:38 lr 0.000011	 wd 0.0500	time 0.3390 (0.4158)	loss 1.2542 (1.1557)	grad_norm 0.3295 (0.3493)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:03:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:13:45 lr 0.000011	 wd 0.0500	time 0.3307 (0.3588)	loss 1.3050 (1.1547)	grad_norm 0.3357 (0.3590)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:04:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:12:30 lr 0.000011	 wd 0.0500	time 0.2885 (0.3409)	loss 1.2549 (1.1705)	grad_norm 0.3426 (0.3587)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:04:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:11:39 lr 0.000011	 wd 0.0500	time 0.2908 (0.3327)	loss 1.4272 (1.1687)	grad_norm 0.3402 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:05:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:10:56 lr 0.000010	 wd 0.0500	time 0.2856 (0.3278)	loss 1.3790 (1.1733)	grad_norm 0.4318 (0.3565)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:05:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:16 lr 0.000010	 wd 0.0500	time 0.2901 (0.3241)	loss 0.8155 (1.1744)	grad_norm 0.3452 (0.3556)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:06:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:09:39 lr 0.000010	 wd 0.0500	time 0.3323 (0.3214)	loss 1.3054 (1.1721)	grad_norm 0.3445 (0.3562)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:06:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:03 lr 0.000010	 wd 0.0500	time 0.3122 (0.3195)	loss 1.2133 (1.1703)	grad_norm 0.3659 (0.3556)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:07:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:08:29 lr 0.000010	 wd 0.0500	time 0.3119 (0.3182)	loss 1.3046 (1.1722)	grad_norm 0.3424 (0.3550)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:07:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:07:56 lr 0.000010	 wd 0.0500	time 0.2981 (0.3170)	loss 1.0414 (1.1716)	grad_norm 0.3573 (0.3563)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:08:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:23 lr 0.000010	 wd 0.0500	time 0.3070 (0.3161)	loss 1.0428 (1.1761)	grad_norm 0.3323 (0.3584)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:08:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:06:50 lr 0.000010	 wd 0.0500	time 0.3021 (0.3153)	loss 0.7778 (1.1761)	grad_norm 0.3283 (0.3575)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:09:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:18 lr 0.000010	 wd 0.0500	time 0.2995 (0.3148)	loss 1.4440 (1.1733)	grad_norm 0.3500 (0.3565)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:09:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:46 lr 0.000010	 wd 0.0500	time 0.3015 (0.3143)	loss 1.3393 (1.1734)	grad_norm 0.3218 (0.3566)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:10:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:14 lr 0.000010	 wd 0.0500	time 0.3114 (0.3139)	loss 1.4615 (1.1739)	grad_norm 0.3622 (0.3565)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:10:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:43 lr 0.000010	 wd 0.0500	time 0.3003 (0.3142)	loss 1.4107 (1.1738)	grad_norm 0.3324 (0.3560)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:11:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:11 lr 0.000010	 wd 0.0500	time 0.2887 (0.3139)	loss 1.5566 (1.1757)	grad_norm 0.3280 (0.3558)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:11:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:40 lr 0.000010	 wd 0.0500	time 0.3104 (0.3135)	loss 0.9797 (1.1750)	grad_norm 0.3614 (0.3554)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:12:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:08 lr 0.000010	 wd 0.0500	time 0.2853 (0.3132)	loss 1.2685 (1.1741)	grad_norm 0.3347 (0.3549)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:12:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:37 lr 0.000010	 wd 0.0500	time 0.3358 (0.3130)	loss 1.2161 (1.1744)	grad_norm 0.3445 (0.3545)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:13:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:05 lr 0.000010	 wd 0.0500	time 0.3079 (0.3127)	loss 1.4837 (1.1742)	grad_norm 0.3815 (nan)	loss_scale 4096.0000 (4166.1837)	mem 16679MB
[2024-08-01 13:13:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:34 lr 0.000010	 wd 0.0500	time 0.2983 (0.3128)	loss 1.3995 (1.1740)	grad_norm 0.4553 (nan)	loss_scale 4096.0000 (4162.9950)	mem 16679MB
[2024-08-01 13:14:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:03 lr 0.000010	 wd 0.0500	time 0.3246 (0.3127)	loss 0.8130 (1.1735)	grad_norm 0.3840 (nan)	loss_scale 4096.0000 (4160.0834)	mem 16679MB
[2024-08-01 13:14:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:31 lr 0.000010	 wd 0.0500	time 0.3413 (0.3124)	loss 0.9472 (1.1721)	grad_norm 0.3586 (nan)	loss_scale 4096.0000 (4157.4144)	mem 16679MB
[2024-08-01 13:15:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2843 (0.3118)	loss 1.4763 (1.1718)	grad_norm 0.4887 (nan)	loss_scale 4096.0000 (4154.9588)	mem 16679MB
[2024-08-01 13:15:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 17 training takes 0:13:02
[2024-08-01 13:15:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.995 (11.995)	Loss 0.4949 (0.4949)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 13:15:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.728 Acc@5 97.736
[2024-08-01 13:15:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 13:15:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-01 13:16:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][0/2502]	eta 6:44:26 lr 0.000009	 wd 0.0500	time 9.6987 (9.6987)	loss 1.4769 (1.4769)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:16:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:16:38 lr 0.000009	 wd 0.0500	time 0.2942 (0.4157)	loss 1.5031 (1.2079)	grad_norm 0.3451 (0.3613)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:03 lr 0.000009	 wd 0.0500	time 0.3012 (0.3664)	loss 1.0367 (1.1979)	grad_norm 0.3854 (0.3532)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:17:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:12:40 lr 0.000009	 wd 0.0500	time 0.2944 (0.3452)	loss 1.5221 (1.1971)	grad_norm 0.3385 (nan)	loss_scale 2048.0000 (3783.0166)	mem 16679MB
[2024-08-01 13:18:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:11:43 lr 0.000009	 wd 0.0500	time 0.2971 (0.3347)	loss 0.7543 (1.1958)	grad_norm 0.4233 (nan)	loss_scale 2048.0000 (3350.3441)	mem 16679MB
[2024-08-01 13:18:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:10:57 lr 0.000009	 wd 0.0500	time 0.2901 (0.3284)	loss 1.3122 (1.1871)	grad_norm 0.3206 (nan)	loss_scale 2048.0000 (3090.3952)	mem 16679MB
[2024-08-01 13:19:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:10:16 lr 0.000009	 wd 0.0500	time 0.2853 (0.3243)	loss 1.5060 (1.1793)	grad_norm 0.3429 (nan)	loss_scale 2048.0000 (2916.9517)	mem 16679MB
[2024-08-01 13:19:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:09:40 lr 0.000009	 wd 0.0500	time 0.3257 (0.3220)	loss 1.4763 (1.1844)	grad_norm 0.3368 (nan)	loss_scale 2048.0000 (2792.9929)	mem 16679MB
[2024-08-01 13:20:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:06 lr 0.000009	 wd 0.0500	time 0.3047 (0.3210)	loss 1.3357 (1.1876)	grad_norm 0.3685 (nan)	loss_scale 2048.0000 (2699.9850)	mem 16679MB
[2024-08-01 13:20:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:08:35 lr 0.000009	 wd 0.0500	time 0.2873 (0.3218)	loss 0.9425 (1.1874)	grad_norm 0.3474 (nan)	loss_scale 2048.0000 (2627.6226)	mem 16679MB
[2024-08-01 13:21:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:01 lr 0.000009	 wd 0.0500	time 0.3189 (0.3204)	loss 1.4359 (1.1831)	grad_norm 0.3359 (nan)	loss_scale 2048.0000 (2569.7183)	mem 16679MB
[2024-08-01 13:21:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:27 lr 0.000009	 wd 0.0500	time 0.3283 (0.3192)	loss 1.0202 (1.1851)	grad_norm 0.3434 (nan)	loss_scale 2048.0000 (2522.3324)	mem 16679MB
[2024-08-01 13:22:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:06:54 lr 0.000009	 wd 0.0500	time 0.2868 (0.3186)	loss 1.0685 (1.1783)	grad_norm 0.3439 (nan)	loss_scale 2048.0000 (2482.8376)	mem 16679MB
[2024-08-01 13:22:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:21 lr 0.000009	 wd 0.0500	time 0.3077 (0.3177)	loss 1.5057 (1.1786)	grad_norm 0.3519 (nan)	loss_scale 2048.0000 (2449.4143)	mem 16679MB
[2024-08-01 13:23:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:05:49 lr 0.000009	 wd 0.0500	time 0.3063 (0.3171)	loss 1.1749 (1.1790)	grad_norm 0.3285 (nan)	loss_scale 2048.0000 (2420.7623)	mem 16679MB
[2024-08-01 13:23:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:17 lr 0.000009	 wd 0.0500	time 0.3201 (0.3166)	loss 1.3312 (1.1786)	grad_norm 0.3379 (nan)	loss_scale 2048.0000 (2395.9280)	mem 16679MB
[2024-08-01 13:24:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:04:45 lr 0.000009	 wd 0.0500	time 0.3144 (0.3160)	loss 1.4713 (1.1804)	grad_norm 0.3555 (nan)	loss_scale 2048.0000 (2374.1961)	mem 16679MB
[2024-08-01 13:24:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:13 lr 0.000009	 wd 0.0500	time 0.2985 (0.3157)	loss 1.3453 (1.1823)	grad_norm 0.3492 (nan)	loss_scale 2048.0000 (2355.0194)	mem 16679MB
[2024-08-01 13:25:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:41 lr 0.000009	 wd 0.0500	time 0.3322 (0.3155)	loss 0.7285 (1.1826)	grad_norm 0.3393 (nan)	loss_scale 2048.0000 (2337.9722)	mem 16679MB
[2024-08-01 13:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:09 lr 0.000009	 wd 0.0500	time 0.3193 (0.3152)	loss 1.4386 (1.1823)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (2322.7186)	mem 16679MB
[2024-08-01 13:26:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:38 lr 0.000008	 wd 0.0500	time 0.3010 (0.3151)	loss 1.0893 (1.1819)	grad_norm 0.3379 (nan)	loss_scale 2048.0000 (2308.9895)	mem 16679MB
[2024-08-01 13:27:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:06 lr 0.000008	 wd 0.0500	time 0.2938 (0.3146)	loss 1.3634 (1.1808)	grad_norm 0.3465 (nan)	loss_scale 2048.0000 (2296.5673)	mem 16679MB
[2024-08-01 13:27:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.3134 (0.3145)	loss 1.0412 (1.1809)	grad_norm 0.3386 (nan)	loss_scale 2048.0000 (2285.2740)	mem 16679MB
[2024-08-01 13:28:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.3140 (0.3144)	loss 1.3333 (1.1816)	grad_norm 0.3353 (nan)	loss_scale 2048.0000 (2274.9622)	mem 16679MB
[2024-08-01 13:28:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.3153 (0.3141)	loss 1.4428 (1.1822)	grad_norm 0.3636 (nan)	loss_scale 2048.0000 (2265.5094)	mem 16679MB
[2024-08-01 13:29:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2958 (0.3134)	loss 1.2673 (1.1824)	grad_norm 0.3380 (nan)	loss_scale 2048.0000 (2256.8125)	mem 16679MB
[2024-08-01 13:29:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 18 training takes 0:13:06
[2024-08-01 13:29:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.417 (12.417)	Loss 0.5039 (0.5039)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 13:29:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.768 Acc@5 97.746
[2024-08-01 13:29:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 13:29:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.77%
[2024-08-01 13:29:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 13:29:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 13:29:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:46:19 lr 0.000008	 wd 0.0500	time 11.1827 (11.1827)	loss 0.8035 (0.8035)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:30:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:16:27 lr 0.000008	 wd 0.0500	time 0.3002 (0.4110)	loss 1.6124 (1.2293)	grad_norm 0.3347 (0.3455)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:30:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:13:48 lr 0.000008	 wd 0.0500	time 0.3097 (0.3601)	loss 1.1854 (1.1988)	grad_norm 0.3584 (0.3467)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:31:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:12:32 lr 0.000008	 wd 0.0500	time 0.2884 (0.3416)	loss 1.4283 (1.1852)	grad_norm 0.3446 (0.3483)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:31:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:11:38 lr 0.000008	 wd 0.0500	time 0.2959 (0.3324)	loss 0.9784 (1.1782)	grad_norm 0.3258 (0.3493)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:32:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:10:54 lr 0.000008	 wd 0.0500	time 0.3212 (0.3269)	loss 1.4874 (1.1825)	grad_norm 0.9005 (0.3516)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:32:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:17 lr 0.000008	 wd 0.0500	time 0.2880 (0.3245)	loss 0.8606 (1.1717)	grad_norm 0.3642 (0.3542)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:33:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:09:41 lr 0.000008	 wd 0.0500	time 0.3221 (0.3227)	loss 1.2569 (1.1696)	grad_norm 0.3428 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:33:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:06 lr 0.000008	 wd 0.0500	time 0.3162 (0.3212)	loss 1.5336 (1.1713)	grad_norm 0.3560 (0.3526)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:34:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:08:33 lr 0.000008	 wd 0.0500	time 0.2907 (0.3203)	loss 1.2865 (1.1718)	grad_norm 0.3502 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:34:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:07:59 lr 0.000008	 wd 0.0500	time 0.3043 (0.3196)	loss 1.4572 (1.1732)	grad_norm 0.3422 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:35:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:26 lr 0.000008	 wd 0.0500	time 0.3080 (0.3186)	loss 1.1914 (1.1723)	grad_norm 0.3426 (0.3518)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:35:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:06:53 lr 0.000008	 wd 0.0500	time 0.3145 (0.3180)	loss 1.3460 (1.1718)	grad_norm 0.3501 (0.3525)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:36:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:21 lr 0.000008	 wd 0.0500	time 0.3069 (0.3176)	loss 0.7817 (1.1708)	grad_norm 0.3587 (0.3527)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:37:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:05:51 lr 0.000008	 wd 0.0500	time 0.2888 (0.3186)	loss 0.8324 (1.1699)	grad_norm 0.3609 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:37:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:18 lr 0.000008	 wd 0.0500	time 0.2879 (0.3183)	loss 1.3768 (1.1711)	grad_norm 0.3348 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:38:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:46 lr 0.000007	 wd 0.0500	time 0.2998 (0.3178)	loss 1.4446 (1.1730)	grad_norm 0.3377 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:38:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:14 lr 0.000007	 wd 0.0500	time 0.3401 (0.3172)	loss 0.8913 (1.1735)	grad_norm 0.3527 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 13:39:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:42 lr 0.000007	 wd 0.0500	time 0.3315 (0.3168)	loss 1.3330 (1.1760)	grad_norm 0.3362 (0.3530)	loss_scale 4096.0000 (2102.5830)	mem 16679MB
[2024-08-01 13:39:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:10 lr 0.000007	 wd 0.0500	time 0.2968 (0.3169)	loss 1.1236 (1.1749)	grad_norm 0.3524 (0.3531)	loss_scale 4096.0000 (2207.4445)	mem 16679MB
[2024-08-01 13:40:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:38 lr 0.000007	 wd 0.0500	time 0.2968 (0.3166)	loss 1.4969 (1.1760)	grad_norm 0.3471 (0.3527)	loss_scale 4096.0000 (2301.8251)	mem 16679MB
[2024-08-01 13:40:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:07 lr 0.000007	 wd 0.0500	time 0.2886 (0.3162)	loss 1.5881 (1.1775)	grad_norm 0.3318 (0.3525)	loss_scale 4096.0000 (2387.2213)	mem 16679MB
[2024-08-01 13:41:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:35 lr 0.000007	 wd 0.0500	time 0.2943 (0.3159)	loss 1.1652 (1.1767)	grad_norm 0.3606 (0.3522)	loss_scale 4096.0000 (2464.8578)	mem 16679MB
[2024-08-01 13:41:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:03 lr 0.000007	 wd 0.0500	time 0.2919 (0.3157)	loss 1.0386 (1.1769)	grad_norm 0.3706 (0.3519)	loss_scale 4096.0000 (2535.7462)	mem 16679MB
[2024-08-01 13:42:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:32 lr 0.000007	 wd 0.0500	time 0.3052 (0.3153)	loss 1.1194 (1.1759)	grad_norm 0.3553 (0.3519)	loss_scale 4096.0000 (2600.7297)	mem 16679MB
[2024-08-01 13:42:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2842 (0.3146)	loss 0.8246 (1.1762)	grad_norm 0.3443 (0.3518)	loss_scale 4096.0000 (2660.5166)	mem 16679MB
[2024-08-01 13:42:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 19 training takes 0:13:09
[2024-08-01 13:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.185 (12.185)	Loss 0.4863 (0.4863)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 13:43:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.770 Acc@5 97.740
[2024-08-01 13:43:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 13:43:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.77%
[2024-08-01 13:43:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 13:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 13:43:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:01:47 lr 0.000007	 wd 0.0500	time 11.5538 (11.5538)	loss 1.5588 (1.5588)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:43:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:16:48 lr 0.000007	 wd 0.0500	time 0.3364 (0.4197)	loss 1.0838 (1.1864)	grad_norm 0.3561 (0.3542)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:44:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:13:54 lr 0.000007	 wd 0.0500	time 0.3063 (0.3626)	loss 1.3269 (1.1695)	grad_norm 0.3706 (0.3494)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:44:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:12:32 lr 0.000007	 wd 0.0500	time 0.2856 (0.3418)	loss 0.7142 (1.1816)	grad_norm 0.3594 (0.3522)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:45:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:11:38 lr 0.000007	 wd 0.0500	time 0.2978 (0.3324)	loss 1.3333 (1.1813)	grad_norm 0.3434 (0.3510)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:10:54 lr 0.000007	 wd 0.0500	time 0.2931 (0.3268)	loss 1.3020 (1.1793)	grad_norm 0.3485 (0.3498)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:46:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:10:16 lr 0.000007	 wd 0.0500	time 0.3011 (0.3240)	loss 1.2967 (1.1844)	grad_norm 0.3743 (0.3488)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:46:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:09:39 lr 0.000007	 wd 0.0500	time 0.2909 (0.3217)	loss 1.2882 (1.1824)	grad_norm 0.3433 (0.3498)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:47:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:04 lr 0.000007	 wd 0.0500	time 0.2901 (0.3200)	loss 1.1938 (1.1831)	grad_norm 0.3343 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:48:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:08:30 lr 0.000007	 wd 0.0500	time 0.3453 (0.3186)	loss 1.2704 (1.1824)	grad_norm 0.3495 (0.3506)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:48:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:07:57 lr 0.000007	 wd 0.0500	time 0.3643 (0.3178)	loss 1.4125 (1.1844)	grad_norm 0.3285 (0.3500)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:49:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:24 lr 0.000007	 wd 0.0500	time 0.2901 (0.3172)	loss 1.0906 (1.1823)	grad_norm 0.3532 (0.3559)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:49:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:06:51 lr 0.000006	 wd 0.0500	time 0.3246 (0.3164)	loss 1.2367 (1.1783)	grad_norm 0.3599 (0.3548)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:50:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:19 lr 0.000006	 wd 0.0500	time 0.3201 (0.3157)	loss 1.3638 (1.1778)	grad_norm 0.3501 (0.3541)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:50:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:05:47 lr 0.000006	 wd 0.0500	time 0.2928 (0.3150)	loss 1.0620 (1.1792)	grad_norm 0.3641 (0.3538)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:51:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:15 lr 0.000006	 wd 0.0500	time 0.3025 (0.3147)	loss 1.3550 (1.1820)	grad_norm 0.3314 (0.3538)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:51:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:04:43 lr 0.000006	 wd 0.0500	time 0.3273 (0.3148)	loss 1.3073 (1.1842)	grad_norm 0.3419 (0.3538)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:52:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:12 lr 0.000006	 wd 0.0500	time 0.3164 (0.3146)	loss 1.6331 (1.1854)	grad_norm 0.3270 (0.3535)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:52:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:40 lr 0.000006	 wd 0.0500	time 0.2986 (0.3144)	loss 0.7734 (1.1830)	grad_norm 0.3442 (0.3534)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:53:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:09 lr 0.000006	 wd 0.0500	time 0.3079 (0.3144)	loss 0.8467 (1.1843)	grad_norm 0.3395 (0.3532)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:53:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:37 lr 0.000006	 wd 0.0500	time 0.2976 (0.3142)	loss 1.1120 (1.1834)	grad_norm 0.3654 (0.3532)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:06 lr 0.000006	 wd 0.0500	time 0.2965 (0.3145)	loss 1.2633 (1.1834)	grad_norm 0.3283 (0.3535)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:54:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:34 lr 0.000006	 wd 0.0500	time 0.3061 (0.3144)	loss 1.4101 (1.1846)	grad_norm 0.3413 (0.3534)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:55:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:03 lr 0.000006	 wd 0.0500	time 0.3063 (0.3144)	loss 0.8672 (1.1858)	grad_norm 0.3559 (0.3530)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:55:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:32 lr 0.000006	 wd 0.0500	time 0.2903 (0.3148)	loss 0.7958 (1.1851)	grad_norm 0.3546 (0.3532)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:56:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2841 (0.3141)	loss 1.0644 (1.1870)	grad_norm 0.3421 (0.3534)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:56:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 20 training takes 0:13:08
[2024-08-01 13:56:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_20.pth saving......
[2024-08-01 13:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_20.pth saved !!!
[2024-08-01 13:56:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.601 (11.601)	Loss 0.4968 (0.4968)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 13:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.794 Acc@5 97.748
[2024-08-01 13:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 13:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 13:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 13:56:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 13:57:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:49:18 lr 0.000006	 wd 0.0500	time 11.2543 (11.2543)	loss 0.9863 (0.9863)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:57:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:16:32 lr 0.000006	 wd 0.0500	time 0.2925 (0.4132)	loss 1.3875 (1.2082)	grad_norm 0.3942 (0.3506)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:58:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:13:46 lr 0.000006	 wd 0.0500	time 0.3132 (0.3592)	loss 1.3486 (1.2029)	grad_norm 0.3468 (0.3490)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 13:58:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:12:32 lr 0.000006	 wd 0.0500	time 0.2891 (0.3416)	loss 1.1944 (1.1916)	grad_norm 0.3660 (nan)	loss_scale 2048.0000 (4068.7841)	mem 16679MB
[2024-08-01 13:59:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:11:39 lr 0.000006	 wd 0.0500	time 0.3182 (0.3326)	loss 1.3569 (1.1833)	grad_norm 0.3431 (nan)	loss_scale 2048.0000 (3564.8479)	mem 16679MB
[2024-08-01 13:59:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:10:55 lr 0.000006	 wd 0.0500	time 0.3005 (0.3276)	loss 0.9140 (1.1782)	grad_norm 0.3440 (nan)	loss_scale 2048.0000 (3262.0838)	mem 16679MB
[2024-08-01 14:00:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:16 lr 0.000006	 wd 0.0500	time 0.3136 (0.3242)	loss 1.3138 (1.1735)	grad_norm 2.1807 (nan)	loss_scale 2048.0000 (3060.0732)	mem 16679MB
[2024-08-01 14:00:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:09:41 lr 0.000006	 wd 0.0500	time 0.2965 (0.3225)	loss 1.2488 (1.1709)	grad_norm 0.3352 (nan)	loss_scale 2048.0000 (2915.6976)	mem 16679MB
[2024-08-01 14:01:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:05 lr 0.000006	 wd 0.0500	time 0.3177 (0.3207)	loss 1.2314 (1.1694)	grad_norm 0.3599 (nan)	loss_scale 2048.0000 (2807.3708)	mem 16679MB
[2024-08-01 14:01:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:08:31 lr 0.000005	 wd 0.0500	time 0.3200 (0.3193)	loss 1.4554 (1.1727)	grad_norm 0.3449 (nan)	loss_scale 2048.0000 (2723.0899)	mem 16679MB
[2024-08-01 14:02:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:07:57 lr 0.000005	 wd 0.0500	time 0.3057 (0.3180)	loss 1.5207 (1.1763)	grad_norm 0.3413 (nan)	loss_scale 2048.0000 (2655.6484)	mem 16679MB
[2024-08-01 14:02:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:23 lr 0.000005	 wd 0.0500	time 0.2928 (0.3166)	loss 0.7991 (1.1796)	grad_norm 0.3442 (nan)	loss_scale 2048.0000 (2600.4578)	mem 16679MB
[2024-08-01 14:03:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:06:51 lr 0.000005	 wd 0.0500	time 0.2918 (0.3157)	loss 1.4129 (1.1765)	grad_norm 0.3493 (nan)	loss_scale 2048.0000 (2554.4580)	mem 16679MB
[2024-08-01 14:03:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:19 lr 0.000005	 wd 0.0500	time 0.2880 (0.3154)	loss 1.2183 (1.1769)	grad_norm 0.3370 (nan)	loss_scale 2048.0000 (2515.5296)	mem 16679MB
[2024-08-01 14:04:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:05:47 lr 0.000005	 wd 0.0500	time 0.2930 (0.3150)	loss 0.8331 (1.1741)	grad_norm 0.5648 (nan)	loss_scale 2048.0000 (2482.1585)	mem 16679MB
[2024-08-01 14:04:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:15 lr 0.000005	 wd 0.0500	time 0.2916 (0.3148)	loss 0.8536 (1.1758)	grad_norm 0.3437 (nan)	loss_scale 2048.0000 (2453.2338)	mem 16679MB
[2024-08-01 14:05:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:04:43 lr 0.000005	 wd 0.0500	time 0.2937 (0.3147)	loss 1.0614 (1.1723)	grad_norm 0.3627 (nan)	loss_scale 2048.0000 (2427.9225)	mem 16679MB
[2024-08-01 14:05:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:12 lr 0.000005	 wd 0.0500	time 0.3335 (0.3154)	loss 1.3858 (1.1730)	grad_norm 0.3393 (nan)	loss_scale 2048.0000 (2405.5873)	mem 16679MB
[2024-08-01 14:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:41 lr 0.000005	 wd 0.0500	time 0.2988 (0.3151)	loss 0.7827 (1.1735)	grad_norm 0.3568 (nan)	loss_scale 2048.0000 (2385.7324)	mem 16679MB
[2024-08-01 14:06:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:09 lr 0.000005	 wd 0.0500	time 0.3251 (0.3149)	loss 0.8620 (1.1742)	grad_norm 0.3377 (nan)	loss_scale 2048.0000 (2367.9663)	mem 16679MB
[2024-08-01 14:07:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:37 lr 0.000005	 wd 0.0500	time 0.3332 (0.3146)	loss 1.0749 (1.1732)	grad_norm 0.3669 (nan)	loss_scale 2048.0000 (2351.9760)	mem 16679MB
[2024-08-01 14:07:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:06 lr 0.000005	 wd 0.0500	time 0.3013 (0.3145)	loss 1.1592 (1.1728)	grad_norm 0.3480 (nan)	loss_scale 2048.0000 (2337.5079)	mem 16679MB
[2024-08-01 14:08:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:34 lr 0.000005	 wd 0.0500	time 0.2947 (0.3144)	loss 1.4416 (1.1714)	grad_norm 0.3561 (nan)	loss_scale 2048.0000 (2324.3544)	mem 16679MB
[2024-08-01 14:08:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:03 lr 0.000005	 wd 0.0500	time 0.2895 (0.3143)	loss 1.5586 (1.1731)	grad_norm 0.3631 (nan)	loss_scale 2048.0000 (2312.3442)	mem 16679MB
[2024-08-01 14:09:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:32 lr 0.000005	 wd 0.0500	time 0.2921 (0.3141)	loss 1.2375 (1.1748)	grad_norm 0.3517 (nan)	loss_scale 2048.0000 (2301.3344)	mem 16679MB
[2024-08-01 14:09:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2843 (0.3134)	loss 1.5046 (1.1752)	grad_norm 0.3564 (nan)	loss_scale 2048.0000 (2291.2051)	mem 16679MB
[2024-08-01 14:09:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 21 training takes 0:13:06
[2024-08-01 14:10:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.604 (11.604)	Loss 0.5166 (0.5166)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 14:10:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.770 Acc@5 97.728
[2024-08-01 14:10:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 14:10:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 14:10:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:51:03 lr 0.000005	 wd 0.0500	time 11.2963 (11.2963)	loss 1.0007 (1.0007)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:11:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:16:48 lr 0.000005	 wd 0.0500	time 0.2960 (0.4198)	loss 1.4770 (1.1911)	grad_norm 0.3689 (0.3571)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:11:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:13:58 lr 0.000005	 wd 0.0500	time 0.2799 (0.3640)	loss 0.7774 (1.1840)	grad_norm 0.3303 (0.3617)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:12:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:12:38 lr 0.000005	 wd 0.0500	time 0.3349 (0.3446)	loss 1.3949 (1.1897)	grad_norm 0.3434 (0.3587)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:12:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:11:45 lr 0.000005	 wd 0.0500	time 0.3083 (0.3358)	loss 0.7705 (1.1958)	grad_norm 0.3344 (0.3582)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:13:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:01 lr 0.000005	 wd 0.0500	time 0.2972 (0.3305)	loss 1.2120 (1.1931)	grad_norm 0.3352 (0.3568)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:13:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:10:21 lr 0.000005	 wd 0.0500	time 0.3478 (0.3269)	loss 0.8973 (1.1944)	grad_norm 0.3313 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:09:44 lr 0.000005	 wd 0.0500	time 0.2892 (0.3244)	loss 1.3753 (1.1891)	grad_norm 0.4037 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:09 lr 0.000004	 wd 0.0500	time 0.3353 (0.3226)	loss 0.9490 (1.1878)	grad_norm 0.3326 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:15:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:08:34 lr 0.000004	 wd 0.0500	time 0.2965 (0.3211)	loss 1.1171 (1.1867)	grad_norm 0.3548 (0.3675)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:15:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:00 lr 0.000004	 wd 0.0500	time 0.2873 (0.3197)	loss 1.3039 (1.1869)	grad_norm 0.3368 (0.3665)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:16:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:07:27 lr 0.000004	 wd 0.0500	time 0.2868 (0.3189)	loss 0.8337 (1.1841)	grad_norm 0.3677 (0.3649)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:16:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:06:54 lr 0.000004	 wd 0.0500	time 0.3188 (0.3181)	loss 1.3929 (1.1851)	grad_norm 0.3415 (0.3649)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:17:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:21 lr 0.000004	 wd 0.0500	time 0.3065 (0.3175)	loss 0.8560 (1.1788)	grad_norm 0.3286 (0.3631)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:17:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:05:49 lr 0.000004	 wd 0.0500	time 0.3005 (0.3169)	loss 1.0080 (1.1732)	grad_norm 0.3438 (0.3628)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:18:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:17 lr 0.000004	 wd 0.0500	time 0.2936 (0.3164)	loss 1.2271 (1.1728)	grad_norm 0.3545 (0.3619)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:04:45 lr 0.000004	 wd 0.0500	time 0.2862 (0.3160)	loss 1.5319 (1.1729)	grad_norm 0.3351 (0.3610)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:19:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:13 lr 0.000004	 wd 0.0500	time 0.3393 (0.3158)	loss 0.9042 (1.1711)	grad_norm 0.3446 (0.3603)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:19:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:41 lr 0.000004	 wd 0.0500	time 0.3006 (0.3154)	loss 1.4428 (1.1697)	grad_norm 0.3216 (0.3599)	loss_scale 4096.0000 (2054.8229)	mem 16679MB
[2024-08-01 14:20:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:09 lr 0.000004	 wd 0.0500	time 0.2903 (0.3152)	loss 0.9615 (1.1705)	grad_norm 0.3312 (0.3597)	loss_scale 4096.0000 (2162.1967)	mem 16679MB
[2024-08-01 14:20:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:38 lr 0.000004	 wd 0.0500	time 0.2929 (0.3150)	loss 0.9216 (1.1715)	grad_norm 0.3367 (0.3600)	loss_scale 4096.0000 (2258.8386)	mem 16679MB
[2024-08-01 14:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:06 lr 0.000004	 wd 0.0500	time 0.2912 (0.3150)	loss 1.3232 (1.1730)	grad_norm 0.3327 (0.3595)	loss_scale 4096.0000 (2346.2808)	mem 16679MB
[2024-08-01 14:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:35 lr 0.000004	 wd 0.0500	time 0.2921 (0.3154)	loss 1.1234 (1.1728)	grad_norm 0.3530 (0.3590)	loss_scale 4096.0000 (2425.7774)	mem 16679MB
[2024-08-01 14:22:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:03 lr 0.000004	 wd 0.0500	time 0.3171 (0.3153)	loss 1.1549 (1.1729)	grad_norm 0.3275 (0.3585)	loss_scale 4096.0000 (2498.3642)	mem 16679MB
[2024-08-01 14:23:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:32 lr 0.000004	 wd 0.0500	time 0.2912 (0.3154)	loss 1.3552 (1.1740)	grad_norm 0.3585 (0.3582)	loss_scale 4096.0000 (2564.9046)	mem 16679MB
[2024-08-01 14:23:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2835 (0.3148)	loss 1.1287 (1.1736)	grad_norm 0.3285 (0.3578)	loss_scale 4096.0000 (2626.1240)	mem 16679MB
[2024-08-01 14:23:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 22 training takes 0:13:10
[2024-08-01 14:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.584 (12.584)	Loss 0.4998 (0.4998)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 14:24:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.744 Acc@5 97.736
[2024-08-01 14:24:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 14:24:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 14:24:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:39:26 lr 0.000004	 wd 0.0500	time 11.0178 (11.0178)	loss 0.7879 (0.7879)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:24:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:16:45 lr 0.000004	 wd 0.0500	time 0.2979 (0.4184)	loss 1.4949 (1.1643)	grad_norm 0.3585 (0.3486)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:25:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:13:53 lr 0.000004	 wd 0.0500	time 0.3357 (0.3622)	loss 0.8622 (1.1689)	grad_norm 0.3457 (0.3475)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:25:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:12:38 lr 0.000004	 wd 0.0500	time 0.3143 (0.3445)	loss 1.4374 (1.1593)	grad_norm 0.3479 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:26:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:12:03 lr 0.000004	 wd 0.0500	time 0.3190 (0.3443)	loss 0.9010 (1.1647)	grad_norm 0.3465 (0.3531)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:26:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:22 lr 0.000004	 wd 0.0500	time 0.3158 (0.3410)	loss 0.8316 (1.1678)	grad_norm 0.3238 (0.3515)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:27:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:10:39 lr 0.000004	 wd 0.0500	time 0.2930 (0.3360)	loss 1.4540 (1.1672)	grad_norm 0.3804 (0.3507)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:27:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:09:58 lr 0.000004	 wd 0.0500	time 0.2918 (0.3320)	loss 1.1464 (1.1704)	grad_norm 0.3769 (0.3504)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:28:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:19 lr 0.000003	 wd 0.0500	time 0.2941 (0.3288)	loss 1.4581 (1.1722)	grad_norm 0.3283 (0.3497)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:28:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:08:42 lr 0.000003	 wd 0.0500	time 0.2903 (0.3261)	loss 1.1334 (1.1728)	grad_norm 0.3336 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:29:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:07 lr 0.000003	 wd 0.0500	time 0.2882 (0.3243)	loss 1.1530 (1.1682)	grad_norm 0.3142 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:30:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:33 lr 0.000003	 wd 0.0500	time 0.3094 (0.3233)	loss 1.0492 (1.1712)	grad_norm 0.3511 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:30:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:06:59 lr 0.000003	 wd 0.0500	time 0.2881 (0.3219)	loss 1.4310 (1.1742)	grad_norm 0.3397 (0.3500)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:31:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:25 lr 0.000003	 wd 0.0500	time 0.2909 (0.3208)	loss 1.2919 (1.1749)	grad_norm 0.3282 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:31:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:05:52 lr 0.000003	 wd 0.0500	time 0.2920 (0.3200)	loss 1.2836 (1.1750)	grad_norm 0.5479 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:32:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:20 lr 0.000003	 wd 0.0500	time 0.3083 (0.3194)	loss 1.3089 (1.1735)	grad_norm 0.3424 (0.3504)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:04:47 lr 0.000003	 wd 0.0500	time 0.2994 (0.3188)	loss 1.1520 (1.1769)	grad_norm 0.3250 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:33:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:15 lr 0.000003	 wd 0.0500	time 0.2894 (0.3185)	loss 0.7286 (1.1742)	grad_norm 0.3445 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:33:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:43 lr 0.000003	 wd 0.0500	time 0.3323 (0.3183)	loss 1.3235 (1.1733)	grad_norm 0.3608 (0.3499)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:34:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:11 lr 0.000003	 wd 0.0500	time 0.2935 (0.3180)	loss 1.3155 (1.1727)	grad_norm 0.3319 (0.3499)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:34:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:39 lr 0.000003	 wd 0.0500	time 0.2914 (0.3179)	loss 0.9430 (1.1723)	grad_norm 0.3496 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:35:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:07 lr 0.000003	 wd 0.0500	time 0.3490 (0.3177)	loss 1.4036 (1.1728)	grad_norm 0.3464 (0.3508)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:35:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:35 lr 0.000003	 wd 0.0500	time 0.3339 (0.3173)	loss 1.3409 (1.1714)	grad_norm 0.3566 (0.3505)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:36:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:04 lr 0.000003	 wd 0.0500	time 0.3109 (0.3170)	loss 1.3785 (1.1714)	grad_norm 0.3322 (0.3506)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:36:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:32 lr 0.000003	 wd 0.0500	time 0.2938 (0.3166)	loss 0.8144 (1.1713)	grad_norm 0.3379 (0.3508)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:37:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2844 (0.3159)	loss 0.8899 (1.1705)	grad_norm 0.3319 (0.3507)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:37:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 23 training takes 0:13:13
[2024-08-01 14:37:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.146 (11.146)	Loss 0.5176 (0.5176)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 14:37:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.742 Acc@5 97.714
[2024-08-01 14:37:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 14:37:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 14:38:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:03:25 lr 0.000003	 wd 0.0500	time 11.5928 (11.5928)	loss 1.2455 (1.2455)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:38:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:16:37 lr 0.000003	 wd 0.0500	time 0.2930 (0.4151)	loss 0.9444 (1.1781)	grad_norm 0.3699 (0.3533)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:39:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:13:46 lr 0.000003	 wd 0.0500	time 0.2916 (0.3590)	loss 1.2362 (1.1666)	grad_norm 0.3170 (0.3529)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:39:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:12:30 lr 0.000003	 wd 0.0500	time 0.2980 (0.3410)	loss 1.3861 (1.1812)	grad_norm 0.3395 (0.3514)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:11:37 lr 0.000003	 wd 0.0500	time 0.3314 (0.3320)	loss 0.7320 (1.1808)	grad_norm 0.3492 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:40:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:10:54 lr 0.000003	 wd 0.0500	time 0.3101 (0.3272)	loss 1.4574 (1.1809)	grad_norm 0.3637 (0.3513)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:41:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:10:15 lr 0.000003	 wd 0.0500	time 0.2942 (0.3237)	loss 0.6896 (1.1761)	grad_norm 0.3384 (0.3517)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:41:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:09:38 lr 0.000003	 wd 0.0500	time 0.2918 (0.3213)	loss 1.2085 (1.1743)	grad_norm 0.3288 (0.3527)	loss_scale 4096.0000 (4096.0000)	mem 16679MB
[2024-08-01 14:42:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:04 lr 0.000003	 wd 0.0500	time 0.2956 (0.3199)	loss 1.2430 (1.1764)	grad_norm 0.3502 (0.3522)	loss_scale 8192.0000 (4147.1361)	mem 16679MB
[2024-08-01 14:42:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:08:30 lr 0.000003	 wd 0.0500	time 0.3475 (0.3185)	loss 0.9536 (1.1803)	grad_norm 0.3299 (0.3517)	loss_scale 8192.0000 (4596.0666)	mem 16679MB
[2024-08-01 14:43:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:07:57 lr 0.000003	 wd 0.0500	time 0.3007 (0.3179)	loss 0.8021 (1.1774)	grad_norm 0.3465 (0.3522)	loss_scale 8192.0000 (4955.3007)	mem 16679MB
[2024-08-01 14:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:07:24 lr 0.000003	 wd 0.0500	time 0.3459 (0.3171)	loss 0.8557 (1.1767)	grad_norm 0.3542 (0.3520)	loss_scale 8192.0000 (5249.2788)	mem 16679MB
[2024-08-01 14:44:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:06:51 lr 0.000002	 wd 0.0500	time 0.3016 (0.3160)	loss 0.9086 (1.1747)	grad_norm 0.3521 (0.3523)	loss_scale 8192.0000 (5494.3014)	mem 16679MB
[2024-08-01 14:44:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:19 lr 0.000002	 wd 0.0500	time 0.3017 (0.3157)	loss 0.7675 (1.1717)	grad_norm 0.6418 (0.3542)	loss_scale 8192.0000 (5701.6572)	mem 16679MB
[2024-08-01 14:45:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:05:47 lr 0.000002	 wd 0.0500	time 0.3085 (0.3151)	loss 1.4811 (1.1728)	grad_norm 0.3397 (nan)	loss_scale 4096.0000 (5803.3976)	mem 16679MB
[2024-08-01 14:45:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:15 lr 0.000002	 wd 0.0500	time 0.3075 (0.3148)	loss 1.2488 (1.1689)	grad_norm 0.3467 (nan)	loss_scale 2048.0000 (5585.9507)	mem 16679MB
[2024-08-01 14:46:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:04:43 lr 0.000002	 wd 0.0500	time 0.3098 (0.3145)	loss 1.3083 (1.1678)	grad_norm 0.3536 (nan)	loss_scale 2048.0000 (5364.9669)	mem 16679MB
[2024-08-01 14:46:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:12 lr 0.000002	 wd 0.0500	time 0.3060 (0.3147)	loss 1.5258 (1.1693)	grad_norm 0.3873 (nan)	loss_scale 2048.0000 (5169.9659)	mem 16679MB
[2024-08-01 14:47:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:40 lr 0.000002	 wd 0.0500	time 0.3000 (0.3146)	loss 1.1062 (1.1693)	grad_norm 0.3569 (nan)	loss_scale 2048.0000 (4996.6197)	mem 16679MB
[2024-08-01 14:47:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:09 lr 0.000002	 wd 0.0500	time 0.2907 (0.3144)	loss 0.9367 (1.1705)	grad_norm 0.3902 (nan)	loss_scale 2048.0000 (4841.5108)	mem 16679MB
[2024-08-01 14:48:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:37 lr 0.000002	 wd 0.0500	time 0.3166 (0.3143)	loss 1.2952 (1.1704)	grad_norm 0.3694 (nan)	loss_scale 2048.0000 (4701.9050)	mem 16679MB
[2024-08-01 14:48:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:06 lr 0.000002	 wd 0.0500	time 0.3017 (0.3143)	loss 1.0299 (1.1707)	grad_norm 0.3506 (nan)	loss_scale 2048.0000 (4575.5888)	mem 16679MB
[2024-08-01 14:49:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:34 lr 0.000002	 wd 0.0500	time 0.3119 (0.3143)	loss 1.0299 (1.1724)	grad_norm 0.3622 (nan)	loss_scale 2048.0000 (4460.7506)	mem 16679MB
[2024-08-01 14:49:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:03 lr 0.000002	 wd 0.0500	time 0.3079 (0.3141)	loss 1.3944 (1.1748)	grad_norm 0.3801 (nan)	loss_scale 2048.0000 (4355.8940)	mem 16679MB
[2024-08-01 14:50:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:32 lr 0.000002	 wd 0.0500	time 0.2920 (0.3140)	loss 1.2188 (1.1726)	grad_norm 0.3296 (nan)	loss_scale 2048.0000 (4259.7718)	mem 16679MB
[2024-08-01 14:50:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2843 (0.3134)	loss 0.9591 (1.1732)	grad_norm 0.3250 (nan)	loss_scale 2048.0000 (4171.3363)	mem 16679MB
[2024-08-01 14:50:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 24 training takes 0:13:08
[2024-08-01 14:51:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.774 (12.774)	Loss 0.4998 (0.4998)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 14:51:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.754 Acc@5 97.750
[2024-08-01 14:51:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 14:51:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 14:51:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:17:27 lr 0.000002	 wd 0.0500	time 11.9294 (11.9294)	loss 1.3019 (1.3019)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:52:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:16:50 lr 0.000002	 wd 0.0500	time 0.2986 (0.4207)	loss 1.2183 (1.2334)	grad_norm 0.3405 (0.3713)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:52:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:13:58 lr 0.000002	 wd 0.0500	time 0.3058 (0.3644)	loss 1.4463 (1.2017)	grad_norm 0.3412 (0.3611)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:53:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:12:38 lr 0.000002	 wd 0.0500	time 0.2995 (0.3445)	loss 1.1374 (1.1848)	grad_norm 0.3457 (0.3594)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:53:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:11:45 lr 0.000002	 wd 0.0500	time 0.3101 (0.3355)	loss 1.3403 (1.1856)	grad_norm 0.3279 (0.3627)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:54:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:11:00 lr 0.000002	 wd 0.0500	time 0.3178 (0.3299)	loss 1.4414 (1.1837)	grad_norm 0.3570 (0.3591)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:54:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:10:20 lr 0.000002	 wd 0.0500	time 0.2954 (0.3260)	loss 0.8352 (1.1736)	grad_norm 0.3442 (0.3564)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:55:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:09:43 lr 0.000002	 wd 0.0500	time 0.3090 (0.3238)	loss 0.7999 (1.1755)	grad_norm 0.3518 (0.3577)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:55:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:07 lr 0.000002	 wd 0.0500	time 0.3101 (0.3216)	loss 1.3607 (1.1744)	grad_norm 0.8648 (0.3575)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:56:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:08:32 lr 0.000002	 wd 0.0500	time 0.3107 (0.3200)	loss 1.4813 (1.1743)	grad_norm 0.3486 (0.3571)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:56:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:07:59 lr 0.000002	 wd 0.0500	time 0.2931 (0.3191)	loss 1.5242 (1.1756)	grad_norm 0.4050 (0.3563)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:57:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:07:25 lr 0.000002	 wd 0.0500	time 0.2891 (0.3181)	loss 1.5634 (1.1764)	grad_norm 0.3511 (0.3552)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:57:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:06:53 lr 0.000002	 wd 0.0500	time 0.2941 (0.3175)	loss 0.9670 (1.1728)	grad_norm 0.3290 (0.3560)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:21 lr 0.000002	 wd 0.0500	time 0.3271 (0.3171)	loss 0.8521 (1.1717)	grad_norm 0.3593 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:58:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:05:48 lr 0.000002	 wd 0.0500	time 0.2921 (0.3166)	loss 0.8364 (1.1713)	grad_norm 0.4498 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 14:59:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:16 lr 0.000002	 wd 0.0500	time 0.3289 (0.3163)	loss 0.8729 (1.1715)	grad_norm 0.3553 (nan)	loss_scale 1024.0000 (2038.4490)	mem 16679MB
[2024-08-01 14:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:04:45 lr 0.000002	 wd 0.0500	time 0.2979 (0.3163)	loss 0.8142 (1.1706)	grad_norm 0.3263 (nan)	loss_scale 1024.0000 (1975.0856)	mem 16679MB
[2024-08-01 15:00:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:13 lr 0.000002	 wd 0.0500	time 0.3498 (0.3160)	loss 0.7447 (1.1682)	grad_norm 0.3550 (nan)	loss_scale 1024.0000 (1919.1723)	mem 16679MB
[2024-08-01 15:00:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:41 lr 0.000002	 wd 0.0500	time 0.2987 (0.3158)	loss 0.8893 (1.1702)	grad_norm 0.3394 (nan)	loss_scale 1024.0000 (1869.4681)	mem 16679MB
[2024-08-01 15:01:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:09 lr 0.000002	 wd 0.0500	time 0.3080 (0.3155)	loss 1.6611 (1.1726)	grad_norm 0.3378 (nan)	loss_scale 1024.0000 (1824.9932)	mem 16679MB
[2024-08-01 15:02:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:38 lr 0.000002	 wd 0.0500	time 0.3028 (0.3155)	loss 0.9996 (1.1730)	grad_norm 0.3310 (nan)	loss_scale 1024.0000 (1784.9635)	mem 16679MB
[2024-08-01 15:02:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:06 lr 0.000002	 wd 0.0500	time 0.2942 (0.3153)	loss 1.3138 (1.1743)	grad_norm 0.3332 (nan)	loss_scale 1024.0000 (1748.7444)	mem 16679MB
[2024-08-01 15:03:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:35 lr 0.000001	 wd 0.0500	time 0.3116 (0.3151)	loss 1.1200 (1.1746)	grad_norm 0.3446 (nan)	loss_scale 1024.0000 (1715.8164)	mem 16679MB
[2024-08-01 15:03:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:03 lr 0.000001	 wd 0.0500	time 0.2995 (0.3150)	loss 1.3521 (1.1740)	grad_norm 0.3348 (nan)	loss_scale 1024.0000 (1685.7505)	mem 16679MB
[2024-08-01 15:04:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:32 lr 0.000001	 wd 0.0500	time 0.3625 (0.3153)	loss 1.3996 (1.1732)	grad_norm 0.3164 (nan)	loss_scale 1024.0000 (1658.1891)	mem 16679MB
[2024-08-01 15:04:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2840 (0.3146)	loss 0.7553 (1.1719)	grad_norm 0.3770 (nan)	loss_scale 1024.0000 (1632.8317)	mem 16679MB
[2024-08-01 15:04:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 25 training takes 0:13:11
[2024-08-01 15:04:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.267 (12.267)	Loss 0.5220 (0.5220)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 15:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.752 Acc@5 97.732
[2024-08-01 15:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 15:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 15:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:10:06 lr 0.000001	 wd 0.0500	time 11.7533 (11.7533)	loss 1.1975 (1.1975)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:05:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:16:46 lr 0.000001	 wd 0.0500	time 0.3167 (0.4191)	loss 1.3313 (1.1853)	grad_norm 0.3276 (0.3495)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:06:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:13:54 lr 0.000001	 wd 0.0500	time 0.3299 (0.3626)	loss 1.1125 (1.1841)	grad_norm 0.3577 (0.3548)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:06:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:12:42 lr 0.000001	 wd 0.0500	time 0.3256 (0.3462)	loss 0.9175 (1.1658)	grad_norm 0.3369 (0.3566)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:11:47 lr 0.000001	 wd 0.0500	time 0.3111 (0.3364)	loss 1.3603 (1.1641)	grad_norm 0.3541 (0.3535)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:08:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:02 lr 0.000001	 wd 0.0500	time 0.2936 (0.3309)	loss 1.3148 (1.1731)	grad_norm 0.3347 (0.3526)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:08:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:10:22 lr 0.000001	 wd 0.0500	time 0.3007 (0.3272)	loss 0.8040 (1.1773)	grad_norm 0.3442 (0.3520)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:09:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:09:44 lr 0.000001	 wd 0.0500	time 0.2935 (0.3243)	loss 1.4616 (1.1733)	grad_norm 0.3206 (0.3510)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:09:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:08 lr 0.000001	 wd 0.0500	time 0.3123 (0.3221)	loss 1.2721 (1.1774)	grad_norm 0.3555 (0.3539)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:10:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:08:33 lr 0.000001	 wd 0.0500	time 0.3060 (0.3207)	loss 1.2998 (1.1766)	grad_norm 0.3463 (0.3538)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:10:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:00 lr 0.000001	 wd 0.0500	time 0.3076 (0.3196)	loss 1.1395 (1.1756)	grad_norm 0.3380 (0.3540)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:11:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:26 lr 0.000001	 wd 0.0500	time 0.2949 (0.3188)	loss 0.9571 (1.1756)	grad_norm 0.3370 (0.3533)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:11:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:06:53 lr 0.000001	 wd 0.0500	time 0.3466 (0.3178)	loss 1.1003 (1.1773)	grad_norm 0.3352 (0.3545)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:12:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:21 lr 0.000001	 wd 0.0500	time 0.3378 (0.3172)	loss 1.3136 (1.1789)	grad_norm 0.3598 (0.3542)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:12:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:05:48 lr 0.000001	 wd 0.0500	time 0.2995 (0.3165)	loss 1.3970 (1.1797)	grad_norm 0.3429 (0.3546)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:16 lr 0.000001	 wd 0.0500	time 0.2934 (0.3158)	loss 1.4234 (1.1815)	grad_norm 0.3303 (0.3545)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:13:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:04:44 lr 0.000001	 wd 0.0500	time 0.3117 (0.3152)	loss 1.2183 (1.1788)	grad_norm 0.3380 (0.3540)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:14:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:12 lr 0.000001	 wd 0.0500	time 0.3026 (0.3149)	loss 1.3049 (1.1769)	grad_norm 0.3377 (0.3559)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:14:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:40 lr 0.000001	 wd 0.0500	time 0.2819 (0.3146)	loss 0.8765 (1.1768)	grad_norm 0.3621 (0.3557)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:15:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:09 lr 0.000001	 wd 0.0500	time 0.3312 (0.3149)	loss 1.3011 (1.1778)	grad_norm 0.3466 (0.3552)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:15:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:37 lr 0.000001	 wd 0.0500	time 0.2937 (0.3147)	loss 1.1943 (1.1763)	grad_norm 0.3540 (0.3556)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:16:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:06 lr 0.000001	 wd 0.0500	time 0.2977 (0.3145)	loss 1.3734 (1.1752)	grad_norm 0.3566 (0.3552)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:16:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:34 lr 0.000001	 wd 0.0500	time 0.3068 (0.3143)	loss 1.3005 (1.1736)	grad_norm 0.3501 (0.3548)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:17:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:03 lr 0.000001	 wd 0.0500	time 0.3214 (0.3141)	loss 1.3079 (1.1738)	grad_norm 0.3214 (0.3547)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:17:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:32 lr 0.000001	 wd 0.0500	time 0.3032 (0.3139)	loss 0.7287 (1.1742)	grad_norm 0.3379 (0.3545)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:18:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2842 (0.3133)	loss 0.9354 (1.1747)	grad_norm 0.3733 (0.3552)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:18:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 26 training takes 0:13:08
[2024-08-01 15:18:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.017 (12.017)	Loss 0.5059 (0.5059)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 15:18:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.758 Acc@5 97.740
[2024-08-01 15:18:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 15:18:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 15:19:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:15:42 lr 0.000001	 wd 0.0500	time 11.8874 (11.8874)	loss 1.4231 (1.4231)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:19:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:16:37 lr 0.000001	 wd 0.0500	time 0.2864 (0.4152)	loss 1.3033 (1.1656)	grad_norm 0.3135 (0.3505)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:20:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:13:46 lr 0.000001	 wd 0.0500	time 0.2944 (0.3591)	loss 1.3848 (1.1776)	grad_norm 0.3451 (0.3678)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:12:29 lr 0.000001	 wd 0.0500	time 0.2881 (0.3403)	loss 1.3653 (1.1774)	grad_norm 0.3440 (0.3647)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:21:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:11:35 lr 0.000001	 wd 0.0500	time 0.2995 (0.3310)	loss 1.0195 (1.1735)	grad_norm 0.3488 (0.3727)	loss_scale 1024.0000 (1024.0000)	mem 16679MB
[2024-08-01 15:21:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:10:53 lr 0.000001	 wd 0.0500	time 0.2969 (0.3262)	loss 0.6944 (1.1745)	grad_norm 0.3376 (0.3685)	loss_scale 2048.0000 (1060.7904)	mem 16679MB
[2024-08-01 15:22:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:10:13 lr 0.000001	 wd 0.0500	time 0.3061 (0.3228)	loss 1.3322 (1.1745)	grad_norm 0.3557 (0.3644)	loss_scale 2048.0000 (1225.0516)	mem 16679MB
[2024-08-01 15:22:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:09:37 lr 0.000001	 wd 0.0500	time 0.2901 (0.3203)	loss 0.9959 (1.1763)	grad_norm 0.3598 (0.3622)	loss_scale 2048.0000 (1342.4479)	mem 16679MB
[2024-08-01 15:23:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:02 lr 0.000001	 wd 0.0500	time 0.2996 (0.3186)	loss 0.9368 (1.1780)	grad_norm 0.3563 (0.3604)	loss_scale 2048.0000 (1430.5318)	mem 16679MB
[2024-08-01 15:23:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:08:28 lr 0.000001	 wd 0.0500	time 0.2960 (0.3173)	loss 1.2774 (1.1760)	grad_norm 0.3398 (0.3586)	loss_scale 2048.0000 (1499.0633)	mem 16679MB
[2024-08-01 15:24:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:07:55 lr 0.000001	 wd 0.0500	time 0.2880 (0.3162)	loss 1.1607 (1.1726)	grad_norm 0.3123 (0.3578)	loss_scale 2048.0000 (1553.9021)	mem 16679MB
[2024-08-01 15:24:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:22 lr 0.000001	 wd 0.0500	time 0.3043 (0.3154)	loss 1.1584 (1.1748)	grad_norm 0.3349 (0.3583)	loss_scale 2048.0000 (1598.7793)	mem 16679MB
[2024-08-01 15:25:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:06:50 lr 0.000001	 wd 0.0500	time 0.3088 (0.3150)	loss 0.7256 (1.1755)	grad_norm 0.3297 (0.3576)	loss_scale 2048.0000 (1636.1832)	mem 16679MB
[2024-08-01 15:25:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:18 lr 0.000001	 wd 0.0500	time 0.2933 (0.3145)	loss 0.7974 (1.1722)	grad_norm 0.3480 (0.3569)	loss_scale 2048.0000 (1667.8370)	mem 16679MB
[2024-08-01 15:26:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:05:45 lr 0.000001	 wd 0.0500	time 0.3166 (0.3139)	loss 0.8407 (1.1734)	grad_norm 0.3281 (0.3586)	loss_scale 2048.0000 (1694.9722)	mem 16679MB
[2024-08-01 15:26:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:14 lr 0.000001	 wd 0.0500	time 0.2856 (0.3134)	loss 1.4897 (1.1744)	grad_norm 0.3244 (0.3577)	loss_scale 2048.0000 (1718.4917)	mem 16679MB
[2024-08-01 15:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:04:44 lr 0.000001	 wd 0.0500	time 0.3153 (0.3151)	loss 1.3326 (1.1761)	grad_norm 0.3486 (0.3570)	loss_scale 2048.0000 (1739.0731)	mem 16679MB
[2024-08-01 15:27:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:12 lr 0.000001	 wd 0.0500	time 0.3004 (0.3146)	loss 1.3705 (1.1777)	grad_norm 0.3521 (0.3571)	loss_scale 2048.0000 (1757.2346)	mem 16679MB
[2024-08-01 15:28:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:40 lr 0.000001	 wd 0.0500	time 0.3054 (0.3143)	loss 1.4269 (1.1772)	grad_norm 0.3365 (0.3570)	loss_scale 2048.0000 (1773.3792)	mem 16679MB
[2024-08-01 15:28:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:09 lr 0.000001	 wd 0.0500	time 0.3301 (0.3142)	loss 1.3985 (1.1779)	grad_norm 0.3469 (0.3564)	loss_scale 2048.0000 (1787.8254)	mem 16679MB
[2024-08-01 15:29:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:37 lr 0.000001	 wd 0.0500	time 0.3070 (0.3141)	loss 1.4195 (1.1786)	grad_norm 0.3372 (0.3558)	loss_scale 2048.0000 (1800.8276)	mem 16679MB
[2024-08-01 15:29:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:06 lr 0.000001	 wd 0.0500	time 0.2983 (0.3139)	loss 1.5218 (1.1777)	grad_norm 0.3536 (0.3558)	loss_scale 2048.0000 (1812.5921)	mem 16679MB
[2024-08-01 15:30:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:34 lr 0.000001	 wd 0.0500	time 0.2992 (0.3140)	loss 1.1982 (1.1790)	grad_norm 0.3463 (0.3555)	loss_scale 2048.0000 (1823.2876)	mem 16679MB
[2024-08-01 15:30:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:03 lr 0.000001	 wd 0.0500	time 0.3143 (0.3137)	loss 1.3199 (1.1787)	grad_norm 0.3372 (0.3552)	loss_scale 2048.0000 (1833.0535)	mem 16679MB
[2024-08-01 15:31:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:32 lr 0.000001	 wd 0.0500	time 0.2999 (0.3138)	loss 1.3725 (1.1804)	grad_norm 0.3850 (0.3550)	loss_scale 2048.0000 (1842.0058)	mem 16679MB
[2024-08-01 15:31:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2857 (0.3134)	loss 1.2892 (1.1796)	grad_norm 0.3509 (0.3546)	loss_scale 2048.0000 (1850.2423)	mem 16679MB
[2024-08-01 15:32:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 27 training takes 0:13:08
[2024-08-01 15:32:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.424 (12.424)	Loss 0.4941 (0.4941)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 15:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.786 Acc@5 97.752
[2024-08-01 15:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 15:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 15:32:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:25:29 lr 0.000001	 wd 0.0500	time 10.6831 (10.6831)	loss 0.9750 (0.9750)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:33:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:16:49 lr 0.000000	 wd 0.0500	time 0.2886 (0.4202)	loss 0.9140 (1.2141)	grad_norm 0.3344 (0.3486)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:33:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:13:55 lr 0.000000	 wd 0.0500	time 0.2883 (0.3630)	loss 1.5497 (1.2010)	grad_norm 0.3462 (0.3453)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:34:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:12:46 lr 0.000000	 wd 0.0500	time 0.2958 (0.3482)	loss 1.4540 (1.1953)	grad_norm 0.3684 (0.3474)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:34:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:11:50 lr 0.000000	 wd 0.0500	time 0.3051 (0.3382)	loss 0.7526 (1.1865)	grad_norm 0.3325 (0.3470)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:04 lr 0.000000	 wd 0.0500	time 0.3089 (0.3319)	loss 0.8210 (1.1916)	grad_norm 0.3385 (0.3487)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:35:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:10:23 lr 0.000000	 wd 0.0500	time 0.2945 (0.3280)	loss 0.9002 (1.1841)	grad_norm 0.3617 (0.3478)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:36:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:09:45 lr 0.000000	 wd 0.0500	time 0.3171 (0.3251)	loss 1.3029 (1.1809)	grad_norm 0.3498 (0.3503)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:36:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:09 lr 0.000000	 wd 0.0500	time 0.2944 (0.3231)	loss 0.8535 (1.1814)	grad_norm 0.3403 (0.3504)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:37:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:08:34 lr 0.000000	 wd 0.0500	time 0.3109 (0.3214)	loss 0.8576 (1.1777)	grad_norm 0.3655 (0.3500)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:37:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:00 lr 0.000000	 wd 0.0500	time 0.2897 (0.3201)	loss 1.0180 (1.1793)	grad_norm 0.3358 (0.3518)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:38:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:27 lr 0.000000	 wd 0.0500	time 0.3090 (0.3190)	loss 1.2865 (1.1796)	grad_norm 0.3400 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:38:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:06:53 lr 0.000000	 wd 0.0500	time 0.3324 (0.3179)	loss 1.4663 (1.1798)	grad_norm 0.3735 (0.3536)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:39:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:21 lr 0.000000	 wd 0.0500	time 0.3087 (0.3172)	loss 1.5313 (1.1812)	grad_norm 0.3452 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:40:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:05:48 lr 0.000000	 wd 0.0500	time 0.3219 (0.3167)	loss 0.9611 (1.1806)	grad_norm 0.3333 (0.3528)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:40:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:16 lr 0.000000	 wd 0.0500	time 0.2974 (0.3161)	loss 1.1938 (1.1791)	grad_norm 0.3383 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:41:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:04:44 lr 0.000000	 wd 0.0500	time 0.3123 (0.3157)	loss 1.4178 (1.1819)	grad_norm 0.3695 (0.3521)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:41:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:13 lr 0.000000	 wd 0.0500	time 0.3157 (0.3155)	loss 1.2535 (1.1813)	grad_norm 0.3512 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:42:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:41 lr 0.000000	 wd 0.0500	time 0.2874 (0.3151)	loss 0.8032 (1.1809)	grad_norm 0.3431 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:42:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:09 lr 0.000000	 wd 0.0500	time 0.2962 (0.3149)	loss 1.1082 (1.1785)	grad_norm 0.3370 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:43:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:37 lr 0.000000	 wd 0.0500	time 0.2941 (0.3146)	loss 1.1311 (1.1786)	grad_norm 0.3451 (0.3522)	loss_scale 4096.0000 (2068.4698)	mem 16679MB
[2024-08-01 15:43:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:06 lr 0.000000	 wd 0.0500	time 0.3171 (0.3143)	loss 0.9835 (1.1786)	grad_norm 0.3570 (0.3520)	loss_scale 4096.0000 (2164.9729)	mem 16679MB
[2024-08-01 15:44:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:34 lr 0.000000	 wd 0.0500	time 0.3238 (0.3142)	loss 1.3054 (1.1777)	grad_norm 0.3525 (0.3521)	loss_scale 4096.0000 (2252.7070)	mem 16679MB
[2024-08-01 15:44:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:03 lr 0.000000	 wd 0.0500	time 0.2949 (0.3139)	loss 1.2434 (1.1776)	grad_norm 0.3550 (0.3520)	loss_scale 4096.0000 (2332.8153)	mem 16679MB
[2024-08-01 15:45:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:32 lr 0.000000	 wd 0.0500	time 0.3312 (0.3138)	loss 1.5327 (1.1762)	grad_norm 0.3225 (nan)	loss_scale 2048.0000 (2346.5423)	mem 16679MB
[2024-08-01 15:45:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2883 (0.3131)	loss 1.0841 (1.1755)	grad_norm 0.3303 (nan)	loss_scale 2048.0000 (2334.6054)	mem 16679MB
[2024-08-01 15:45:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 28 training takes 0:13:07
[2024-08-01 15:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 12.371 (12.371)	Loss 0.5020 (0.5020)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 16679MB
[2024-08-01 15:46:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.780 Acc@5 97.728
[2024-08-01 15:46:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 15:46:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 15:46:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:27:53 lr 0.000000	 wd 0.0500	time 12.1797 (12.1797)	loss 1.2432 (1.2432)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:47:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:16:58 lr 0.000000	 wd 0.0500	time 0.3024 (0.4241)	loss 1.4376 (1.1877)	grad_norm 0.3579 (0.3480)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:47:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:13:59 lr 0.000000	 wd 0.0500	time 0.2912 (0.3648)	loss 0.8380 (1.1855)	grad_norm 0.3661 (0.3750)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:48:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:12:40 lr 0.000000	 wd 0.0500	time 0.2862 (0.3452)	loss 0.8967 (1.1816)	grad_norm 0.3495 (0.3654)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:48:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:45 lr 0.000000	 wd 0.0500	time 0.2896 (0.3357)	loss 1.1210 (1.1753)	grad_norm 0.3468 (0.3610)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:49:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:10:58 lr 0.000000	 wd 0.0500	time 0.3155 (0.3291)	loss 1.3216 (1.1705)	grad_norm 0.3676 (0.3598)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:49:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:10:19 lr 0.000000	 wd 0.0500	time 0.3081 (0.3257)	loss 0.7419 (1.1742)	grad_norm 0.3407 (0.3598)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:50:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:09:42 lr 0.000000	 wd 0.0500	time 0.3006 (0.3230)	loss 1.1469 (1.1749)	grad_norm 0.3391 (0.3587)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:50:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:05 lr 0.000000	 wd 0.0500	time 0.2880 (0.3206)	loss 0.7839 (1.1757)	grad_norm 0.3308 (0.3579)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:51:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:08:31 lr 0.000000	 wd 0.0500	time 0.2891 (0.3193)	loss 1.4981 (1.1771)	grad_norm 0.3555 (0.3567)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:51:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:07:57 lr 0.000000	 wd 0.0500	time 0.2882 (0.3180)	loss 1.4472 (1.1779)	grad_norm 0.3494 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:52:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:24 lr 0.000000	 wd 0.0500	time 0.2904 (0.3168)	loss 1.2517 (1.1777)	grad_norm 0.3352 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:52:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:06:51 lr 0.000000	 wd 0.0500	time 0.3351 (0.3160)	loss 1.0248 (1.1754)	grad_norm 0.3245 (0.3544)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:53:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:18 lr 0.000000	 wd 0.0500	time 0.3297 (0.3152)	loss 1.1610 (1.1788)	grad_norm 0.3669 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:53:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:05:46 lr 0.000000	 wd 0.0500	time 0.3113 (0.3145)	loss 1.1316 (1.1739)	grad_norm 0.4456 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:54:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:14 lr 0.000000	 wd 0.0500	time 0.3133 (0.3142)	loss 1.0345 (1.1740)	grad_norm 0.3364 (0.3562)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:54:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:04:43 lr 0.000000	 wd 0.0500	time 0.3035 (0.3142)	loss 0.8307 (1.1761)	grad_norm 0.3490 (0.3568)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:55:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:11 lr 0.000000	 wd 0.0500	time 0.3172 (0.3142)	loss 1.3743 (1.1788)	grad_norm 0.3447 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:40 lr 0.000000	 wd 0.0500	time 0.3060 (0.3142)	loss 1.2099 (1.1772)	grad_norm 0.3232 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:56:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:09 lr 0.000000	 wd 0.0500	time 0.2903 (0.3141)	loss 0.7862 (1.1776)	grad_norm 1.0448 (0.3559)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:37 lr 0.000000	 wd 0.0500	time 0.3286 (0.3141)	loss 0.9111 (1.1793)	grad_norm 0.3464 (0.3566)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:57:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:06 lr 0.000000	 wd 0.0500	time 0.3039 (0.3139)	loss 0.7777 (1.1803)	grad_norm 0.3120 (0.3573)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:57:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:34 lr 0.000000	 wd 0.0500	time 0.3101 (0.3137)	loss 0.8493 (1.1808)	grad_norm 0.3511 (0.3568)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:58:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:03 lr 0.000000	 wd 0.0500	time 0.3171 (0.3137)	loss 0.8572 (1.1779)	grad_norm 0.3533 (0.3566)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:58:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:31 lr 0.000000	 wd 0.0500	time 0.2961 (0.3136)	loss 1.2749 (1.1780)	grad_norm 0.3723 (0.3565)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:59:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2842 (0.3131)	loss 1.3470 (1.1772)	grad_norm 0.3331 (0.3565)	loss_scale 2048.0000 (2048.0000)	mem 16679MB
[2024-08-01 15:59:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 249): INFO EPOCH 29 training takes 0:13:08
[2024-08-01 15:59:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_29.pth saving......
[2024-08-01 15:59:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_29.pth saved !!!
[2024-08-01 15:59:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 289): INFO Test: [0/98]	Time 11.593 (11.593)	Loss 0.4917 (0.4917)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 16679MB
[2024-08-01 15:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 296): INFO  * Acc@1 85.782 Acc@5 97.756
[2024-08-01 15:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 15:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 15:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1] (main.py 189): INFO Training time 6:48:18
