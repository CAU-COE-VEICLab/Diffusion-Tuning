[2024-07-31 09:46:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/config.json
[2024-07-31 09:46:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage1
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_step_stage1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-31 09:46:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_step_stage1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_step_stage1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-31 09:46:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1
[2024-07-31 09:46:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-31 09:46:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 113): INFO number of params: 882568
[2024-07-31 09:46:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1, ignoring auto resume
[2024-07-31 09:46:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-31 09:46:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-31 09:46:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_adapter_smt_l_step_stage0/ckpt_epoch_best.pth'
[2024-07-31 09:46:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 14.952 (14.952)	Loss 0.5176 (0.5176)	Acc@1 93.555 (93.555)	Acc@5 98.242 (98.242)	Mem 2322MB
[2024-07-31 09:46:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.660 Acc@5 97.668
[2024-07-31 09:46:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 09:46:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 168): INFO Start training
[2024-07-31 09:47:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:39:55 lr 0.000000	 wd 0.0500	time 12.4681 (12.4681)	loss 1.5404 (1.5404)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 12226MB
[2024-07-31 09:47:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:15:07 lr 0.000000	 wd 0.0500	time 0.2391 (0.3779)	loss 1.2990 (1.2207)	grad_norm 0.3379 (nan)	loss_scale 32768.0000 (33741.3069)	mem 12226MB
[2024-07-31 09:47:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:12:18 lr 0.000000	 wd 0.0500	time 0.2489 (0.3210)	loss 1.0378 (1.2060)	grad_norm 0.3309 (nan)	loss_scale 16384.0000 (27877.2537)	mem 12226MB
[2024-07-31 09:48:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:10:58 lr 0.000000	 wd 0.0500	time 0.2689 (0.2992)	loss 0.9112 (1.1685)	grad_norm 0.3456 (nan)	loss_scale 16384.0000 (24058.8970)	mem 12226MB
[2024-07-31 09:48:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:10:09 lr 0.000001	 wd 0.0500	time 0.2691 (0.2899)	loss 0.9786 (1.1757)	grad_norm 0.3401 (nan)	loss_scale 8192.0000 (20510.6434)	mem 12226MB
[2024-07-31 09:49:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:09:27 lr 0.000001	 wd 0.0500	time 0.2557 (0.2834)	loss 1.1532 (1.1805)	grad_norm 0.3154 (nan)	loss_scale 8192.0000 (18051.8323)	mem 12226MB
[2024-07-31 09:49:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:08:51 lr 0.000001	 wd 0.0500	time 0.2402 (0.2794)	loss 1.3204 (1.1805)	grad_norm 0.3223 (nan)	loss_scale 8192.0000 (16411.2612)	mem 12226MB
[2024-07-31 09:50:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:08:17 lr 0.000001	 wd 0.0500	time 0.2809 (0.2763)	loss 1.2531 (1.1826)	grad_norm 0.3371 (nan)	loss_scale 4096.0000 (14818.0542)	mem 12226MB
[2024-07-31 09:50:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:07:46 lr 0.000001	 wd 0.0500	time 0.2558 (0.2743)	loss 1.0719 (1.1822)	grad_norm 0.3232 (nan)	loss_scale 4096.0000 (13479.4707)	mem 12226MB
[2024-07-31 09:50:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:07:17 lr 0.000001	 wd 0.0500	time 0.2441 (0.2729)	loss 1.3705 (1.1819)	grad_norm 0.3311 (nan)	loss_scale 4096.0000 (12438.0200)	mem 12226MB
[2024-07-31 09:51:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:06:48 lr 0.000002	 wd 0.0500	time 0.2881 (0.2717)	loss 1.4619 (1.1796)	grad_norm 0.3396 (nan)	loss_scale 4096.0000 (11604.6513)	mem 12226MB
[2024-07-31 09:51:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:06:19 lr 0.000002	 wd 0.0500	time 0.2867 (0.2709)	loss 1.2519 (1.1807)	grad_norm 0.3407 (nan)	loss_scale 4096.0000 (10922.6667)	mem 12226MB
[2024-07-31 09:52:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:05:52 lr 0.000002	 wd 0.0500	time 0.2488 (0.2709)	loss 0.9384 (1.1804)	grad_norm 0.3285 (nan)	loss_scale 4096.0000 (10354.2515)	mem 12226MB
[2024-07-31 09:52:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:05:24 lr 0.000002	 wd 0.0500	time 0.2707 (0.2703)	loss 1.2402 (1.1821)	grad_norm 0.3288 (nan)	loss_scale 4096.0000 (9873.2175)	mem 12226MB
[2024-07-31 09:53:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:04:57 lr 0.000002	 wd 0.0500	time 0.2668 (0.2697)	loss 1.4579 (1.1839)	grad_norm 0.3304 (nan)	loss_scale 4096.0000 (9460.8537)	mem 12226MB
[2024-07-31 09:53:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:29 lr 0.000002	 wd 0.0500	time 0.2431 (0.2691)	loss 1.1852 (1.1879)	grad_norm 0.3373 (nan)	loss_scale 4096.0000 (9103.4350)	mem 12226MB
[2024-07-31 09:53:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:04:02 lr 0.000003	 wd 0.0500	time 0.2501 (0.2686)	loss 1.2200 (1.1850)	grad_norm 0.3262 (nan)	loss_scale 4096.0000 (8790.6658)	mem 12226MB
[2024-07-31 09:54:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:35 lr 0.000003	 wd 0.0500	time 0.2790 (0.2683)	loss 0.9013 (1.1844)	grad_norm 0.3428 (nan)	loss_scale 4096.0000 (8514.6714)	mem 12226MB
[2024-07-31 09:54:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:08 lr 0.000003	 wd 0.0500	time 0.2781 (0.2682)	loss 1.1383 (1.1846)	grad_norm 0.3247 (nan)	loss_scale 4096.0000 (8269.3259)	mem 12226MB
[2024-07-31 09:55:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:41 lr 0.000003	 wd 0.0500	time 0.2917 (0.2679)	loss 1.4194 (1.1842)	grad_norm 0.3283 (nan)	loss_scale 4096.0000 (8049.7927)	mem 12226MB
[2024-07-31 09:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:14 lr 0.000003	 wd 0.0500	time 0.2389 (0.2675)	loss 0.8581 (1.1832)	grad_norm 0.3331 (nan)	loss_scale 4096.0000 (7852.2019)	mem 12226MB
[2024-07-31 09:56:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:47 lr 0.000003	 wd 0.0500	time 0.2566 (0.2672)	loss 1.0794 (1.1820)	grad_norm 0.3381 (nan)	loss_scale 4096.0000 (7673.4203)	mem 12226MB
[2024-07-31 09:56:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:20 lr 0.000004	 wd 0.0500	time 0.2422 (0.2668)	loss 1.5385 (1.1816)	grad_norm 0.3338 (nan)	loss_scale 4096.0000 (7510.8841)	mem 12226MB
[2024-07-31 09:57:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:53 lr 0.000004	 wd 0.0500	time 0.2975 (0.2666)	loss 1.3540 (1.1818)	grad_norm 0.3470 (nan)	loss_scale 4096.0000 (7362.4754)	mem 12226MB
[2024-07-31 09:57:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:27 lr 0.000004	 wd 0.0500	time 0.2473 (0.2666)	loss 1.3270 (1.1817)	grad_norm 0.3300 (nan)	loss_scale 4096.0000 (7226.4290)	mem 12226MB
[2024-07-31 09:57:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2377 (0.2659)	loss 0.8933 (1.1798)	grad_norm 0.3352 (nan)	loss_scale 4096.0000 (7101.2619)	mem 12226MB
[2024-07-31 09:57:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 0 training takes 0:11:07
[2024-07-31 09:57:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_0.pth saving......
[2024-07-31 09:57:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_0.pth saved !!!
[2024-07-31 09:58:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.057 (11.057)	Loss 0.5366 (0.5366)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 09:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.654 Acc@5 97.654
[2024-07-31 09:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 09:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.65%
[2024-07-31 09:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 09:58:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 09:58:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][0/2502]	eta 8:01:01 lr 0.000004	 wd 0.0500	time 11.5353 (11.5353)	loss 1.1244 (1.1244)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 09:59:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:14:47 lr 0.000004	 wd 0.0500	time 0.2504 (0.3696)	loss 1.1952 (1.1628)	grad_norm 0.3360 (0.3327)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 09:59:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:12:02 lr 0.000004	 wd 0.0500	time 0.2467 (0.3140)	loss 0.7778 (1.1795)	grad_norm 0.3361 (0.3341)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 09:59:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:10:49 lr 0.000004	 wd 0.0500	time 0.2384 (0.2950)	loss 0.8915 (1.1791)	grad_norm 0.3357 (0.3338)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:00:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:10:01 lr 0.000005	 wd 0.0500	time 0.2398 (0.2863)	loss 0.9924 (1.1942)	grad_norm 0.3377 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:00:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:09:22 lr 0.000005	 wd 0.0500	time 0.2838 (0.2810)	loss 1.4693 (1.1913)	grad_norm 0.3268 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:01:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:08:48 lr 0.000005	 wd 0.0500	time 0.2574 (0.2776)	loss 1.5106 (1.1902)	grad_norm 0.3249 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:01:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:08:14 lr 0.000005	 wd 0.0500	time 0.2457 (0.2746)	loss 0.8850 (1.1868)	grad_norm 0.3218 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:02:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:07:45 lr 0.000005	 wd 0.0500	time 0.2417 (0.2733)	loss 1.4470 (1.1860)	grad_norm 0.3127 (nan)	loss_scale 2048.0000 (3865.8876)	mem 12226MB
[2024-07-31 10:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:07:15 lr 0.000005	 wd 0.0500	time 0.2602 (0.2721)	loss 1.1917 (1.1850)	grad_norm 0.3540 (nan)	loss_scale 2048.0000 (3664.1243)	mem 12226MB
[2024-07-31 10:02:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:06:46 lr 0.000006	 wd 0.0500	time 0.2527 (0.2708)	loss 1.3787 (1.1837)	grad_norm 0.3449 (nan)	loss_scale 2048.0000 (3502.6733)	mem 12226MB
[2024-07-31 10:03:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:06:19 lr 0.000006	 wd 0.0500	time 0.2466 (0.2704)	loss 1.4365 (1.1821)	grad_norm 0.3284 (nan)	loss_scale 2048.0000 (3370.5504)	mem 12226MB
[2024-07-31 10:03:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:05:51 lr 0.000006	 wd 0.0500	time 0.2998 (0.2698)	loss 0.9686 (1.1799)	grad_norm 0.3299 (nan)	loss_scale 2048.0000 (3260.4296)	mem 12226MB
[2024-07-31 10:04:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:05:23 lr 0.000006	 wd 0.0500	time 0.2971 (0.2694)	loss 1.5458 (1.1809)	grad_norm 0.3465 (nan)	loss_scale 2048.0000 (3167.2375)	mem 12226MB
[2024-07-31 10:04:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:04:55 lr 0.000006	 wd 0.0500	time 0.2595 (0.2686)	loss 0.9636 (1.1809)	grad_norm 0.3434 (nan)	loss_scale 2048.0000 (3087.3490)	mem 12226MB
[2024-07-31 10:05:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:28 lr 0.000006	 wd 0.0500	time 0.2423 (0.2682)	loss 1.4355 (1.1817)	grad_norm 0.3294 (nan)	loss_scale 2048.0000 (3018.1053)	mem 12226MB
[2024-07-31 10:05:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:01 lr 0.000007	 wd 0.0500	time 0.2821 (0.2679)	loss 1.3105 (1.1834)	grad_norm 0.3316 (nan)	loss_scale 2048.0000 (2957.5116)	mem 12226MB
[2024-07-31 10:05:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:34 lr 0.000007	 wd 0.0500	time 0.2664 (0.2678)	loss 1.0026 (1.1837)	grad_norm 0.3532 (nan)	loss_scale 2048.0000 (2904.0423)	mem 12226MB
[2024-07-31 10:06:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:07 lr 0.000007	 wd 0.0500	time 0.2730 (0.2676)	loss 1.7171 (1.1830)	grad_norm 0.3348 (nan)	loss_scale 2048.0000 (2856.5108)	mem 12226MB
[2024-07-31 10:06:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:40 lr 0.000007	 wd 0.0500	time 0.2553 (0.2672)	loss 1.4454 (1.1842)	grad_norm 0.3473 (nan)	loss_scale 2048.0000 (2813.9800)	mem 12226MB
[2024-07-31 10:07:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:14 lr 0.000007	 wd 0.0500	time 0.2800 (0.2671)	loss 1.4205 (1.1847)	grad_norm 0.3429 (nan)	loss_scale 2048.0000 (2775.7001)	mem 12226MB
[2024-07-31 10:07:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:47 lr 0.000007	 wd 0.0500	time 0.2400 (0.2669)	loss 1.5147 (1.1868)	grad_norm 0.3543 (nan)	loss_scale 2048.0000 (2741.0643)	mem 12226MB
[2024-07-31 10:08:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:20 lr 0.000008	 wd 0.0500	time 0.2452 (0.2665)	loss 1.2387 (1.1873)	grad_norm 0.3308 (nan)	loss_scale 2048.0000 (2709.5756)	mem 12226MB
[2024-07-31 10:08:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.2738 (0.2665)	loss 0.9811 (1.1896)	grad_norm 0.3318 (nan)	loss_scale 2048.0000 (2680.8240)	mem 12226MB
[2024-07-31 10:09:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:27 lr 0.000008	 wd 0.0500	time 0.2371 (0.2663)	loss 1.2261 (1.1896)	grad_norm 0.3302 (nan)	loss_scale 2048.0000 (2654.4673)	mem 12226MB
[2024-07-31 10:09:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2373 (0.2657)	loss 0.9176 (1.1898)	grad_norm 0.3345 (nan)	loss_scale 2048.0000 (2630.2183)	mem 12226MB
[2024-07-31 10:09:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 1 training takes 0:11:07
[2024-07-31 10:09:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 10.918 (10.918)	Loss 0.5303 (0.5303)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 10:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.630 Acc@5 97.680
[2024-07-31 10:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-31 10:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.65%
[2024-07-31 10:10:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:42:05 lr 0.000008	 wd 0.0500	time 11.0812 (11.0812)	loss 1.2552 (1.2552)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:10:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:14:48 lr 0.000008	 wd 0.0500	time 0.2667 (0.3698)	loss 1.4401 (1.1941)	grad_norm 0.3328 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:11:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:12:01 lr 0.000008	 wd 0.0500	time 0.2440 (0.3134)	loss 1.4204 (1.1848)	grad_norm 0.3256 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:11:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:10:47 lr 0.000008	 wd 0.0500	time 0.2390 (0.2941)	loss 1.1920 (1.1749)	grad_norm 0.3388 (0.3341)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:11:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:09:58 lr 0.000009	 wd 0.0500	time 0.2714 (0.2848)	loss 1.4391 (1.1778)	grad_norm 0.3243 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:12:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:09:19 lr 0.000009	 wd 0.0500	time 0.2529 (0.2795)	loss 1.4439 (1.1734)	grad_norm 0.3460 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:12:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:08:44 lr 0.000009	 wd 0.0500	time 0.2449 (0.2758)	loss 1.2923 (1.1744)	grad_norm 0.3246 (0.3373)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:13:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:08:12 lr 0.000009	 wd 0.0500	time 0.2406 (0.2735)	loss 1.3350 (1.1794)	grad_norm 0.3363 (0.3363)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:13:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:07:43 lr 0.000009	 wd 0.0500	time 0.2402 (0.2721)	loss 1.4028 (1.1825)	grad_norm 0.3249 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:14:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:07:13 lr 0.000009	 wd 0.0500	time 0.2576 (0.2708)	loss 0.9441 (1.1875)	grad_norm 0.3438 (0.3359)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:14:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:06:45 lr 0.000010	 wd 0.0500	time 0.2546 (0.2697)	loss 0.8872 (1.1846)	grad_norm 0.3466 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:14:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:06:17 lr 0.000010	 wd 0.0500	time 0.2517 (0.2689)	loss 1.3814 (1.1821)	grad_norm 0.3449 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:15:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:05:49 lr 0.000010	 wd 0.0500	time 0.2463 (0.2684)	loss 0.9031 (1.1825)	grad_norm 0.3503 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:15:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:05:21 lr 0.000010	 wd 0.0500	time 0.2696 (0.2678)	loss 1.4195 (1.1835)	grad_norm 0.3436 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:16:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:04:54 lr 0.000010	 wd 0.0500	time 0.2502 (0.2671)	loss 1.2358 (1.1852)	grad_norm 0.3291 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:16:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:04:26 lr 0.000010	 wd 0.0500	time 0.2530 (0.2664)	loss 1.3498 (1.1857)	grad_norm 0.3313 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:17:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:03:59 lr 0.000011	 wd 0.0500	time 0.2650 (0.2661)	loss 0.9783 (1.1862)	grad_norm 0.3263 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:17:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:33 lr 0.000011	 wd 0.0500	time 0.2681 (0.2656)	loss 1.2985 (1.1866)	grad_norm 0.3239 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:17:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:06 lr 0.000011	 wd 0.0500	time 0.2795 (0.2654)	loss 1.0780 (1.1873)	grad_norm 0.3357 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:18:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:39 lr 0.000011	 wd 0.0500	time 0.2499 (0.2651)	loss 1.3640 (1.1871)	grad_norm 0.3558 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:12 lr 0.000011	 wd 0.0500	time 0.2763 (0.2649)	loss 1.1429 (1.1872)	grad_norm 0.3329 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:19:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:46 lr 0.000011	 wd 0.0500	time 0.2555 (0.2647)	loss 1.0320 (1.1857)	grad_norm 0.3242 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:19:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:19 lr 0.000012	 wd 0.0500	time 0.2445 (0.2645)	loss 1.4008 (1.1855)	grad_norm 0.3486 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:20:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:53 lr 0.000012	 wd 0.0500	time 0.2676 (0.2646)	loss 1.2337 (1.1856)	grad_norm 0.3319 (0.3351)	loss_scale 4096.0000 (2129.8844)	mem 12226MB
[2024-07-31 10:20:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:27 lr 0.000012	 wd 0.0500	time 0.2567 (0.2649)	loss 0.8051 (1.1864)	grad_norm 0.3351 (0.3350)	loss_scale 4096.0000 (2211.7718)	mem 12226MB
[2024-07-31 10:21:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2379 (0.2644)	loss 1.2230 (1.1867)	grad_norm 0.3170 (0.3349)	loss_scale 4096.0000 (2287.1108)	mem 12226MB
[2024-07-31 10:21:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 2 training takes 0:11:04
[2024-07-31 10:21:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.082 (12.082)	Loss 0.5127 (0.5127)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 10:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.668 Acc@5 97.690
[2024-07-31 10:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 10:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.67%
[2024-07-31 10:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 10:21:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 10:21:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:33:31 lr 0.000012	 wd 0.0500	time 10.8759 (10.8759)	loss 0.7570 (0.7570)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:22:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:14:35 lr 0.000012	 wd 0.0500	time 0.2431 (0.3644)	loss 1.2416 (1.2231)	grad_norm 0.3383 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:22:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:11:51 lr 0.000012	 wd 0.0500	time 0.2400 (0.3091)	loss 1.5359 (1.1978)	grad_norm 0.3432 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:22:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:10:40 lr 0.000012	 wd 0.0500	time 0.2380 (0.2908)	loss 1.4536 (1.1945)	grad_norm 0.3219 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:23:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:09:53 lr 0.000013	 wd 0.0500	time 0.2405 (0.2824)	loss 1.4326 (1.1867)	grad_norm 0.3381 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:23:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:09:15 lr 0.000013	 wd 0.0500	time 0.2446 (0.2773)	loss 1.0022 (1.1868)	grad_norm 0.3255 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:24:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:08:41 lr 0.000013	 wd 0.0500	time 0.2378 (0.2744)	loss 1.0629 (1.1813)	grad_norm 0.3477 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:24:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:08:11 lr 0.000013	 wd 0.0500	time 0.2397 (0.2726)	loss 1.5319 (1.1788)	grad_norm 0.3342 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:25:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:07:41 lr 0.000013	 wd 0.0500	time 0.2572 (0.2711)	loss 0.7842 (1.1745)	grad_norm 0.3406 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:25:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:07:11 lr 0.000013	 wd 0.0500	time 0.2688 (0.2696)	loss 1.5775 (1.1768)	grad_norm 0.3435 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:26:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:06:43 lr 0.000014	 wd 0.0500	time 0.2520 (0.2688)	loss 1.3069 (1.1775)	grad_norm 0.3139 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:26:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:06:16 lr 0.000014	 wd 0.0500	time 0.2697 (0.2687)	loss 1.0011 (1.1782)	grad_norm 0.3519 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:26:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:05:48 lr 0.000014	 wd 0.0500	time 0.2410 (0.2678)	loss 1.1657 (1.1778)	grad_norm 0.3350 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:27:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:05:23 lr 0.000014	 wd 0.0500	time 2.9661 (0.2692)	loss 1.3398 (1.1771)	grad_norm 0.3265 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:27:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:04:56 lr 0.000014	 wd 0.0500	time 0.2709 (0.2687)	loss 1.1358 (1.1781)	grad_norm 0.3257 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:28:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:28 lr 0.000014	 wd 0.0500	time 0.2485 (0.2680)	loss 1.4266 (1.1765)	grad_norm 0.3379 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 10:28:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:01 lr 0.000015	 wd 0.0500	time 0.2758 (0.2673)	loss 0.7997 (1.1749)	grad_norm 0.3350 (nan)	loss_scale 2048.0000 (3968.0800)	mem 12226MB
[2024-07-31 10:29:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:33 lr 0.000015	 wd 0.0500	time 0.2568 (0.2667)	loss 1.0048 (1.1773)	grad_norm 0.3182 (nan)	loss_scale 2048.0000 (3855.2005)	mem 12226MB
[2024-07-31 10:29:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:06 lr 0.000015	 wd 0.0500	time 0.2684 (0.2664)	loss 1.1597 (1.1785)	grad_norm 0.3459 (nan)	loss_scale 2048.0000 (3754.8562)	mem 12226MB
[2024-07-31 10:29:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:40 lr 0.000015	 wd 0.0500	time 0.2492 (0.2660)	loss 1.2801 (1.1777)	grad_norm 0.3270 (nan)	loss_scale 2048.0000 (3665.0689)	mem 12226MB
[2024-07-31 10:30:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:13 lr 0.000015	 wd 0.0500	time 0.2418 (0.2657)	loss 1.3410 (1.1782)	grad_norm 0.3258 (nan)	loss_scale 2048.0000 (3584.2559)	mem 12226MB
[2024-07-31 10:30:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:46 lr 0.000015	 wd 0.0500	time 0.2661 (0.2655)	loss 0.9929 (1.1787)	grad_norm 0.3451 (nan)	loss_scale 2048.0000 (3511.1356)	mem 12226MB
[2024-07-31 10:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:20 lr 0.000016	 wd 0.0500	time 0.2421 (0.2653)	loss 0.8108 (1.1779)	grad_norm 0.3322 (nan)	loss_scale 2048.0000 (3444.6597)	mem 12226MB
[2024-07-31 10:31:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:53 lr 0.000016	 wd 0.0500	time 0.2460 (0.2652)	loss 0.9257 (1.1781)	grad_norm 0.3455 (nan)	loss_scale 2048.0000 (3383.9618)	mem 12226MB
[2024-07-31 10:32:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:27 lr 0.000016	 wd 0.0500	time 0.2481 (0.2651)	loss 1.4255 (1.1799)	grad_norm 0.3462 (nan)	loss_scale 2048.0000 (3328.3199)	mem 12226MB
[2024-07-31 10:32:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2376 (0.2644)	loss 1.3179 (1.1796)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (3277.1275)	mem 12226MB
[2024-07-31 10:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 3 training takes 0:11:04
[2024-07-31 10:32:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.805 (11.805)	Loss 0.5264 (0.5264)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 10:33:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.644 Acc@5 97.668
[2024-07-31 10:33:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-31 10:33:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.67%
[2024-07-31 10:33:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][0/2502]	eta 8:05:50 lr 0.000016	 wd 0.0500	time 11.6508 (11.6508)	loss 1.2755 (1.2755)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:33:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:15:09 lr 0.000016	 wd 0.0500	time 0.2618 (0.3786)	loss 0.9624 (1.1933)	grad_norm 0.3402 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:34:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:12:12 lr 0.000016	 wd 0.0500	time 0.6843 (0.3181)	loss 0.8719 (1.1820)	grad_norm 0.3395 (0.3370)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:34:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:10:56 lr 0.000016	 wd 0.0500	time 0.2818 (0.2980)	loss 0.8151 (1.1788)	grad_norm 0.3330 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:10:04 lr 0.000017	 wd 0.0500	time 0.2388 (0.2877)	loss 1.3146 (1.1740)	grad_norm 0.3485 (0.3359)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:35:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:09:25 lr 0.000017	 wd 0.0500	time 0.2603 (0.2824)	loss 1.2658 (1.1747)	grad_norm 0.3298 (0.3364)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:35:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:08:49 lr 0.000017	 wd 0.0500	time 0.2549 (0.2783)	loss 1.1840 (1.1795)	grad_norm 0.3548 (0.3363)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:36:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:08:16 lr 0.000017	 wd 0.0500	time 0.2562 (0.2754)	loss 0.8505 (1.1799)	grad_norm 0.3424 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:36:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:07:47 lr 0.000017	 wd 0.0500	time 0.2406 (0.2747)	loss 0.7922 (1.1824)	grad_norm 0.3461 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:37:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:07:18 lr 0.000017	 wd 0.0500	time 0.2401 (0.2740)	loss 0.8317 (1.1838)	grad_norm 0.3305 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:37:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:06:49 lr 0.000018	 wd 0.0500	time 0.2514 (0.2727)	loss 1.6010 (1.1838)	grad_norm 0.3216 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:38:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:06:21 lr 0.000018	 wd 0.0500	time 0.2711 (0.2720)	loss 1.4788 (1.1846)	grad_norm 0.3303 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:38:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:05:53 lr 0.000018	 wd 0.0500	time 0.2628 (0.2714)	loss 1.2916 (1.1836)	grad_norm 0.3397 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:38:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:05:25 lr 0.000018	 wd 0.0500	time 0.3012 (0.2707)	loss 1.0233 (1.1842)	grad_norm 0.3504 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:39:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:04:57 lr 0.000018	 wd 0.0500	time 0.2452 (0.2701)	loss 1.4339 (1.1841)	grad_norm 0.3394 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:39:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:04:30 lr 0.000018	 wd 0.0500	time 0.2481 (0.2697)	loss 1.2219 (1.1845)	grad_norm 0.3299 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:40:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:04:02 lr 0.000019	 wd 0.0500	time 0.3030 (0.2693)	loss 1.4770 (1.1831)	grad_norm 0.3417 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:40:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:35 lr 0.000019	 wd 0.0500	time 0.2665 (0.2687)	loss 1.1803 (1.1837)	grad_norm 0.3396 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:41:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:08 lr 0.000019	 wd 0.0500	time 0.2732 (0.2687)	loss 1.5435 (1.1858)	grad_norm 0.3332 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:41:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:41 lr 0.000019	 wd 0.0500	time 0.2618 (0.2683)	loss 1.4250 (1.1862)	grad_norm 0.3383 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:42:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:14 lr 0.000019	 wd 0.0500	time 0.2518 (0.2679)	loss 0.8671 (1.1840)	grad_norm 0.3410 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:42:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:47 lr 0.000019	 wd 0.0500	time 0.2699 (0.2677)	loss 1.0230 (1.1827)	grad_norm 0.3283 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:42:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:20 lr 0.000020	 wd 0.0500	time 0.2539 (0.2676)	loss 0.8934 (1.1824)	grad_norm 0.3171 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:43:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:54 lr 0.000020	 wd 0.0500	time 0.2789 (0.2674)	loss 0.8024 (1.1817)	grad_norm 0.3332 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:43:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:27 lr 0.000020	 wd 0.0500	time 0.2461 (0.2673)	loss 0.7973 (1.1815)	grad_norm 0.3128 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:44:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2379 (0.2678)	loss 1.2523 (1.1822)	grad_norm 0.3471 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:44:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 4 training takes 0:11:12
[2024-07-31 10:44:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.481 (12.481)	Loss 0.5161 (0.5161)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 10:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.646 Acc@5 97.670
[2024-07-31 10:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-31 10:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.67%
[2024-07-31 10:44:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:08:10 lr 0.000020	 wd 0.0500	time 11.7070 (11.7070)	loss 1.4139 (1.4139)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:15:03 lr 0.000020	 wd 0.0500	time 0.2628 (0.3762)	loss 0.9972 (1.2124)	grad_norm 0.3447 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:45:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:12:08 lr 0.000020	 wd 0.0500	time 0.2492 (0.3164)	loss 1.3626 (1.1822)	grad_norm 0.3337 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:46:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:10:56 lr 0.000020	 wd 0.0500	time 0.2555 (0.2980)	loss 1.0172 (1.1710)	grad_norm 0.3294 (0.3364)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:46:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:10:05 lr 0.000020	 wd 0.0500	time 0.2423 (0.2878)	loss 1.2795 (1.1801)	grad_norm 0.3424 (0.3387)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:47:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:09:25 lr 0.000020	 wd 0.0500	time 0.2779 (0.2824)	loss 1.0099 (1.1786)	grad_norm 0.3314 (0.3378)	loss_scale 4096.0000 (2064.3513)	mem 12226MB
[2024-07-31 10:47:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:08:50 lr 0.000020	 wd 0.0500	time 0.2672 (0.2787)	loss 1.0523 (1.1801)	grad_norm 0.3513 (nan)	loss_scale 2048.0000 (2170.6755)	mem 12226MB
[2024-07-31 10:47:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:08:18 lr 0.000020	 wd 0.0500	time 0.2720 (0.2765)	loss 0.9233 (1.1785)	grad_norm 0.3243 (nan)	loss_scale 2048.0000 (2153.1755)	mem 12226MB
[2024-07-31 10:48:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:07:47 lr 0.000020	 wd 0.0500	time 0.2543 (0.2745)	loss 1.5409 (1.1777)	grad_norm 0.3278 (nan)	loss_scale 2048.0000 (2140.0449)	mem 12226MB
[2024-07-31 10:48:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:07:17 lr 0.000020	 wd 0.0500	time 0.2381 (0.2731)	loss 1.0699 (1.1766)	grad_norm 0.3255 (nan)	loss_scale 2048.0000 (2129.8291)	mem 12226MB
[2024-07-31 10:49:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:48 lr 0.000020	 wd 0.0500	time 0.2554 (0.2719)	loss 1.2538 (1.1749)	grad_norm 0.3142 (nan)	loss_scale 2048.0000 (2121.6543)	mem 12226MB
[2024-07-31 10:49:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:06:19 lr 0.000020	 wd 0.0500	time 0.2657 (0.2710)	loss 1.2000 (1.1759)	grad_norm 0.3406 (nan)	loss_scale 2048.0000 (2114.9646)	mem 12226MB
[2024-07-31 10:50:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:05:52 lr 0.000020	 wd 0.0500	time 0.2436 (0.2704)	loss 1.4330 (1.1745)	grad_norm 0.3339 (nan)	loss_scale 2048.0000 (2109.3888)	mem 12226MB
[2024-07-31 10:50:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:05:24 lr 0.000020	 wd 0.0500	time 0.3396 (0.2699)	loss 1.0000 (1.1753)	grad_norm 0.3372 (nan)	loss_scale 2048.0000 (2104.6703)	mem 12226MB
[2024-07-31 10:51:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:04:56 lr 0.000020	 wd 0.0500	time 0.2715 (0.2692)	loss 1.4487 (1.1791)	grad_norm 0.3475 (nan)	loss_scale 2048.0000 (2100.6253)	mem 12226MB
[2024-07-31 10:51:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:04:29 lr 0.000020	 wd 0.0500	time 0.2537 (0.2686)	loss 0.8559 (1.1803)	grad_norm 0.3483 (nan)	loss_scale 2048.0000 (2097.1193)	mem 12226MB
[2024-07-31 10:51:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:02 lr 0.000020	 wd 0.0500	time 0.2398 (0.2684)	loss 1.4288 (1.1829)	grad_norm 0.3385 (nan)	loss_scale 2048.0000 (2094.0512)	mem 12226MB
[2024-07-31 10:52:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:35 lr 0.000020	 wd 0.0500	time 0.2584 (0.2681)	loss 0.9673 (1.1831)	grad_norm 0.3220 (nan)	loss_scale 2048.0000 (2091.3439)	mem 12226MB
[2024-07-31 10:52:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:08 lr 0.000020	 wd 0.0500	time 0.2518 (0.2681)	loss 0.7757 (1.1811)	grad_norm 0.3323 (nan)	loss_scale 2048.0000 (2088.9373)	mem 12226MB
[2024-07-31 10:53:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:41 lr 0.000020	 wd 0.0500	time 0.2392 (0.2678)	loss 1.1459 (1.1806)	grad_norm 0.3275 (nan)	loss_scale 2048.0000 (2086.7838)	mem 12226MB
[2024-07-31 10:53:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:14 lr 0.000020	 wd 0.0500	time 0.2517 (0.2678)	loss 1.5829 (1.1822)	grad_norm 0.3199 (nan)	loss_scale 2048.0000 (2084.8456)	mem 12226MB
[2024-07-31 10:54:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:47 lr 0.000020	 wd 0.0500	time 0.2682 (0.2675)	loss 1.4967 (1.1818)	grad_norm 0.3458 (nan)	loss_scale 2048.0000 (2083.0919)	mem 12226MB
[2024-07-31 10:54:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:20 lr 0.000020	 wd 0.0500	time 0.2671 (0.2675)	loss 1.2383 (1.1831)	grad_norm 0.3280 (nan)	loss_scale 2048.0000 (2081.4975)	mem 12226MB
[2024-07-31 10:55:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:53 lr 0.000020	 wd 0.0500	time 0.2750 (0.2673)	loss 1.1448 (1.1815)	grad_norm 0.3477 (nan)	loss_scale 2048.0000 (2080.0417)	mem 12226MB
[2024-07-31 10:55:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:27 lr 0.000020	 wd 0.0500	time 0.2697 (0.2673)	loss 0.7935 (1.1809)	grad_norm 0.3489 (nan)	loss_scale 2048.0000 (2078.7072)	mem 12226MB
[2024-07-31 10:55:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2376 (0.2666)	loss 1.3153 (1.1822)	grad_norm 0.3432 (nan)	loss_scale 2048.0000 (2077.4794)	mem 12226MB
[2024-07-31 10:55:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 5 training takes 0:11:09
[2024-07-31 10:56:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.877 (11.877)	Loss 0.5127 (0.5127)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 10:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.678 Acc@5 97.700
[2024-07-31 10:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 10:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 10:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 10:56:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 10:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:32:39 lr 0.000020	 wd 0.0500	time 10.8550 (10.8550)	loss 1.2174 (1.2174)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:57:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:14:40 lr 0.000020	 wd 0.0500	time 0.2422 (0.3666)	loss 0.8975 (1.2041)	grad_norm 0.3504 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:57:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:12:00 lr 0.000020	 wd 0.0500	time 0.2388 (0.3131)	loss 0.8739 (1.1945)	grad_norm 0.3082 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:57:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:10:52 lr 0.000020	 wd 0.0500	time 0.2811 (0.2962)	loss 0.9562 (1.1863)	grad_norm 0.3357 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:58:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:10:04 lr 0.000020	 wd 0.0500	time 0.2552 (0.2874)	loss 0.8032 (1.1885)	grad_norm 0.3355 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:58:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:09:24 lr 0.000020	 wd 0.0500	time 0.2481 (0.2821)	loss 1.2265 (1.1889)	grad_norm 0.3301 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:59:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:08:50 lr 0.000020	 wd 0.0500	time 0.2611 (0.2789)	loss 0.8714 (1.1800)	grad_norm 0.3299 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 10:59:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:08:18 lr 0.000020	 wd 0.0500	time 0.2497 (0.2765)	loss 1.4947 (1.1826)	grad_norm 0.3149 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:00:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:07:46 lr 0.000020	 wd 0.0500	time 0.2553 (0.2742)	loss 0.7299 (1.1872)	grad_norm 0.3187 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:00:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:07:16 lr 0.000020	 wd 0.0500	time 0.2427 (0.2725)	loss 1.4969 (1.1835)	grad_norm 0.3216 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:00:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:06:47 lr 0.000020	 wd 0.0500	time 0.2392 (0.2713)	loss 1.0586 (1.1832)	grad_norm 0.3208 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:01:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:06:19 lr 0.000020	 wd 0.0500	time 0.2569 (0.2704)	loss 0.8723 (1.1798)	grad_norm 0.3361 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:01:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:05:51 lr 0.000020	 wd 0.0500	time 0.2503 (0.2701)	loss 1.2590 (1.1767)	grad_norm 0.3450 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:02:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:05:24 lr 0.000020	 wd 0.0500	time 0.2860 (0.2696)	loss 1.1198 (1.1754)	grad_norm 0.3241 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:02:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:04:56 lr 0.000020	 wd 0.0500	time 0.2474 (0.2691)	loss 0.9511 (1.1755)	grad_norm 0.3466 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:03:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:04:29 lr 0.000020	 wd 0.0500	time 0.2468 (0.2689)	loss 0.7439 (1.1763)	grad_norm 0.3293 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:03:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:02 lr 0.000020	 wd 0.0500	time 0.2496 (0.2685)	loss 1.4559 (1.1737)	grad_norm 0.3241 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:04:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:35 lr 0.000020	 wd 0.0500	time 0.2841 (0.2683)	loss 1.3916 (1.1731)	grad_norm 0.3295 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:04:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:08 lr 0.000020	 wd 0.0500	time 0.2469 (0.2681)	loss 1.2237 (1.1735)	grad_norm 0.3275 (0.3359)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:04:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:41 lr 0.000020	 wd 0.0500	time 0.2503 (0.2679)	loss 1.3640 (1.1747)	grad_norm 0.3305 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:05:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:14 lr 0.000020	 wd 0.0500	time 0.2662 (0.2675)	loss 0.9931 (1.1755)	grad_norm 0.3392 (0.3359)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:05:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:47 lr 0.000020	 wd 0.0500	time 0.2603 (0.2672)	loss 1.0032 (1.1757)	grad_norm 0.3371 (0.3359)	loss_scale 4096.0000 (2116.2342)	mem 12226MB
[2024-07-31 11:06:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:20 lr 0.000020	 wd 0.0500	time 0.2560 (0.2673)	loss 0.8149 (1.1750)	grad_norm 0.3607 (0.3358)	loss_scale 4096.0000 (2206.1826)	mem 12226MB
[2024-07-31 11:06:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:53 lr 0.000020	 wd 0.0500	time 0.2809 (0.2670)	loss 1.0452 (1.1776)	grad_norm 0.3505 (nan)	loss_scale 2048.0000 (2242.0304)	mem 12226MB
[2024-07-31 11:07:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:27 lr 0.000020	 wd 0.0500	time 0.2956 (0.2668)	loss 1.4043 (1.1768)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (2233.9492)	mem 12226MB
[2024-07-31 11:07:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2378 (0.2662)	loss 1.3735 (1.1765)	grad_norm 0.3259 (nan)	loss_scale 2048.0000 (2226.5142)	mem 12226MB
[2024-07-31 11:07:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 6 training takes 0:11:08
[2024-07-31 11:07:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.529 (11.529)	Loss 0.5234 (0.5234)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 11:07:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.664 Acc@5 97.688
[2024-07-31 11:07:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 11:07:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 11:08:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:16:17 lr 0.000020	 wd 0.0500	time 11.9015 (11.9015)	loss 0.9303 (0.9303)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:08:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.2419 (0.3698)	loss 1.2121 (1.1604)	grad_norm 0.3312 (0.3337)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:09:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:12:02 lr 0.000020	 wd 0.0500	time 0.2493 (0.3140)	loss 1.1425 (1.1826)	grad_norm 0.3389 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:09:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:10:50 lr 0.000020	 wd 0.0500	time 0.2598 (0.2955)	loss 1.3195 (1.1773)	grad_norm 0.3356 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:09:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:10:03 lr 0.000020	 wd 0.0500	time 0.2689 (0.2871)	loss 0.8393 (1.1825)	grad_norm 0.3588 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:10:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:09:25 lr 0.000020	 wd 0.0500	time 0.2430 (0.2822)	loss 1.3585 (1.1795)	grad_norm 0.3234 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:10:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:08:49 lr 0.000020	 wd 0.0500	time 0.2587 (0.2786)	loss 1.3247 (1.1836)	grad_norm 0.3384 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:11:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:08:17 lr 0.000020	 wd 0.0500	time 0.2687 (0.2760)	loss 0.9926 (1.1784)	grad_norm 0.3267 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:11:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:07:46 lr 0.000020	 wd 0.0500	time 0.2358 (0.2738)	loss 0.8887 (1.1760)	grad_norm 0.3292 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:12:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:07:16 lr 0.000020	 wd 0.0500	time 0.2701 (0.2723)	loss 1.4629 (1.1777)	grad_norm 0.3266 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:12:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:06:47 lr 0.000020	 wd 0.0500	time 0.2586 (0.2712)	loss 0.8127 (1.1777)	grad_norm 0.3582 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:12:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:06:18 lr 0.000020	 wd 0.0500	time 0.2783 (0.2701)	loss 1.3705 (1.1769)	grad_norm 0.3305 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:13:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:05:50 lr 0.000020	 wd 0.0500	time 0.2601 (0.2695)	loss 1.4111 (1.1735)	grad_norm 0.3359 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:13:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:05:23 lr 0.000020	 wd 0.0500	time 0.2566 (0.2690)	loss 1.3833 (1.1746)	grad_norm 0.3150 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:14:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:04:55 lr 0.000019	 wd 0.0500	time 0.2680 (0.2683)	loss 1.2919 (1.1743)	grad_norm 0.3262 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:14:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:28 lr 0.000019	 wd 0.0500	time 0.2529 (0.2679)	loss 0.8404 (1.1750)	grad_norm 0.3257 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:15:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:01 lr 0.000019	 wd 0.0500	time 0.2711 (0.2676)	loss 1.1162 (1.1756)	grad_norm 0.4634 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:15:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:34 lr 0.000019	 wd 0.0500	time 0.2683 (0.2674)	loss 1.3489 (1.1758)	grad_norm 0.3198 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:16:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:07 lr 0.000019	 wd 0.0500	time 0.2584 (0.2674)	loss 0.7577 (1.1781)	grad_norm 0.3264 (0.3342)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:16:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:40 lr 0.000019	 wd 0.0500	time 0.2668 (0.2673)	loss 1.1380 (1.1785)	grad_norm 0.3351 (0.3342)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:16:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:14 lr 0.000019	 wd 0.0500	time 0.2415 (0.2669)	loss 1.1781 (1.1792)	grad_norm 0.3357 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:17:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:47 lr 0.000019	 wd 0.0500	time 0.2620 (0.2667)	loss 1.4597 (1.1818)	grad_norm 0.3670 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:20 lr 0.000019	 wd 0.0500	time 0.2829 (0.2664)	loss 1.2437 (1.1800)	grad_norm 0.3261 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:18:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:53 lr 0.000019	 wd 0.0500	time 0.2532 (0.2664)	loss 1.4139 (1.1791)	grad_norm 0.3435 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:27 lr 0.000019	 wd 0.0500	time 0.2439 (0.2663)	loss 1.2625 (1.1795)	grad_norm 0.3301 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:19:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2716 (0.2657)	loss 1.5945 (1.1795)	grad_norm 0.3179 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:19:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 7 training takes 0:11:07
[2024-07-31 11:19:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.375 (12.375)	Loss 0.5010 (0.5010)	Acc@1 93.555 (93.555)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 11:19:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.678 Acc@5 97.686
[2024-07-31 11:19:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 11:19:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 11:19:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 11:19:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 11:19:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][0/2502]	eta 6:58:51 lr 0.000019	 wd 0.0500	time 10.0445 (10.0445)	loss 1.2693 (1.2693)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:20:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:14:33 lr 0.000019	 wd 0.0500	time 0.2520 (0.3635)	loss 0.9830 (1.2224)	grad_norm 0.3181 (0.3339)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:11:57 lr 0.000019	 wd 0.0500	time 0.2457 (0.3119)	loss 0.8315 (1.2079)	grad_norm 0.3308 (0.3342)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:21:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:10:48 lr 0.000019	 wd 0.0500	time 0.2926 (0.2943)	loss 1.4201 (1.1987)	grad_norm 0.3371 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:10:01 lr 0.000019	 wd 0.0500	time 0.3005 (0.2863)	loss 0.8494 (1.1974)	grad_norm 0.3353 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:21:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:09:22 lr 0.000019	 wd 0.0500	time 0.2990 (0.2809)	loss 0.8154 (1.1862)	grad_norm 0.3229 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:22:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:08:48 lr 0.000019	 wd 0.0500	time 0.2761 (0.2780)	loss 0.9126 (1.1832)	grad_norm 0.3542 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:22:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:08:19 lr 0.000019	 wd 0.0500	time 0.2523 (0.2771)	loss 1.3583 (1.1836)	grad_norm 0.3446 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:23:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:07:48 lr 0.000019	 wd 0.0500	time 0.2422 (0.2755)	loss 1.4607 (1.1853)	grad_norm 0.3431 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:23:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:07:19 lr 0.000019	 wd 0.0500	time 0.2511 (0.2742)	loss 0.9546 (1.1803)	grad_norm 0.3262 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:24:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:06:50 lr 0.000019	 wd 0.0500	time 0.2960 (0.2731)	loss 1.3653 (1.1838)	grad_norm 0.3336 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:24:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:06:21 lr 0.000019	 wd 0.0500	time 0.2523 (0.2722)	loss 1.2147 (1.1819)	grad_norm 0.3193 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:25:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:05:53 lr 0.000019	 wd 0.0500	time 0.2618 (0.2714)	loss 1.1253 (1.1816)	grad_norm 0.3475 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:25:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:05:25 lr 0.000019	 wd 0.0500	time 0.2460 (0.2709)	loss 0.9018 (1.1810)	grad_norm 0.3447 (0.3349)	loss_scale 4096.0000 (2136.1537)	mem 12226MB
[2024-07-31 11:25:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:04:57 lr 0.000019	 wd 0.0500	time 0.2540 (0.2701)	loss 1.2794 (1.1820)	grad_norm 0.3376 (0.3347)	loss_scale 4096.0000 (2276.0428)	mem 12226MB
[2024-07-31 11:26:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:04:30 lr 0.000019	 wd 0.0500	time 0.2716 (0.2696)	loss 1.1808 (1.1822)	grad_norm 0.3406 (0.3347)	loss_scale 4096.0000 (2397.2925)	mem 12226MB
[2024-07-31 11:26:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:03 lr 0.000019	 wd 0.0500	time 0.2726 (0.2695)	loss 1.3792 (1.1849)	grad_norm 0.3460 (0.3348)	loss_scale 4096.0000 (2503.3954)	mem 12226MB
[2024-07-31 11:27:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:35 lr 0.000019	 wd 0.0500	time 0.2422 (0.2689)	loss 0.7725 (1.1875)	grad_norm 0.3431 (0.3350)	loss_scale 4096.0000 (2597.0229)	mem 12226MB
[2024-07-31 11:27:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:08 lr 0.000019	 wd 0.0500	time 0.2849 (0.2685)	loss 0.7960 (1.1861)	grad_norm 0.3401 (0.3348)	loss_scale 4096.0000 (2680.2532)	mem 12226MB
[2024-07-31 11:28:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:41 lr 0.000019	 wd 0.0500	time 0.2415 (0.2684)	loss 1.1891 (1.1879)	grad_norm 0.3229 (0.3350)	loss_scale 4096.0000 (2754.7270)	mem 12226MB
[2024-07-31 11:28:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:14 lr 0.000019	 wd 0.0500	time 0.2879 (0.2689)	loss 0.8050 (1.1867)	grad_norm 0.3412 (0.3350)	loss_scale 4096.0000 (2821.7571)	mem 12226MB
[2024-07-31 11:29:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:47 lr 0.000019	 wd 0.0500	time 0.2558 (0.2686)	loss 0.8843 (1.1872)	grad_norm 0.3429 (0.3349)	loss_scale 4096.0000 (2882.4065)	mem 12226MB
[2024-07-31 11:29:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:21 lr 0.000019	 wd 0.0500	time 0.2763 (0.2683)	loss 1.1362 (1.1855)	grad_norm 0.3560 (0.3348)	loss_scale 4096.0000 (2937.5448)	mem 12226MB
[2024-07-31 11:29:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:54 lr 0.000019	 wd 0.0500	time 0.2994 (0.2683)	loss 1.3528 (1.1842)	grad_norm 0.3175 (0.3362)	loss_scale 4096.0000 (2987.8905)	mem 12226MB
[2024-07-31 11:30:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:27 lr 0.000019	 wd 0.0500	time 0.2764 (0.2684)	loss 1.1753 (1.1841)	grad_norm 0.3485 (0.3361)	loss_scale 4096.0000 (3034.0425)	mem 12226MB
[2024-07-31 11:30:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2372 (0.2677)	loss 0.9056 (1.1839)	grad_norm 0.3171 (0.3360)	loss_scale 4096.0000 (3076.5038)	mem 12226MB
[2024-07-31 11:30:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 8 training takes 0:11:12
[2024-07-31 11:31:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.245 (12.245)	Loss 0.5015 (0.5015)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 11:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.720 Acc@5 97.702
[2024-07-31 11:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 11:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 11:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 11:31:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 11:31:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:43:34 lr 0.000019	 wd 0.0500	time 11.1170 (11.1170)	loss 1.3608 (1.3608)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:31:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:14:31 lr 0.000019	 wd 0.0500	time 0.2441 (0.3629)	loss 1.2604 (1.1614)	grad_norm 0.3354 (0.3357)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:32:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:11:51 lr 0.000019	 wd 0.0500	time 0.2652 (0.3091)	loss 0.9249 (1.1853)	grad_norm 0.3342 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:32:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:10:47 lr 0.000019	 wd 0.0500	time 0.2571 (0.2939)	loss 1.5227 (1.1936)	grad_norm 0.3286 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:33:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:09:58 lr 0.000019	 wd 0.0500	time 0.2658 (0.2847)	loss 0.8552 (1.1770)	grad_norm 0.3306 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:33:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:09:20 lr 0.000019	 wd 0.0500	time 0.2449 (0.2802)	loss 0.7635 (1.1723)	grad_norm 0.3435 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:34:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:08:47 lr 0.000019	 wd 0.0500	time 0.2398 (0.2774)	loss 0.8400 (1.1765)	grad_norm 0.3289 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:34:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:08:15 lr 0.000019	 wd 0.0500	time 0.2767 (0.2747)	loss 1.4047 (1.1802)	grad_norm 0.3270 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:34:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.2743 (0.2733)	loss 1.3854 (1.1814)	grad_norm 0.3214 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:07:16 lr 0.000019	 wd 0.0500	time 0.2524 (0.2722)	loss 1.3661 (1.1809)	grad_norm 0.3243 (0.3341)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:35:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:06:46 lr 0.000019	 wd 0.0500	time 0.2687 (0.2706)	loss 1.1728 (1.1817)	grad_norm 0.3231 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:36:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:06:17 lr 0.000018	 wd 0.0500	time 0.2526 (0.2694)	loss 1.3429 (1.1776)	grad_norm 0.3332 (0.3337)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:36:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:05:49 lr 0.000018	 wd 0.0500	time 0.2543 (0.2686)	loss 0.8430 (1.1765)	grad_norm 0.3356 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 11:37:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:05:23 lr 0.000018	 wd 0.0500	time 0.2381 (0.2691)	loss 1.2853 (1.1774)	grad_norm 0.3278 (nan)	loss_scale 2048.0000 (4007.8463)	mem 12226MB
[2024-07-31 11:37:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:04:56 lr 0.000018	 wd 0.0500	time 0.2502 (0.2687)	loss 1.0708 (1.1765)	grad_norm 0.3237 (nan)	loss_scale 2048.0000 (3867.9572)	mem 12226MB
[2024-07-31 11:37:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:28 lr 0.000018	 wd 0.0500	time 0.2515 (0.2682)	loss 0.8690 (1.1798)	grad_norm 0.3464 (nan)	loss_scale 2048.0000 (3746.7075)	mem 12226MB
[2024-07-31 11:38:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:04:02 lr 0.000018	 wd 0.0500	time 0.2479 (0.2692)	loss 0.9291 (1.1780)	grad_norm 0.3308 (nan)	loss_scale 2048.0000 (3640.6046)	mem 12226MB
[2024-07-31 11:38:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:35 lr 0.000018	 wd 0.0500	time 0.2495 (0.2687)	loss 1.4636 (1.1785)	grad_norm 0.3416 (nan)	loss_scale 2048.0000 (3546.9771)	mem 12226MB
[2024-07-31 11:39:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:08 lr 0.000018	 wd 0.0500	time 0.2760 (0.2683)	loss 1.2764 (1.1772)	grad_norm 0.3355 (nan)	loss_scale 2048.0000 (3463.7468)	mem 12226MB
[2024-07-31 11:39:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:41 lr 0.000018	 wd 0.0500	time 0.2840 (0.2679)	loss 1.4201 (1.1767)	grad_norm 0.3369 (nan)	loss_scale 2048.0000 (3389.2730)	mem 12226MB
[2024-07-31 11:40:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:14 lr 0.000018	 wd 0.0500	time 0.2825 (0.2677)	loss 1.4088 (1.1772)	grad_norm 0.3105 (nan)	loss_scale 2048.0000 (3322.2429)	mem 12226MB
[2024-07-31 11:40:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:47 lr 0.000018	 wd 0.0500	time 0.2624 (0.2674)	loss 1.4160 (1.1789)	grad_norm 0.3414 (nan)	loss_scale 2048.0000 (3261.5935)	mem 12226MB
[2024-07-31 11:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:20 lr 0.000018	 wd 0.0500	time 0.2770 (0.2675)	loss 1.1763 (1.1800)	grad_norm 0.3404 (nan)	loss_scale 2048.0000 (3206.4552)	mem 12226MB
[2024-07-31 11:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:54 lr 0.000018	 wd 0.0500	time 0.2465 (0.2676)	loss 0.7904 (1.1799)	grad_norm 0.3354 (nan)	loss_scale 2048.0000 (3156.1095)	mem 12226MB
[2024-07-31 11:42:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:27 lr 0.000018	 wd 0.0500	time 0.2653 (0.2678)	loss 1.0925 (1.1802)	grad_norm 0.3405 (nan)	loss_scale 2048.0000 (3109.9575)	mem 12226MB
[2024-07-31 11:42:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2381 (0.2673)	loss 1.2622 (1.1797)	grad_norm 0.3356 (nan)	loss_scale 2048.0000 (3067.4962)	mem 12226MB
[2024-07-31 11:42:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 9 training takes 0:11:11
[2024-07-31 11:42:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.550 (12.550)	Loss 0.5132 (0.5132)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 11:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.746 Acc@5 97.710
[2024-07-31 11:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 11:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 11:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 11:42:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 11:43:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:20:37 lr 0.000018	 wd 0.0500	time 10.5664 (10.5664)	loss 1.6340 (1.6340)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:43:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:14:39 lr 0.000018	 wd 0.0500	time 0.2561 (0.3662)	loss 0.8806 (1.1903)	grad_norm 0.3649 (0.3339)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:44:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:12:01 lr 0.000018	 wd 0.0500	time 0.2505 (0.3132)	loss 1.4667 (1.1856)	grad_norm 0.3132 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:44:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:10:49 lr 0.000018	 wd 0.0500	time 0.2812 (0.2948)	loss 1.3298 (1.1807)	grad_norm 0.3507 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:09:59 lr 0.000018	 wd 0.0500	time 0.2678 (0.2850)	loss 0.9166 (1.1848)	grad_norm 0.3428 (0.3341)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:45:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:09:18 lr 0.000018	 wd 0.0500	time 0.2719 (0.2792)	loss 0.7089 (1.1804)	grad_norm 0.3246 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:45:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:08:43 lr 0.000018	 wd 0.0500	time 0.2448 (0.2753)	loss 1.1842 (1.1881)	grad_norm 0.3225 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:46:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:08:11 lr 0.000018	 wd 0.0500	time 0.2507 (0.2726)	loss 1.2967 (1.1861)	grad_norm 0.3097 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:46:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:07:40 lr 0.000018	 wd 0.0500	time 0.2796 (0.2705)	loss 1.2953 (1.1888)	grad_norm 0.3294 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:47:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:07:11 lr 0.000018	 wd 0.0500	time 0.2542 (0.2693)	loss 1.4691 (1.1863)	grad_norm 0.3318 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:47:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:06:42 lr 0.000018	 wd 0.0500	time 0.2388 (0.2679)	loss 1.2854 (1.1874)	grad_norm 0.3336 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:47:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:06:14 lr 0.000018	 wd 0.0500	time 0.2569 (0.2671)	loss 1.0150 (1.1934)	grad_norm 0.3453 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:48:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:05:46 lr 0.000018	 wd 0.0500	time 0.2517 (0.2665)	loss 1.4134 (1.1908)	grad_norm 0.3266 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:48:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:05:20 lr 0.000018	 wd 0.0500	time 0.2582 (0.2666)	loss 1.3817 (1.1883)	grad_norm 0.3280 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:49:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:04:54 lr 0.000018	 wd 0.0500	time 0.2870 (0.2670)	loss 1.3502 (1.1862)	grad_norm 0.3278 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:49:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:04:27 lr 0.000018	 wd 0.0500	time 0.2558 (0.2669)	loss 1.1848 (1.1836)	grad_norm 0.3476 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:50:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:04:00 lr 0.000018	 wd 0.0500	time 0.2641 (0.2665)	loss 1.1313 (1.1822)	grad_norm 0.3201 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:50:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:33 lr 0.000018	 wd 0.0500	time 0.2524 (0.2661)	loss 1.2471 (1.1842)	grad_norm 0.3276 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:50:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:06 lr 0.000018	 wd 0.0500	time 0.2589 (0.2658)	loss 1.1527 (1.1832)	grad_norm 0.3508 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:51:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:39 lr 0.000018	 wd 0.0500	time 0.2396 (0.2655)	loss 1.1088 (1.1822)	grad_norm 0.3276 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:13 lr 0.000017	 wd 0.0500	time 0.2453 (0.2654)	loss 1.3535 (1.1857)	grad_norm 0.3357 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:52:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:46 lr 0.000017	 wd 0.0500	time 0.2513 (0.2656)	loss 1.2143 (1.1870)	grad_norm 0.3133 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:52:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:20 lr 0.000017	 wd 0.0500	time 0.2477 (0.2656)	loss 1.3489 (1.1881)	grad_norm 0.3176 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:53:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:53 lr 0.000017	 wd 0.0500	time 0.2408 (0.2654)	loss 1.3130 (1.1860)	grad_norm 0.3342 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:53:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:27 lr 0.000017	 wd 0.0500	time 0.2676 (0.2652)	loss 1.3853 (1.1850)	grad_norm 0.3206 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2377 (0.2645)	loss 0.9092 (1.1846)	grad_norm 0.3296 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 10 training takes 0:11:04
[2024-07-31 11:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_10.pth saving......
[2024-07-31 11:54:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_10.pth saved !!!
[2024-07-31 11:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 10.742 (10.742)	Loss 0.5063 (0.5063)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 11:54:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.734 Acc@5 97.726
[2024-07-31 11:54:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 11:54:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 11:54:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:38:46 lr 0.000017	 wd 0.0500	time 11.0018 (11.0018)	loss 0.9120 (0.9120)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:55:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:14:50 lr 0.000017	 wd 0.0500	time 0.2629 (0.3707)	loss 1.4023 (1.1700)	grad_norm 0.3242 (0.3510)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:55:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:12:13 lr 0.000017	 wd 0.0500	time 0.2551 (0.3188)	loss 1.3483 (1.1907)	grad_norm 0.3371 (0.3432)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 11:55:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:10:59 lr 0.000017	 wd 0.0500	time 0.2393 (0.2995)	loss 0.8703 (1.1804)	grad_norm 0.3251 (0.3402)	loss_scale 4096.0000 (2456.2392)	mem 12226MB
[2024-07-31 11:56:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:10:06 lr 0.000017	 wd 0.0500	time 0.2456 (0.2887)	loss 1.4313 (1.1790)	grad_norm 0.3451 (0.3386)	loss_scale 4096.0000 (2865.1571)	mem 12226MB
[2024-07-31 11:56:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:09:24 lr 0.000017	 wd 0.0500	time 0.2372 (0.2818)	loss 0.7470 (1.1784)	grad_norm 0.3130 (0.3376)	loss_scale 4096.0000 (3110.8343)	mem 12226MB
[2024-07-31 11:57:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:08:47 lr 0.000017	 wd 0.0500	time 0.2766 (0.2775)	loss 1.4679 (1.1790)	grad_norm 0.3313 (0.3377)	loss_scale 4096.0000 (3274.7554)	mem 12226MB
[2024-07-31 11:57:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:08:16 lr 0.000017	 wd 0.0500	time 0.2674 (0.2753)	loss 1.0151 (1.1743)	grad_norm 0.3530 (0.3373)	loss_scale 4096.0000 (3391.9087)	mem 12226MB
[2024-07-31 11:58:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:07:46 lr 0.000017	 wd 0.0500	time 0.2443 (0.2743)	loss 1.5073 (1.1724)	grad_norm 0.3240 (0.3366)	loss_scale 4096.0000 (3479.8102)	mem 12226MB
[2024-07-31 11:58:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:07:17 lr 0.000017	 wd 0.0500	time 0.2764 (0.2733)	loss 1.0208 (1.1752)	grad_norm 0.3460 (0.3361)	loss_scale 4096.0000 (3548.1998)	mem 12226MB
[2024-07-31 11:59:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:06:47 lr 0.000017	 wd 0.0500	time 0.2548 (0.2716)	loss 1.1762 (1.1753)	grad_norm 0.3338 (0.3358)	loss_scale 4096.0000 (3602.9251)	mem 12226MB
[2024-07-31 11:59:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:06:19 lr 0.000017	 wd 0.0500	time 0.2396 (0.2704)	loss 0.9165 (1.1738)	grad_norm 0.3386 (0.3355)	loss_scale 4096.0000 (3647.7094)	mem 12226MB
[2024-07-31 11:59:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:05:50 lr 0.000017	 wd 0.0500	time 0.2559 (0.2692)	loss 0.9465 (1.1706)	grad_norm 0.3262 (0.3354)	loss_scale 4096.0000 (3685.0358)	mem 12226MB
[2024-07-31 12:00:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:05:22 lr 0.000017	 wd 0.0500	time 0.2450 (0.2686)	loss 1.4062 (1.1710)	grad_norm 0.3460 (0.3352)	loss_scale 4096.0000 (3716.6241)	mem 12226MB
[2024-07-31 12:00:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:04:56 lr 0.000017	 wd 0.0500	time 0.2574 (0.2687)	loss 0.8230 (1.1712)	grad_norm 0.3414 (0.3350)	loss_scale 4096.0000 (3743.7031)	mem 12226MB
[2024-07-31 12:01:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:28 lr 0.000017	 wd 0.0500	time 0.2875 (0.2684)	loss 1.4040 (1.1720)	grad_norm 0.3368 (0.3349)	loss_scale 4096.0000 (3767.1739)	mem 12226MB
[2024-07-31 12:01:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:04:02 lr 0.000017	 wd 0.0500	time 0.2454 (0.2683)	loss 1.2952 (1.1731)	grad_norm 0.3312 (0.3350)	loss_scale 4096.0000 (3787.7127)	mem 12226MB
[2024-07-31 12:02:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:34 lr 0.000017	 wd 0.0500	time 0.2531 (0.2679)	loss 1.0885 (1.1722)	grad_norm 0.3274 (0.3348)	loss_scale 4096.0000 (3805.8366)	mem 12226MB
[2024-07-31 12:02:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:07 lr 0.000017	 wd 0.0500	time 0.2468 (0.2676)	loss 0.8515 (1.1722)	grad_norm 0.3203 (0.3349)	loss_scale 4096.0000 (3821.9478)	mem 12226MB
[2024-07-31 12:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:41 lr 0.000017	 wd 0.0500	time 0.2689 (0.2676)	loss 1.1438 (1.1713)	grad_norm 0.3437 (0.3351)	loss_scale 4096.0000 (3836.3640)	mem 12226MB
[2024-07-31 12:03:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:14 lr 0.000017	 wd 0.0500	time 0.2716 (0.2677)	loss 1.0693 (1.1715)	grad_norm 0.3092 (0.3350)	loss_scale 4096.0000 (3849.3393)	mem 12226MB
[2024-07-31 12:03:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:47 lr 0.000017	 wd 0.0500	time 0.2535 (0.2674)	loss 1.3680 (1.1707)	grad_norm 0.3462 (0.3349)	loss_scale 4096.0000 (3861.0795)	mem 12226MB
[2024-07-31 12:04:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:20 lr 0.000017	 wd 0.0500	time 0.2762 (0.2672)	loss 0.9992 (1.1726)	grad_norm 0.3444 (0.3348)	loss_scale 4096.0000 (3871.7528)	mem 12226MB
[2024-07-31 12:04:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:53 lr 0.000016	 wd 0.0500	time 0.2404 (0.2668)	loss 1.1836 (1.1707)	grad_norm 0.3433 (0.3347)	loss_scale 4096.0000 (3881.4985)	mem 12226MB
[2024-07-31 12:05:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:27 lr 0.000016	 wd 0.0500	time 0.3127 (0.2667)	loss 1.4083 (1.1719)	grad_norm 0.3263 (0.3346)	loss_scale 4096.0000 (3890.4323)	mem 12226MB
[2024-07-31 12:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2374 (0.2662)	loss 0.8614 (1.1702)	grad_norm 0.3103 (0.3345)	loss_scale 4096.0000 (3898.6517)	mem 12226MB
[2024-07-31 12:05:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 11 training takes 0:11:08
[2024-07-31 12:05:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.374 (12.374)	Loss 0.5044 (0.5044)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 12:06:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.728 Acc@5 97.704
[2024-07-31 12:06:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 12:06:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 12:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:30:08 lr 0.000016	 wd 0.0500	time 12.2335 (12.2335)	loss 0.9507 (0.9507)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:06:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:15:24 lr 0.000016	 wd 0.0500	time 0.2372 (0.3847)	loss 1.3226 (1.1740)	grad_norm 0.3273 (0.3321)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:07:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:12:15 lr 0.000016	 wd 0.0500	time 0.2435 (0.3196)	loss 1.3154 (1.1714)	grad_norm 0.3265 (0.3332)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:07:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:10:57 lr 0.000016	 wd 0.0500	time 0.2486 (0.2987)	loss 0.7570 (1.1839)	grad_norm 0.3563 (0.3325)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:08:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:10:15 lr 0.000016	 wd 0.0500	time 0.2400 (0.2928)	loss 1.2445 (1.1751)	grad_norm 0.3262 (0.3325)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:08:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:09:33 lr 0.000016	 wd 0.0500	time 0.2622 (0.2863)	loss 1.4534 (1.1750)	grad_norm 0.3295 (0.3325)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:08:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:08:56 lr 0.000016	 wd 0.0500	time 0.2437 (0.2820)	loss 1.4265 (1.1734)	grad_norm 0.3410 (0.3327)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:09:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:08:22 lr 0.000016	 wd 0.0500	time 0.2692 (0.2790)	loss 1.2262 (1.1715)	grad_norm 0.3347 (0.3332)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:09:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:07:51 lr 0.000016	 wd 0.0500	time 0.2630 (0.2769)	loss 1.3804 (1.1727)	grad_norm 0.3260 (0.3335)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:10:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:07:21 lr 0.000016	 wd 0.0500	time 0.2415 (0.2759)	loss 1.5585 (1.1703)	grad_norm 0.3299 (0.3336)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:10:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:06:51 lr 0.000016	 wd 0.0500	time 0.2428 (0.2742)	loss 1.3958 (1.1725)	grad_norm 0.3436 (0.3335)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:11:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:06:23 lr 0.000016	 wd 0.0500	time 0.2415 (0.2734)	loss 0.8259 (1.1740)	grad_norm 0.3312 (0.3337)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:11:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:05:54 lr 0.000016	 wd 0.0500	time 0.2559 (0.2726)	loss 1.4654 (1.1722)	grad_norm 0.3330 (0.3337)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:11:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:05:26 lr 0.000016	 wd 0.0500	time 0.2607 (0.2717)	loss 1.0595 (1.1732)	grad_norm 0.3190 (0.3341)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:12:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:04:58 lr 0.000016	 wd 0.0500	time 0.2523 (0.2710)	loss 1.3519 (1.1729)	grad_norm 0.3207 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:12:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:04:30 lr 0.000016	 wd 0.0500	time 0.2535 (0.2703)	loss 0.8166 (1.1703)	grad_norm 0.3478 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:13:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:04:03 lr 0.000016	 wd 0.0500	time 0.2502 (0.2700)	loss 1.3850 (1.1670)	grad_norm 0.3141 (nan)	loss_scale 2048.0000 (3975.7552)	mem 12226MB
[2024-07-31 12:13:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:36 lr 0.000016	 wd 0.0500	time 0.2440 (0.2703)	loss 1.0823 (1.1692)	grad_norm 0.3388 (nan)	loss_scale 2048.0000 (3862.4245)	mem 12226MB
[2024-07-31 12:14:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:09 lr 0.000016	 wd 0.0500	time 0.2681 (0.2698)	loss 0.8395 (1.1677)	grad_norm 0.3293 (nan)	loss_scale 2048.0000 (3761.6791)	mem 12226MB
[2024-07-31 12:14:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:42 lr 0.000016	 wd 0.0500	time 0.2403 (0.2694)	loss 1.3297 (1.1685)	grad_norm 0.3342 (nan)	loss_scale 2048.0000 (3671.5329)	mem 12226MB
[2024-07-31 12:15:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:15 lr 0.000016	 wd 0.0500	time 0.2451 (0.2690)	loss 1.0692 (1.1693)	grad_norm 0.3284 (nan)	loss_scale 2048.0000 (3590.3968)	mem 12226MB
[2024-07-31 12:15:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:48 lr 0.000016	 wd 0.0500	time 0.2389 (0.2687)	loss 0.8231 (1.1686)	grad_norm 0.3023 (nan)	loss_scale 2048.0000 (3516.9843)	mem 12226MB
[2024-07-31 12:15:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:21 lr 0.000016	 wd 0.0500	time 0.2808 (0.2684)	loss 0.8253 (1.1701)	grad_norm 0.3410 (nan)	loss_scale 2048.0000 (3450.2426)	mem 12226MB
[2024-07-31 12:16:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:54 lr 0.000015	 wd 0.0500	time 0.2418 (0.2680)	loss 1.2155 (1.1706)	grad_norm 0.3269 (nan)	loss_scale 2048.0000 (3389.3020)	mem 12226MB
[2024-07-31 12:16:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:27 lr 0.000015	 wd 0.0500	time 0.2723 (0.2678)	loss 1.3806 (1.1709)	grad_norm 0.3161 (nan)	loss_scale 2048.0000 (3333.4377)	mem 12226MB
[2024-07-31 12:17:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.2375 (0.2671)	loss 0.9492 (1.1702)	grad_norm 0.3351 (nan)	loss_scale 2048.0000 (3282.0408)	mem 12226MB
[2024-07-31 12:17:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 12 training takes 0:11:10
[2024-07-31 12:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.123 (12.123)	Loss 0.5142 (0.5142)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 12:17:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.710 Acc@5 97.728
[2024-07-31 12:17:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 12:17:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 12:17:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:59:14 lr 0.000015	 wd 0.0500	time 11.4927 (11.4927)	loss 1.3503 (1.3503)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:18:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:15:03 lr 0.000015	 wd 0.0500	time 0.2406 (0.3759)	loss 1.3754 (1.2355)	grad_norm 0.3332 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:12:11 lr 0.000015	 wd 0.0500	time 0.2415 (0.3179)	loss 1.4157 (1.2228)	grad_norm 0.3270 (0.3381)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:19:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:10:58 lr 0.000015	 wd 0.0500	time 0.2526 (0.2988)	loss 1.4037 (1.2230)	grad_norm 0.3292 (0.3364)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:10:06 lr 0.000015	 wd 0.0500	time 0.2424 (0.2886)	loss 1.4767 (1.2033)	grad_norm 0.3381 (0.3357)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:20:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:09:25 lr 0.000015	 wd 0.0500	time 0.2375 (0.2824)	loss 1.3110 (1.1966)	grad_norm 0.3521 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:20:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:08:50 lr 0.000015	 wd 0.0500	time 0.2432 (0.2788)	loss 1.1983 (1.1905)	grad_norm 0.3302 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:20:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:08:18 lr 0.000015	 wd 0.0500	time 0.2627 (0.2764)	loss 1.1997 (1.1813)	grad_norm 0.3437 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:07:46 lr 0.000015	 wd 0.0500	time 0.2753 (0.2741)	loss 1.2466 (1.1805)	grad_norm 0.3482 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:21:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:07:17 lr 0.000015	 wd 0.0500	time 0.2814 (0.2732)	loss 1.3274 (1.1811)	grad_norm 0.3456 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:22:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:06:48 lr 0.000015	 wd 0.0500	time 0.2393 (0.2720)	loss 1.3940 (1.1800)	grad_norm 0.3190 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:22:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:06:19 lr 0.000015	 wd 0.0500	time 0.2511 (0.2708)	loss 1.4069 (1.1759)	grad_norm 0.3373 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:23:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:05:51 lr 0.000015	 wd 0.0500	time 0.2664 (0.2698)	loss 1.1934 (1.1800)	grad_norm 0.3394 (0.3342)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:23:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:05:23 lr 0.000015	 wd 0.0500	time 0.2430 (0.2695)	loss 0.7356 (1.1786)	grad_norm 0.3345 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:24:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:04:56 lr 0.000015	 wd 0.0500	time 0.2453 (0.2688)	loss 1.3837 (1.1773)	grad_norm 0.3299 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:24:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:28 lr 0.000015	 wd 0.0500	time 0.2466 (0.2682)	loss 1.2335 (1.1779)	grad_norm 0.3412 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:24:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:04:01 lr 0.000015	 wd 0.0500	time 0.2667 (0.2679)	loss 0.9336 (1.1769)	grad_norm 0.3348 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:25:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:34 lr 0.000015	 wd 0.0500	time 0.2758 (0.2677)	loss 1.0500 (1.1781)	grad_norm 0.3276 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:25:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:08 lr 0.000015	 wd 0.0500	time 0.2663 (0.2678)	loss 1.2569 (1.1784)	grad_norm 0.3332 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:26:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:41 lr 0.000015	 wd 0.0500	time 0.2386 (0.2676)	loss 1.0958 (1.1786)	grad_norm 0.3324 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:26:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:14 lr 0.000015	 wd 0.0500	time 0.2726 (0.2675)	loss 1.4054 (1.1784)	grad_norm 0.3176 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:27:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:47 lr 0.000014	 wd 0.0500	time 0.2398 (0.2674)	loss 1.3810 (1.1785)	grad_norm 0.3334 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:27:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:20 lr 0.000014	 wd 0.0500	time 0.2765 (0.2672)	loss 1.2145 (1.1771)	grad_norm 0.3287 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:27:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:53 lr 0.000014	 wd 0.0500	time 0.2536 (0.2672)	loss 1.3694 (1.1760)	grad_norm 0.3435 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:28:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:27 lr 0.000014	 wd 0.0500	time 0.2426 (0.2672)	loss 0.9487 (1.1788)	grad_norm 0.3140 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:28:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.2597 (0.2665)	loss 0.7927 (1.1775)	grad_norm 0.3291 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:28:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 13 training takes 0:11:09
[2024-07-31 12:29:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 10.653 (10.653)	Loss 0.4902 (0.4902)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 12:29:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.752 Acc@5 97.752
[2024-07-31 12:29:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 12:29:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 12:29:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 12:29:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 12:29:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][0/2502]	eta 6:45:43 lr 0.000014	 wd 0.0500	time 9.7298 (9.7298)	loss 1.4302 (1.4302)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:29:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:14:31 lr 0.000014	 wd 0.0500	time 0.2484 (0.3628)	loss 1.1991 (1.1950)	grad_norm 0.3317 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:30:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:11:59 lr 0.000014	 wd 0.0500	time 0.2775 (0.3125)	loss 1.4871 (1.1937)	grad_norm 0.3237 (0.3334)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:30:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:10:54 lr 0.000014	 wd 0.0500	time 0.2783 (0.2971)	loss 0.9454 (1.1965)	grad_norm 0.3453 (0.3334)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:31:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:10:09 lr 0.000014	 wd 0.0500	time 0.2500 (0.2899)	loss 1.6066 (1.1958)	grad_norm 0.3221 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:09:27 lr 0.000014	 wd 0.0500	time 0.2540 (0.2835)	loss 0.9603 (1.1912)	grad_norm 0.3221 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 12:32:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:08:51 lr 0.000014	 wd 0.0500	time 0.2864 (0.2794)	loss 0.9760 (1.1902)	grad_norm 0.3434 (0.3345)	loss_scale 4096.0000 (2381.9501)	mem 12226MB
[2024-07-31 12:32:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:08:20 lr 0.000014	 wd 0.0500	time 0.2442 (0.2778)	loss 1.1415 (1.1908)	grad_norm 0.3368 (0.3349)	loss_scale 4096.0000 (2626.4650)	mem 12226MB
[2024-07-31 12:33:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:07:48 lr 0.000014	 wd 0.0500	time 0.2620 (0.2755)	loss 0.8104 (1.1846)	grad_norm 0.3186 (0.3350)	loss_scale 4096.0000 (2809.9276)	mem 12226MB
[2024-07-31 12:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:07:18 lr 0.000014	 wd 0.0500	time 0.2391 (0.2740)	loss 0.8643 (1.1834)	grad_norm 0.3423 (0.3350)	loss_scale 4096.0000 (2952.6659)	mem 12226MB
[2024-07-31 12:33:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:06:49 lr 0.000014	 wd 0.0500	time 0.2818 (0.2724)	loss 1.0063 (1.1808)	grad_norm 0.3424 (0.3347)	loss_scale 4096.0000 (3066.8851)	mem 12226MB
[2024-07-31 12:34:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:06:20 lr 0.000014	 wd 0.0500	time 0.2902 (0.2714)	loss 1.3941 (1.1803)	grad_norm 0.3461 (0.3346)	loss_scale 4096.0000 (3160.3560)	mem 12226MB
[2024-07-31 12:34:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:05:52 lr 0.000014	 wd 0.0500	time 0.2399 (0.2710)	loss 1.4897 (1.1800)	grad_norm 0.3434 (0.3347)	loss_scale 4096.0000 (3238.2614)	mem 12226MB
[2024-07-31 12:35:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:05:24 lr 0.000014	 wd 0.0500	time 0.2668 (0.2703)	loss 1.2568 (1.1833)	grad_norm 0.3340 (0.3345)	loss_scale 4096.0000 (3304.1906)	mem 12226MB
[2024-07-31 12:35:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:04:57 lr 0.000014	 wd 0.0500	time 0.2674 (0.2697)	loss 1.2574 (1.1830)	grad_norm 0.3325 (0.3346)	loss_scale 4096.0000 (3360.7081)	mem 12226MB
[2024-07-31 12:36:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:29 lr 0.000014	 wd 0.0500	time 0.2683 (0.2691)	loss 1.4229 (1.1820)	grad_norm 0.3266 (0.3345)	loss_scale 4096.0000 (3409.6949)	mem 12226MB
[2024-07-31 12:36:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:04:02 lr 0.000014	 wd 0.0500	time 0.2662 (0.2687)	loss 0.7695 (1.1807)	grad_norm 0.3313 (0.3345)	loss_scale 4096.0000 (3452.5621)	mem 12226MB
[2024-07-31 12:36:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:35 lr 0.000014	 wd 0.0500	time 0.2433 (0.2684)	loss 0.9499 (1.1796)	grad_norm 0.3264 (0.3345)	loss_scale 4096.0000 (3490.3892)	mem 12226MB
[2024-07-31 12:37:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:08 lr 0.000013	 wd 0.0500	time 0.2963 (0.2681)	loss 0.9173 (1.1802)	grad_norm 0.3393 (0.3344)	loss_scale 4096.0000 (3524.0155)	mem 12226MB
[2024-07-31 12:37:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:41 lr 0.000013	 wd 0.0500	time 0.2553 (0.2678)	loss 1.1863 (1.1806)	grad_norm 0.3534 (0.3346)	loss_scale 4096.0000 (3554.1042)	mem 12226MB
[2024-07-31 12:38:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:14 lr 0.000013	 wd 0.0500	time 0.2962 (0.2677)	loss 1.2402 (1.1799)	grad_norm 0.3335 (0.3344)	loss_scale 4096.0000 (3581.1854)	mem 12226MB
[2024-07-31 12:38:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:47 lr 0.000013	 wd 0.0500	time 0.2769 (0.2678)	loss 1.3790 (1.1792)	grad_norm 0.3358 (0.3343)	loss_scale 4096.0000 (3605.6887)	mem 12226MB
[2024-07-31 12:39:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:20 lr 0.000013	 wd 0.0500	time 0.2435 (0.2673)	loss 1.3267 (1.1783)	grad_norm 0.3331 (0.3343)	loss_scale 4096.0000 (3627.9655)	mem 12226MB
[2024-07-31 12:39:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:53 lr 0.000013	 wd 0.0500	time 0.2406 (0.2668)	loss 0.8508 (1.1771)	grad_norm 0.3331 (0.3342)	loss_scale 4096.0000 (3648.3060)	mem 12226MB
[2024-07-31 12:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:27 lr 0.000013	 wd 0.0500	time 0.2399 (0.2666)	loss 0.8771 (1.1763)	grad_norm 0.3464 (0.3340)	loss_scale 4096.0000 (3666.9521)	mem 12226MB
[2024-07-31 12:40:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2374 (0.2660)	loss 1.2802 (1.1752)	grad_norm 0.3278 (0.3339)	loss_scale 4096.0000 (3684.1072)	mem 12226MB
[2024-07-31 12:40:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 14 training takes 0:11:07
[2024-07-31 12:40:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 10.716 (10.716)	Loss 0.4814 (0.4814)	Acc@1 93.555 (93.555)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 12:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.778 Acc@5 97.770
[2024-07-31 12:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 12:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-07-31 12:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 12:40:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 12:41:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:36:41 lr 0.000013	 wd 0.0500	time 10.9518 (10.9518)	loss 1.2914 (1.2914)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:41:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:14:33 lr 0.000013	 wd 0.0500	time 0.2398 (0.3637)	loss 0.7949 (1.1824)	grad_norm 0.3368 (0.3336)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:42:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:11:50 lr 0.000013	 wd 0.0500	time 0.2478 (0.3089)	loss 1.4648 (1.1675)	grad_norm 0.3436 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:42:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:10:43 lr 0.000013	 wd 0.0500	time 0.2918 (0.2922)	loss 0.9285 (1.1667)	grad_norm 0.3404 (0.3363)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:42:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:09:57 lr 0.000013	 wd 0.0500	time 0.2510 (0.2844)	loss 0.8147 (1.1654)	grad_norm 0.3321 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:43:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:09:18 lr 0.000013	 wd 0.0500	time 0.2568 (0.2790)	loss 0.9606 (1.1768)	grad_norm 0.3463 (0.3356)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:43:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:08:45 lr 0.000013	 wd 0.0500	time 0.2415 (0.2763)	loss 1.4174 (1.1796)	grad_norm 0.3343 (0.3356)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:44:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:08:14 lr 0.000013	 wd 0.0500	time 0.2918 (0.2743)	loss 1.2571 (1.1795)	grad_norm 0.3429 (0.3353)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:44:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:07:44 lr 0.000013	 wd 0.0500	time 0.2408 (0.2731)	loss 1.4214 (1.1811)	grad_norm 0.3180 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:45:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:07:15 lr 0.000013	 wd 0.0500	time 0.2725 (0.2718)	loss 1.2941 (1.1808)	grad_norm 0.3349 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:45:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:06:47 lr 0.000013	 wd 0.0500	time 0.2830 (0.2711)	loss 0.8601 (1.1808)	grad_norm 0.3301 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:06:18 lr 0.000013	 wd 0.0500	time 0.2865 (0.2702)	loss 0.8784 (1.1816)	grad_norm 0.3245 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:46:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:05:51 lr 0.000013	 wd 0.0500	time 0.2381 (0.2698)	loss 0.8362 (1.1816)	grad_norm 0.3529 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:46:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:05:23 lr 0.000013	 wd 0.0500	time 0.2399 (0.2693)	loss 0.8827 (1.1827)	grad_norm 0.3352 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:47:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:04:57 lr 0.000012	 wd 0.0500	time 0.2434 (0.2696)	loss 0.9783 (1.1828)	grad_norm 0.3360 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:47:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:29 lr 0.000012	 wd 0.0500	time 0.2632 (0.2691)	loss 1.0274 (1.1789)	grad_norm 0.3347 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:48:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:04:02 lr 0.000012	 wd 0.0500	time 0.2639 (0.2687)	loss 1.0245 (1.1798)	grad_norm 0.3401 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:48:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:35 lr 0.000012	 wd 0.0500	time 0.2392 (0.2684)	loss 0.8382 (1.1799)	grad_norm 0.3402 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:49:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:08 lr 0.000012	 wd 0.0500	time 0.2394 (0.2683)	loss 1.6850 (1.1779)	grad_norm 0.3618 (0.3365)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:49:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:41 lr 0.000012	 wd 0.0500	time 0.2462 (0.2680)	loss 0.9918 (1.1761)	grad_norm 0.3574 (0.3363)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:49:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:14 lr 0.000012	 wd 0.0500	time 0.2629 (0.2677)	loss 0.8547 (1.1774)	grad_norm 0.3370 (0.3362)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 12:50:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:47 lr 0.000012	 wd 0.0500	time 0.2686 (0.2676)	loss 0.7965 (1.1762)	grad_norm 0.3355 (0.3360)	loss_scale 8192.0000 (4290.9548)	mem 12226MB
[2024-07-31 12:50:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:20 lr 0.000012	 wd 0.0500	time 0.2516 (0.2674)	loss 1.0963 (1.1758)	grad_norm 0.3229 (0.3359)	loss_scale 8192.0000 (4468.1945)	mem 12226MB
[2024-07-31 12:51:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:54 lr 0.000012	 wd 0.0500	time 0.2810 (0.2675)	loss 1.0399 (1.1761)	grad_norm 0.3438 (0.3358)	loss_scale 8192.0000 (4630.0287)	mem 12226MB
[2024-07-31 12:51:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:27 lr 0.000012	 wd 0.0500	time 0.2904 (0.2673)	loss 1.4842 (1.1764)	grad_norm 0.3206 (0.3361)	loss_scale 8192.0000 (4778.3823)	mem 12226MB
[2024-07-31 12:52:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2352 (0.2666)	loss 1.5373 (1.1770)	grad_norm 0.3412 (0.3361)	loss_scale 8192.0000 (4914.8725)	mem 12226MB
[2024-07-31 12:52:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 15 training takes 0:11:09
[2024-07-31 12:52:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.237 (12.237)	Loss 0.5322 (0.5322)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 12:52:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.702 Acc@5 97.738
[2024-07-31 12:52:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 12:52:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-07-31 12:52:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:45:20 lr 0.000012	 wd 0.0500	time 11.1592 (11.1592)	loss 1.2215 (1.2215)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 12:53:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:14:52 lr 0.000012	 wd 0.0500	time 0.2804 (0.3717)	loss 1.0582 (1.1901)	grad_norm 0.3417 (0.3363)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 12:53:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:12:05 lr 0.000012	 wd 0.0500	time 0.2406 (0.3150)	loss 1.2790 (1.1789)	grad_norm 0.3332 (0.3362)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 12:54:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:10:52 lr 0.000012	 wd 0.0500	time 0.2565 (0.2962)	loss 1.0279 (1.1716)	grad_norm 0.3279 (nan)	loss_scale 4096.0000 (6967.2824)	mem 12226MB
[2024-07-31 12:54:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:10:03 lr 0.000012	 wd 0.0500	time 0.2678 (0.2870)	loss 0.9196 (1.1597)	grad_norm 0.3336 (nan)	loss_scale 4096.0000 (6251.2519)	mem 12226MB
[2024-07-31 12:54:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:09:25 lr 0.000012	 wd 0.0500	time 0.2391 (0.2825)	loss 1.5558 (1.1622)	grad_norm 0.3498 (nan)	loss_scale 4096.0000 (5821.0619)	mem 12226MB
[2024-07-31 12:55:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:08:50 lr 0.000012	 wd 0.0500	time 0.2527 (0.2789)	loss 1.4537 (1.1629)	grad_norm 0.3155 (nan)	loss_scale 4096.0000 (5534.0300)	mem 12226MB
[2024-07-31 12:55:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:08:18 lr 0.000012	 wd 0.0500	time 0.2537 (0.2765)	loss 1.0759 (1.1655)	grad_norm 0.3371 (nan)	loss_scale 4096.0000 (5328.8902)	mem 12226MB
[2024-07-31 12:56:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:07:47 lr 0.000012	 wd 0.0500	time 0.2723 (0.2747)	loss 1.3417 (1.1669)	grad_norm 0.3425 (nan)	loss_scale 4096.0000 (5174.9713)	mem 12226MB
[2024-07-31 12:56:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:07:17 lr 0.000012	 wd 0.0500	time 0.2846 (0.2733)	loss 1.1714 (1.1679)	grad_norm 0.3318 (nan)	loss_scale 4096.0000 (5055.2186)	mem 12226MB
[2024-07-31 12:57:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:06:48 lr 0.000011	 wd 0.0500	time 0.2483 (0.2722)	loss 1.3603 (1.1678)	grad_norm 0.3276 (nan)	loss_scale 4096.0000 (4959.3926)	mem 12226MB
[2024-07-31 12:57:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:06:20 lr 0.000011	 wd 0.0500	time 0.2495 (0.2712)	loss 0.8476 (1.1719)	grad_norm 0.3475 (nan)	loss_scale 4096.0000 (4880.9737)	mem 12226MB
[2024-07-31 12:58:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:05:51 lr 0.000011	 wd 0.0500	time 0.2391 (0.2702)	loss 0.8739 (1.1697)	grad_norm 0.3334 (nan)	loss_scale 4096.0000 (4815.6137)	mem 12226MB
[2024-07-31 12:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:05:23 lr 0.000011	 wd 0.0500	time 0.2708 (0.2695)	loss 1.3087 (1.1726)	grad_norm 0.3296 (nan)	loss_scale 4096.0000 (4760.3013)	mem 12226MB
[2024-07-31 12:58:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:04:56 lr 0.000011	 wd 0.0500	time 0.2496 (0.2688)	loss 1.2573 (1.1725)	grad_norm 0.3503 (nan)	loss_scale 4096.0000 (4712.8851)	mem 12226MB
[2024-07-31 12:59:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:04:28 lr 0.000011	 wd 0.0500	time 0.2552 (0.2682)	loss 1.3162 (1.1749)	grad_norm 0.3257 (nan)	loss_scale 4096.0000 (4671.7868)	mem 12226MB
[2024-07-31 12:59:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:04:01 lr 0.000011	 wd 0.0500	time 0.3173 (0.2679)	loss 1.1341 (1.1755)	grad_norm 0.3526 (nan)	loss_scale 4096.0000 (4635.8226)	mem 12226MB
[2024-07-31 13:00:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:34 lr 0.000011	 wd 0.0500	time 0.2639 (0.2678)	loss 1.0922 (1.1746)	grad_norm 0.3420 (nan)	loss_scale 4096.0000 (4604.0870)	mem 12226MB
[2024-07-31 13:00:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:07 lr 0.000011	 wd 0.0500	time 0.2877 (0.2675)	loss 0.8661 (1.1734)	grad_norm 0.3149 (nan)	loss_scale 4096.0000 (4575.8756)	mem 12226MB
[2024-07-31 13:01:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:41 lr 0.000011	 wd 0.0500	time 0.2429 (0.2674)	loss 1.0400 (1.1741)	grad_norm 0.3196 (nan)	loss_scale 4096.0000 (4550.6323)	mem 12226MB
[2024-07-31 13:01:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:14 lr 0.000011	 wd 0.0500	time 0.2583 (0.2673)	loss 1.3645 (1.1748)	grad_norm 0.3132 (nan)	loss_scale 4096.0000 (4527.9120)	mem 12226MB
[2024-07-31 13:01:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:47 lr 0.000011	 wd 0.0500	time 0.2369 (0.2671)	loss 1.3680 (1.1756)	grad_norm 0.3188 (nan)	loss_scale 4096.0000 (4507.3546)	mem 12226MB
[2024-07-31 13:02:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:20 lr 0.000011	 wd 0.0500	time 0.2428 (0.2669)	loss 1.0206 (1.1751)	grad_norm 0.3430 (nan)	loss_scale 4096.0000 (4488.6652)	mem 12226MB
[2024-07-31 13:02:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:53 lr 0.000011	 wd 0.0500	time 0.2389 (0.2668)	loss 0.9069 (1.1733)	grad_norm 0.3495 (nan)	loss_scale 4096.0000 (4471.6002)	mem 12226MB
[2024-07-31 13:03:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:27 lr 0.000011	 wd 0.0500	time 0.2822 (0.2666)	loss 1.2408 (1.1724)	grad_norm 0.3276 (nan)	loss_scale 4096.0000 (4455.9567)	mem 12226MB
[2024-07-31 13:03:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2586 (0.2660)	loss 0.8607 (1.1727)	grad_norm 0.3372 (nan)	loss_scale 4096.0000 (4441.5642)	mem 12226MB
[2024-07-31 13:03:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 16 training takes 0:11:07
[2024-07-31 13:03:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.314 (12.314)	Loss 0.5034 (0.5034)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 13:04:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.738 Acc@5 97.736
[2024-07-31 13:04:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 13:04:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-07-31 13:04:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:23:01 lr 0.000011	 wd 0.0500	time 10.6240 (10.6240)	loss 1.4126 (1.4126)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:04:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:14:56 lr 0.000011	 wd 0.0500	time 0.2570 (0.3730)	loss 1.2552 (1.1558)	grad_norm 0.3226 (0.3335)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:12:03 lr 0.000011	 wd 0.0500	time 0.2378 (0.3143)	loss 1.3047 (1.1548)	grad_norm 0.3285 (0.3326)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:05:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:10:49 lr 0.000011	 wd 0.0500	time 0.2867 (0.2950)	loss 1.2542 (1.1705)	grad_norm 0.3299 (0.3337)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:06:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:10:03 lr 0.000011	 wd 0.0500	time 0.2436 (0.2869)	loss 1.4274 (1.1688)	grad_norm 0.3336 (0.3333)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:06:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:09:24 lr 0.000010	 wd 0.0500	time 0.2472 (0.2822)	loss 1.3799 (1.1734)	grad_norm 0.3404 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:08:48 lr 0.000010	 wd 0.0500	time 0.2400 (0.2781)	loss 0.8156 (1.1745)	grad_norm 0.3300 (0.3336)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:07:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:08:16 lr 0.000010	 wd 0.0500	time 0.2471 (0.2754)	loss 1.3065 (1.1722)	grad_norm 0.3393 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:07:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:07:45 lr 0.000010	 wd 0.0500	time 0.2771 (0.2737)	loss 1.2148 (1.1704)	grad_norm 0.3407 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:08:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:07:15 lr 0.000010	 wd 0.0500	time 0.2464 (0.2721)	loss 1.3039 (1.1723)	grad_norm 0.3357 (0.3356)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:06:47 lr 0.000010	 wd 0.0500	time 0.2430 (0.2712)	loss 1.0407 (1.1716)	grad_norm 0.3464 (0.3355)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:09:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:06:22 lr 0.000010	 wd 0.0500	time 0.2452 (0.2726)	loss 1.0441 (1.1762)	grad_norm 0.3249 (0.3355)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:09:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:53 lr 0.000010	 wd 0.0500	time 0.2409 (0.2714)	loss 0.7774 (1.1762)	grad_norm 0.3226 (0.3355)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:10:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:05:25 lr 0.000010	 wd 0.0500	time 0.2621 (0.2706)	loss 1.4449 (1.1734)	grad_norm 0.3457 (0.3353)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:10:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:04:57 lr 0.000010	 wd 0.0500	time 0.2439 (0.2702)	loss 1.3388 (1.1735)	grad_norm 0.3127 (0.3355)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:10:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:30 lr 0.000010	 wd 0.0500	time 0.2400 (0.2698)	loss 1.4615 (1.1740)	grad_norm 0.3290 (0.3355)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:11:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:02 lr 0.000010	 wd 0.0500	time 0.2418 (0.2693)	loss 1.4127 (1.1739)	grad_norm 0.3253 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:11:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:35 lr 0.000010	 wd 0.0500	time 0.2466 (0.2688)	loss 1.5563 (1.1758)	grad_norm 0.3203 (0.3353)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:12:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:08 lr 0.000010	 wd 0.0500	time 0.2453 (0.2685)	loss 0.9804 (1.1751)	grad_norm 0.3512 (0.3354)	loss_scale 8192.0000 (4305.2349)	mem 12226MB
[2024-07-31 13:12:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:41 lr 0.000010	 wd 0.0500	time 0.2848 (0.2681)	loss 1.2683 (1.1742)	grad_norm 0.3296 (0.3351)	loss_scale 8192.0000 (4509.6938)	mem 12226MB
[2024-07-31 13:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:14 lr 0.000010	 wd 0.0500	time 0.2415 (0.2679)	loss 1.2141 (1.1745)	grad_norm 0.3357 (0.3351)	loss_scale 8192.0000 (4693.7171)	mem 12226MB
[2024-07-31 13:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:47 lr 0.000010	 wd 0.0500	time 0.2699 (0.2677)	loss 1.4837 (1.1743)	grad_norm 0.3603 (0.3350)	loss_scale 8192.0000 (4860.2228)	mem 12226MB
[2024-07-31 13:14:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:20 lr 0.000010	 wd 0.0500	time 0.2508 (0.2674)	loss 1.3994 (1.1741)	grad_norm 0.3251 (0.3349)	loss_scale 8192.0000 (5011.5984)	mem 12226MB
[2024-07-31 13:14:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:53 lr 0.000010	 wd 0.0500	time 0.2610 (0.2671)	loss 0.8132 (1.1736)	grad_norm 0.3369 (0.3349)	loss_scale 8192.0000 (5149.8166)	mem 12226MB
[2024-07-31 13:14:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:27 lr 0.000010	 wd 0.0500	time 0.2634 (0.2671)	loss 0.9471 (1.1722)	grad_norm 0.3479 (0.3350)	loss_scale 8192.0000 (5276.5214)	mem 12226MB
[2024-07-31 13:15:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2376 (0.2664)	loss 1.4768 (1.1719)	grad_norm 0.3275 (0.3350)	loss_scale 8192.0000 (5393.0940)	mem 12226MB
[2024-07-31 13:15:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 17 training takes 0:11:08
[2024-07-31 13:15:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.095 (12.095)	Loss 0.4956 (0.4956)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 13:15:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.738 Acc@5 97.742
[2024-07-31 13:15:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 13:15:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-07-31 13:16:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:06:58 lr 0.000009	 wd 0.0500	time 11.6779 (11.6779)	loss 1.4779 (1.4779)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 13:16:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:14:58 lr 0.000009	 wd 0.0500	time 0.2503 (0.3739)	loss 1.5044 (1.2081)	grad_norm 0.3368 (0.3344)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 13:16:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:12:02 lr 0.000009	 wd 0.0500	time 0.2500 (0.3140)	loss 1.0361 (1.1981)	grad_norm 0.3647 (0.3347)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 13:17:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:10:51 lr 0.000009	 wd 0.0500	time 0.2457 (0.2957)	loss 1.5220 (1.1973)	grad_norm 0.3258 (0.3346)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 13:17:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:10:04 lr 0.000009	 wd 0.0500	time 0.2604 (0.2875)	loss 0.7540 (1.1960)	grad_norm 0.3391 (0.3355)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 13:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:09:24 lr 0.000009	 wd 0.0500	time 0.2715 (0.2818)	loss 1.3125 (1.1873)	grad_norm 0.3119 (0.3368)	loss_scale 8192.0000 (8192.0000)	mem 12226MB
[2024-07-31 13:18:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:08:47 lr 0.000009	 wd 0.0500	time 0.2445 (0.2776)	loss 1.5069 (1.1795)	grad_norm 0.3328 (nan)	loss_scale 4096.0000 (7892.1265)	mem 12226MB
[2024-07-31 13:19:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:08:15 lr 0.000009	 wd 0.0500	time 0.2618 (0.2750)	loss 1.4755 (1.1846)	grad_norm 0.3295 (nan)	loss_scale 4096.0000 (7350.5963)	mem 12226MB
[2024-07-31 13:19:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:07:44 lr 0.000009	 wd 0.0500	time 0.2580 (0.2729)	loss 1.3359 (1.1878)	grad_norm 0.3224 (nan)	loss_scale 4096.0000 (6944.2797)	mem 12226MB
[2024-07-31 13:19:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:07:14 lr 0.000009	 wd 0.0500	time 0.2648 (0.2715)	loss 0.9420 (1.1876)	grad_norm 0.3419 (nan)	loss_scale 4096.0000 (6628.1554)	mem 12226MB
[2024-07-31 13:20:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:06:45 lr 0.000009	 wd 0.0500	time 0.2520 (0.2702)	loss 1.4357 (1.1833)	grad_norm 0.3306 (nan)	loss_scale 4096.0000 (6375.1928)	mem 12226MB
[2024-07-31 13:20:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:06:17 lr 0.000009	 wd 0.0500	time 0.2744 (0.2696)	loss 1.0209 (1.1853)	grad_norm 0.3238 (nan)	loss_scale 4096.0000 (6168.1817)	mem 12226MB
[2024-07-31 13:21:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:05:50 lr 0.000009	 wd 0.0500	time 0.2407 (0.2688)	loss 1.0698 (1.1784)	grad_norm 0.3390 (nan)	loss_scale 4096.0000 (5995.6436)	mem 12226MB
[2024-07-31 13:21:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:05:22 lr 0.000009	 wd 0.0500	time 0.2450 (0.2682)	loss 1.5054 (1.1787)	grad_norm 0.3402 (nan)	loss_scale 4096.0000 (5849.6295)	mem 12226MB
[2024-07-31 13:22:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:04:55 lr 0.000009	 wd 0.0500	time 0.2782 (0.2677)	loss 1.1763 (1.1792)	grad_norm 0.3199 (nan)	loss_scale 4096.0000 (5724.4597)	mem 12226MB
[2024-07-31 13:22:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:27 lr 0.000009	 wd 0.0500	time 0.2916 (0.2674)	loss 1.3316 (1.1787)	grad_norm 0.3262 (nan)	loss_scale 4096.0000 (5615.9680)	mem 12226MB
[2024-07-31 13:22:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:04:01 lr 0.000009	 wd 0.0500	time 0.2405 (0.2682)	loss 1.4717 (1.1805)	grad_norm 0.3496 (nan)	loss_scale 4096.0000 (5521.0294)	mem 12226MB
[2024-07-31 13:23:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:34 lr 0.000009	 wd 0.0500	time 0.2689 (0.2678)	loss 1.3456 (1.1824)	grad_norm 0.3412 (nan)	loss_scale 4096.0000 (5437.2534)	mem 12226MB
[2024-07-31 13:23:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:07 lr 0.000009	 wd 0.0500	time 0.2439 (0.2675)	loss 0.7289 (1.1828)	grad_norm 0.3320 (nan)	loss_scale 4096.0000 (5362.7807)	mem 12226MB
[2024-07-31 13:24:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:40 lr 0.000009	 wd 0.0500	time 0.2392 (0.2672)	loss 1.4390 (1.1824)	grad_norm 0.3306 (nan)	loss_scale 4096.0000 (5296.1431)	mem 12226MB
[2024-07-31 13:24:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:14 lr 0.000008	 wd 0.0500	time 0.2525 (0.2671)	loss 1.0881 (1.1821)	grad_norm 0.3237 (nan)	loss_scale 4096.0000 (5236.1659)	mem 12226MB
[2024-07-31 13:25:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.2493 (0.2668)	loss 1.3652 (1.1810)	grad_norm 0.3306 (nan)	loss_scale 4096.0000 (5181.8981)	mem 12226MB
[2024-07-31 13:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:20 lr 0.000008	 wd 0.0500	time 0.2486 (0.2665)	loss 1.0415 (1.1811)	grad_norm 0.3162 (nan)	loss_scale 4096.0000 (5132.5616)	mem 12226MB
[2024-07-31 13:26:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.2401 (0.2664)	loss 1.3346 (1.1817)	grad_norm 0.3247 (nan)	loss_scale 4096.0000 (5087.5133)	mem 12226MB
[2024-07-31 13:26:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:27 lr 0.000008	 wd 0.0500	time 0.2409 (0.2662)	loss 1.4447 (1.1823)	grad_norm 0.3523 (nan)	loss_scale 4096.0000 (5046.2174)	mem 12226MB
[2024-07-31 13:26:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2428 (0.2656)	loss 1.2687 (1.1826)	grad_norm 0.3280 (nan)	loss_scale 4096.0000 (5008.2239)	mem 12226MB
[2024-07-31 13:26:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 18 training takes 0:11:06
[2024-07-31 13:27:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.790 (11.790)	Loss 0.5044 (0.5044)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 13:27:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.764 Acc@5 97.738
[2024-07-31 13:27:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 13:27:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-07-31 13:27:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:27:35 lr 0.000008	 wd 0.0500	time 10.7334 (10.7334)	loss 0.8050 (0.8050)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:14:42 lr 0.000008	 wd 0.0500	time 0.2472 (0.3675)	loss 1.6135 (1.2293)	grad_norm 0.3234 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:28:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:11:57 lr 0.000008	 wd 0.0500	time 0.2643 (0.3116)	loss 1.1867 (1.1989)	grad_norm 0.3420 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:28:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:10:45 lr 0.000008	 wd 0.0500	time 0.2722 (0.2932)	loss 1.4283 (1.1852)	grad_norm 0.3390 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:29:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:09:57 lr 0.000008	 wd 0.0500	time 0.2413 (0.2844)	loss 0.9784 (1.1783)	grad_norm 0.3309 (0.3338)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:29:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:09:19 lr 0.000008	 wd 0.0500	time 0.2806 (0.2795)	loss 1.4885 (1.1826)	grad_norm 0.3385 (0.3340)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:30:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:08:45 lr 0.000008	 wd 0.0500	time 0.2496 (0.2763)	loss 0.8609 (1.1718)	grad_norm 0.3446 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:30:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:08:13 lr 0.000008	 wd 0.0500	time 0.2637 (0.2741)	loss 1.2569 (1.1697)	grad_norm 0.3213 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:31:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:07:43 lr 0.000008	 wd 0.0500	time 0.2768 (0.2726)	loss 1.5331 (1.1714)	grad_norm 0.3697 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:31:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:07:16 lr 0.000008	 wd 0.0500	time 0.2670 (0.2726)	loss 1.2863 (1.1719)	grad_norm 0.3424 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:31:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:06:47 lr 0.000008	 wd 0.0500	time 0.2526 (0.2711)	loss 1.4568 (1.1733)	grad_norm 0.3246 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:32:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:06:18 lr 0.000008	 wd 0.0500	time 0.2664 (0.2703)	loss 1.1923 (1.1724)	grad_norm 0.3334 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:32:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:05:50 lr 0.000008	 wd 0.0500	time 0.2595 (0.2694)	loss 1.3462 (1.1719)	grad_norm 0.3431 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:33:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:05:22 lr 0.000008	 wd 0.0500	time 0.2497 (0.2686)	loss 0.7816 (1.1709)	grad_norm 0.3489 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:33:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:04:55 lr 0.000008	 wd 0.0500	time 0.2408 (0.2681)	loss 0.8326 (1.1700)	grad_norm 0.3372 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:34:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:04:28 lr 0.000008	 wd 0.0500	time 0.2395 (0.2680)	loss 1.3762 (1.1712)	grad_norm 0.3246 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:34:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:01 lr 0.000007	 wd 0.0500	time 0.2518 (0.2674)	loss 1.4442 (1.1730)	grad_norm 0.3308 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:03:34 lr 0.000007	 wd 0.0500	time 0.3080 (0.2671)	loss 0.8917 (1.1736)	grad_norm 0.3474 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:35:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:07 lr 0.000007	 wd 0.0500	time 0.2473 (0.2669)	loss 1.3320 (1.1761)	grad_norm 0.3301 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:35:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:40 lr 0.000007	 wd 0.0500	time 0.2584 (0.2667)	loss 1.1237 (1.1750)	grad_norm 0.3462 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:36:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:13 lr 0.000007	 wd 0.0500	time 0.2476 (0.2667)	loss 1.4963 (1.1761)	grad_norm 0.3411 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:36:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:47 lr 0.000007	 wd 0.0500	time 0.2771 (0.2665)	loss 1.5882 (1.1777)	grad_norm 0.3267 (0.3345)	loss_scale 8192.0000 (4185.6792)	mem 12226MB
[2024-07-31 13:37:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:20 lr 0.000007	 wd 0.0500	time 0.2630 (0.2664)	loss 1.1663 (1.1768)	grad_norm 0.3481 (0.3345)	loss_scale 8192.0000 (4367.7020)	mem 12226MB
[2024-07-31 13:37:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:53 lr 0.000007	 wd 0.0500	time 0.2507 (0.2664)	loss 1.0400 (1.1770)	grad_norm 0.4387 (0.3345)	loss_scale 8192.0000 (4533.9035)	mem 12226MB
[2024-07-31 13:38:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:27 lr 0.000007	 wd 0.0500	time 0.2418 (0.2664)	loss 1.1191 (1.1761)	grad_norm 0.3358 (nan)	loss_scale 4096.0000 (4570.2557)	mem 12226MB
[2024-07-31 13:38:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2377 (0.2657)	loss 0.8244 (1.1763)	grad_norm 0.3327 (nan)	loss_scale 4096.0000 (4551.2931)	mem 12226MB
[2024-07-31 13:38:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 19 training takes 0:11:07
[2024-07-31 13:38:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.506 (11.506)	Loss 0.4873 (0.4873)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 13:38:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.790 Acc@5 97.744
[2024-07-31 13:38:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 13:38:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 13:38:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 13:39:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 13:39:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:52:10 lr 0.000007	 wd 0.0500	time 11.3233 (11.3233)	loss 1.5591 (1.5591)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:39:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:14:54 lr 0.000007	 wd 0.0500	time 0.2463 (0.3725)	loss 1.0838 (1.1865)	grad_norm 0.3475 (0.3366)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:40:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:12:09 lr 0.000007	 wd 0.0500	time 0.2471 (0.3167)	loss 1.3264 (1.1696)	grad_norm 0.3503 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:40:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:11:03 lr 0.000007	 wd 0.0500	time 0.2730 (0.3015)	loss 0.7145 (1.1817)	grad_norm 0.3452 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:40:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:10:11 lr 0.000007	 wd 0.0500	time 0.2690 (0.2910)	loss 1.3332 (1.1814)	grad_norm 0.3336 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:41:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:09:31 lr 0.000007	 wd 0.0500	time 0.2598 (0.2853)	loss 1.3018 (1.1794)	grad_norm 0.3305 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:41:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:08:56 lr 0.000007	 wd 0.0500	time 0.2389 (0.2818)	loss 1.2963 (1.1845)	grad_norm 0.3586 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:42:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:08:23 lr 0.000007	 wd 0.0500	time 0.2542 (0.2796)	loss 1.2890 (1.1825)	grad_norm 0.3345 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:42:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:07:52 lr 0.000007	 wd 0.0500	time 0.2526 (0.2776)	loss 1.1919 (1.1831)	grad_norm 0.3238 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:43:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:07:21 lr 0.000007	 wd 0.0500	time 0.2885 (0.2757)	loss 1.2709 (1.1825)	grad_norm 0.3441 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:43:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:06:52 lr 0.000007	 wd 0.0500	time 0.2747 (0.2744)	loss 1.4110 (1.1845)	grad_norm 0.3195 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:44:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:06:23 lr 0.000007	 wd 0.0500	time 0.2432 (0.2733)	loss 1.0911 (1.1824)	grad_norm 0.3449 (0.3363)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:44:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:05:54 lr 0.000006	 wd 0.0500	time 0.2595 (0.2723)	loss 1.2365 (1.1784)	grad_norm 0.3271 (0.3361)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:44:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:05:26 lr 0.000006	 wd 0.0500	time 0.2455 (0.2717)	loss 1.3626 (1.1779)	grad_norm 0.3333 (0.3370)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:45:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:04:58 lr 0.000006	 wd 0.0500	time 0.2454 (0.2711)	loss 1.0619 (1.1793)	grad_norm 0.3484 (0.3368)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:45:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:31 lr 0.000006	 wd 0.0500	time 0.2466 (0.2709)	loss 1.3544 (1.1821)	grad_norm 0.3298 (0.3367)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:46:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:04:03 lr 0.000006	 wd 0.0500	time 0.2583 (0.2704)	loss 1.3081 (1.1843)	grad_norm 0.3328 (0.3366)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:46:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:36 lr 0.000006	 wd 0.0500	time 0.2431 (0.2698)	loss 1.6343 (1.1855)	grad_norm 0.3212 (0.3365)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:47:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:09 lr 0.000006	 wd 0.0500	time 0.2796 (0.2697)	loss 0.7741 (1.1831)	grad_norm 0.3335 (0.3364)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:47:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:42 lr 0.000006	 wd 0.0500	time 0.2464 (0.2693)	loss 0.8466 (1.1844)	grad_norm 0.3343 (0.3363)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:47:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:15 lr 0.000006	 wd 0.0500	time 0.2510 (0.2691)	loss 1.1125 (1.1835)	grad_norm 0.3594 (0.3385)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:48:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:48 lr 0.000006	 wd 0.0500	time 0.2433 (0.2689)	loss 1.2648 (1.1835)	grad_norm 0.3242 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:21 lr 0.000006	 wd 0.0500	time 0.2505 (0.2685)	loss 1.4081 (1.1847)	grad_norm 0.3327 (0.3385)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:49:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:54 lr 0.000006	 wd 0.0500	time 0.2874 (0.2681)	loss 0.8674 (1.1859)	grad_norm 0.3403 (0.3383)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:49:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:27 lr 0.000006	 wd 0.0500	time 0.2504 (0.2678)	loss 0.7964 (1.1852)	grad_norm 0.3469 (0.3381)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:50:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2372 (0.2669)	loss 1.0648 (1.1871)	grad_norm 0.3256 (0.3379)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:50:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 20 training takes 0:11:10
[2024-07-31 13:50:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_20.pth saving......
[2024-07-31 13:50:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_20.pth saved !!!
[2024-07-31 13:50:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.408 (11.408)	Loss 0.4973 (0.4973)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 13:50:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.794 Acc@5 97.740
[2024-07-31 13:50:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 13:50:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 13:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 13:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 13:50:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:25:09 lr 0.000006	 wd 0.0500	time 10.6753 (10.6753)	loss 0.9868 (0.9868)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:51:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:14:23 lr 0.000006	 wd 0.0500	time 0.2706 (0.3595)	loss 1.3879 (1.2085)	grad_norm 0.3509 (0.3333)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:51:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:11:48 lr 0.000006	 wd 0.0500	time 0.2671 (0.3078)	loss 1.3485 (1.2030)	grad_norm 0.3385 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:52:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:10:40 lr 0.000006	 wd 0.0500	time 0.2452 (0.2909)	loss 1.1935 (1.1918)	grad_norm 0.3511 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:52:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:09:55 lr 0.000006	 wd 0.0500	time 0.2468 (0.2833)	loss 1.3572 (1.1835)	grad_norm 0.3324 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:52:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:09:18 lr 0.000006	 wd 0.0500	time 0.2424 (0.2790)	loss 0.9143 (1.1784)	grad_norm 0.3351 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:53:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:08:45 lr 0.000006	 wd 0.0500	time 0.2592 (0.2765)	loss 1.3152 (1.1736)	grad_norm 0.3293 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:53:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:08:14 lr 0.000006	 wd 0.0500	time 0.2975 (0.2746)	loss 1.2482 (1.1711)	grad_norm 0.3303 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:54:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:07:45 lr 0.000006	 wd 0.0500	time 0.2458 (0.2733)	loss 1.2323 (1.1695)	grad_norm 0.3356 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:54:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:07:15 lr 0.000005	 wd 0.0500	time 0.2406 (0.2720)	loss 1.4559 (1.1728)	grad_norm 0.3408 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:55:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:06:47 lr 0.000005	 wd 0.0500	time 0.2777 (0.2713)	loss 1.5201 (1.1764)	grad_norm 0.3345 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:55:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:06:19 lr 0.000005	 wd 0.0500	time 0.2475 (0.2708)	loss 0.7991 (1.1797)	grad_norm 0.3316 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:56:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:05:51 lr 0.000005	 wd 0.0500	time 0.2532 (0.2702)	loss 1.4130 (1.1766)	grad_norm 0.3390 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:56:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:05:24 lr 0.000005	 wd 0.0500	time 0.2517 (0.2698)	loss 1.2187 (1.1770)	grad_norm 0.3332 (0.3341)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 13:56:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:04:56 lr 0.000005	 wd 0.0500	time 0.2505 (0.2689)	loss 0.8330 (1.1742)	grad_norm 0.3499 (0.3342)	loss_scale 8192.0000 (4306.5011)	mem 12226MB
[2024-07-31 13:57:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:04:28 lr 0.000005	 wd 0.0500	time 0.2582 (0.2683)	loss 0.8527 (1.1758)	grad_norm 0.3390 (0.3344)	loss_scale 8192.0000 (4565.3618)	mem 12226MB
[2024-07-31 13:57:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:04:01 lr 0.000005	 wd 0.0500	time 0.2578 (0.2678)	loss 1.0608 (1.1724)	grad_norm 0.3589 (0.3344)	loss_scale 8192.0000 (4791.8851)	mem 12226MB
[2024-07-31 13:58:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:34 lr 0.000005	 wd 0.0500	time 0.2635 (0.2679)	loss 1.3863 (1.1731)	grad_norm 0.3193 (0.3344)	loss_scale 8192.0000 (4991.7743)	mem 12226MB
[2024-07-31 13:58:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:08 lr 0.000005	 wd 0.0500	time 0.2680 (0.2679)	loss 0.7828 (1.1736)	grad_norm 0.3550 (0.3345)	loss_scale 8192.0000 (5169.4659)	mem 12226MB
[2024-07-31 13:59:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:41 lr 0.000005	 wd 0.0500	time 0.2518 (0.2678)	loss 0.8613 (1.1743)	grad_norm 0.3322 (0.3345)	loss_scale 8192.0000 (5328.4629)	mem 12226MB
[2024-07-31 13:59:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:14 lr 0.000005	 wd 0.0500	time 0.2498 (0.2675)	loss 1.0750 (1.1732)	grad_norm 0.3532 (nan)	loss_scale 4096.0000 (5303.7161)	mem 12226MB
[2024-07-31 14:00:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:47 lr 0.000005	 wd 0.0500	time 0.2601 (0.2672)	loss 1.1606 (1.1729)	grad_norm 0.3326 (nan)	loss_scale 4096.0000 (5246.2332)	mem 12226MB
[2024-07-31 14:00:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:20 lr 0.000005	 wd 0.0500	time 0.2715 (0.2670)	loss 1.4417 (1.1715)	grad_norm 0.3397 (nan)	loss_scale 4096.0000 (5193.9736)	mem 12226MB
[2024-07-31 14:00:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:53 lr 0.000005	 wd 0.0500	time 0.2611 (0.2669)	loss 1.5585 (1.1731)	grad_norm 0.3552 (nan)	loss_scale 4096.0000 (5146.2564)	mem 12226MB
[2024-07-31 14:01:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:27 lr 0.000005	 wd 0.0500	time 0.2441 (0.2667)	loss 1.2372 (1.1749)	grad_norm 0.3432 (nan)	loss_scale 2048.0000 (5083.7484)	mem 12226MB
[2024-07-31 14:01:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2378 (0.2660)	loss 1.5040 (1.1753)	grad_norm 0.3384 (nan)	loss_scale 2048.0000 (4962.3671)	mem 12226MB
[2024-07-31 14:01:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 21 training takes 0:11:08
[2024-07-31 14:01:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.927 (11.927)	Loss 0.5171 (0.5171)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 14:02:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.768 Acc@5 97.734
[2024-07-31 14:02:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 14:02:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 14:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:14:55 lr 0.000005	 wd 0.0500	time 11.8688 (11.8688)	loss 1.0006 (1.0006)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:14:54 lr 0.000005	 wd 0.0500	time 0.2378 (0.3725)	loss 1.4781 (1.1912)	grad_norm 0.3590 (0.3365)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:03:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:12:28 lr 0.000005	 wd 0.0500	time 0.3125 (0.3251)	loss 0.7764 (1.1842)	grad_norm 0.3227 (0.3448)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:03:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:11:07 lr 0.000005	 wd 0.0500	time 0.2967 (0.3029)	loss 1.3952 (1.1898)	grad_norm 0.3533 (0.3436)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:04:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:10:15 lr 0.000005	 wd 0.0500	time 0.2825 (0.2927)	loss 0.7719 (1.1959)	grad_norm 0.3281 (0.3419)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:04:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:09:34 lr 0.000005	 wd 0.0500	time 0.2659 (0.2868)	loss 1.2124 (1.1932)	grad_norm 0.3249 (0.3402)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:05:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:08:56 lr 0.000005	 wd 0.0500	time 0.2815 (0.2823)	loss 0.8981 (1.1946)	grad_norm 0.3259 (0.3394)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:05:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:08:28 lr 0.000005	 wd 0.0500	time 0.2623 (0.2823)	loss 1.3746 (1.1892)	grad_norm 0.3542 (0.3389)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:06:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:07:56 lr 0.000004	 wd 0.0500	time 0.2643 (0.2798)	loss 0.9486 (1.1879)	grad_norm 0.3243 (0.3386)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:06:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:07:25 lr 0.000004	 wd 0.0500	time 0.2656 (0.2779)	loss 1.1169 (1.1868)	grad_norm 0.3477 (0.3387)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:06:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:06:54 lr 0.000004	 wd 0.0500	time 0.2554 (0.2761)	loss 1.3050 (1.1870)	grad_norm 0.3278 (0.3382)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:07:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:06:24 lr 0.000004	 wd 0.0500	time 0.2485 (0.2743)	loss 0.8345 (1.1842)	grad_norm 0.3315 (0.3378)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:07:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:05:55 lr 0.000004	 wd 0.0500	time 0.2489 (0.2730)	loss 1.3915 (1.1852)	grad_norm 0.3291 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:08:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:05:27 lr 0.000004	 wd 0.0500	time 0.2627 (0.2721)	loss 0.8564 (1.1789)	grad_norm 0.3150 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:08:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:04:58 lr 0.000004	 wd 0.0500	time 0.2578 (0.2713)	loss 1.0071 (1.1733)	grad_norm 0.3387 (0.3363)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:09:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:30 lr 0.000004	 wd 0.0500	time 0.2663 (0.2704)	loss 1.2272 (1.1729)	grad_norm 0.3124 (0.3363)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:09:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:04:03 lr 0.000004	 wd 0.0500	time 0.2575 (0.2699)	loss 1.5315 (1.1730)	grad_norm 0.3286 (0.3370)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:09:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:36 lr 0.000004	 wd 0.0500	time 0.2589 (0.2697)	loss 0.9047 (1.1712)	grad_norm 0.3402 (0.3368)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:10:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:09 lr 0.000004	 wd 0.0500	time 0.2656 (0.2693)	loss 1.4424 (1.1698)	grad_norm 0.3142 (0.3368)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:10:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:42 lr 0.000004	 wd 0.0500	time 0.2507 (0.2693)	loss 0.9616 (1.1706)	grad_norm 0.3216 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:15 lr 0.000004	 wd 0.0500	time 0.2953 (0.2691)	loss 0.9207 (1.1716)	grad_norm 0.3301 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:11:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:48 lr 0.000004	 wd 0.0500	time 0.2392 (0.2688)	loss 1.3227 (1.1731)	grad_norm 0.3260 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:12:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:21 lr 0.000004	 wd 0.0500	time 0.2594 (0.2686)	loss 1.1235 (1.1729)	grad_norm 0.3450 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:54 lr 0.000004	 wd 0.0500	time 0.2390 (0.2683)	loss 1.1548 (1.1730)	grad_norm 0.3210 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:12:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:27 lr 0.000004	 wd 0.0500	time 0.2763 (0.2680)	loss 1.3558 (1.1741)	grad_norm 0.3496 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:13:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2377 (0.2673)	loss 1.1291 (1.1737)	grad_norm 0.3230 (0.3364)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:13:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 22 training takes 0:11:11
[2024-07-31 14:13:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.765 (11.765)	Loss 0.5000 (0.5000)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 14:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.754 Acc@5 97.736
[2024-07-31 14:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 14:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 14:14:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:28:44 lr 0.000004	 wd 0.0500	time 10.7614 (10.7614)	loss 0.7885 (0.7885)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:14:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:14:46 lr 0.000004	 wd 0.0500	time 0.2401 (0.3689)	loss 1.4935 (1.1643)	grad_norm 0.3417 (0.3386)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:15:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:12:04 lr 0.000004	 wd 0.0500	time 0.2584 (0.3148)	loss 0.8616 (1.1690)	grad_norm 0.3375 (0.3357)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:15:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:10:51 lr 0.000004	 wd 0.0500	time 0.2442 (0.2959)	loss 1.4370 (1.1593)	grad_norm 0.3425 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:15:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:10:02 lr 0.000004	 wd 0.0500	time 0.2480 (0.2865)	loss 0.9021 (1.1647)	grad_norm 0.3401 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:16:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:09:22 lr 0.000004	 wd 0.0500	time 0.2468 (0.2811)	loss 0.8312 (1.1679)	grad_norm 0.3157 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:16:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:08:47 lr 0.000004	 wd 0.0500	time 0.2561 (0.2772)	loss 1.4538 (1.1673)	grad_norm 0.3479 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:17:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:08:15 lr 0.000004	 wd 0.0500	time 0.2424 (0.2751)	loss 1.1455 (1.1705)	grad_norm 0.3478 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:17:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:07:45 lr 0.000003	 wd 0.0500	time 0.2471 (0.2733)	loss 1.4587 (1.1723)	grad_norm 0.3236 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:18:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:07:15 lr 0.000003	 wd 0.0500	time 0.2661 (0.2720)	loss 1.1335 (1.1729)	grad_norm 0.3292 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:18:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:06:47 lr 0.000003	 wd 0.0500	time 0.2624 (0.2713)	loss 1.1528 (1.1683)	grad_norm 0.2984 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:18:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:06:19 lr 0.000003	 wd 0.0500	time 0.2398 (0.2704)	loss 1.0490 (1.1713)	grad_norm 0.3444 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:19:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:05:51 lr 0.000003	 wd 0.0500	time 0.2778 (0.2698)	loss 1.4319 (1.1743)	grad_norm 0.3334 (0.3359)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:19:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:05:23 lr 0.000003	 wd 0.0500	time 0.2471 (0.2691)	loss 1.2945 (1.1750)	grad_norm 0.3213 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:20:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:04:56 lr 0.000003	 wd 0.0500	time 0.2631 (0.2688)	loss 1.2846 (1.1751)	grad_norm 0.3485 (0.3359)	loss_scale 4096.0000 (2086.0071)	mem 12226MB
[2024-07-31 14:20:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:04:28 lr 0.000003	 wd 0.0500	time 0.2589 (0.2682)	loss 1.3097 (1.1736)	grad_norm 0.3347 (0.3360)	loss_scale 4096.0000 (2219.9174)	mem 12226MB
[2024-07-31 14:21:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:04:01 lr 0.000003	 wd 0.0500	time 0.2490 (0.2679)	loss 1.1526 (1.1770)	grad_norm 0.3189 (0.3364)	loss_scale 4096.0000 (2337.0993)	mem 12226MB
[2024-07-31 14:21:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:03:34 lr 0.000003	 wd 0.0500	time 0.2624 (0.2677)	loss 0.7294 (1.1743)	grad_norm 0.3394 (0.3364)	loss_scale 4096.0000 (2440.5032)	mem 12226MB
[2024-07-31 14:21:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:07 lr 0.000003	 wd 0.0500	time 0.2774 (0.2674)	loss 1.3238 (1.1734)	grad_norm 0.3548 (0.3363)	loss_scale 4096.0000 (2532.4242)	mem 12226MB
[2024-07-31 14:22:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:40 lr 0.000003	 wd 0.0500	time 0.2400 (0.2672)	loss 1.3165 (1.1728)	grad_norm 0.3264 (0.3361)	loss_scale 4096.0000 (2614.6744)	mem 12226MB
[2024-07-31 14:22:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:13 lr 0.000003	 wd 0.0500	time 0.2850 (0.2669)	loss 0.9421 (1.1725)	grad_norm 0.3627 (0.3362)	loss_scale 4096.0000 (2688.7036)	mem 12226MB
[2024-07-31 14:23:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:47 lr 0.000003	 wd 0.0500	time 0.2772 (0.2670)	loss 1.4037 (1.1730)	grad_norm 0.3314 (0.3360)	loss_scale 4096.0000 (2755.6859)	mem 12226MB
[2024-07-31 14:23:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:20 lr 0.000003	 wd 0.0500	time 0.2420 (0.2667)	loss 1.3418 (1.1715)	grad_norm 0.3499 (0.3362)	loss_scale 4096.0000 (2816.5816)	mem 12226MB
[2024-07-31 14:24:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:53 lr 0.000003	 wd 0.0500	time 0.2387 (0.2666)	loss 1.3784 (1.1716)	grad_norm 0.3209 (0.3361)	loss_scale 4096.0000 (2872.1843)	mem 12226MB
[2024-07-31 14:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:27 lr 0.000003	 wd 0.0500	time 0.2450 (0.2665)	loss 0.8153 (1.1714)	grad_norm 0.3303 (0.3361)	loss_scale 4096.0000 (2923.1554)	mem 12226MB
[2024-07-31 14:25:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2375 (0.2659)	loss 0.8897 (1.1707)	grad_norm 0.3266 (0.3361)	loss_scale 4096.0000 (2970.0504)	mem 12226MB
[2024-07-31 14:25:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 23 training takes 0:11:08
[2024-07-31 14:25:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.191 (12.191)	Loss 0.5181 (0.5181)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 14:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.750 Acc@5 97.730
[2024-07-31 14:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 14:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 14:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:07:00 lr 0.000003	 wd 0.0500	time 11.6788 (11.6788)	loss 1.2435 (1.2435)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:26:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:14:46 lr 0.000003	 wd 0.0500	time 0.2407 (0.3691)	loss 0.9449 (1.1782)	grad_norm 0.3194 (0.3415)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:26:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:12:04 lr 0.000003	 wd 0.0500	time 0.2881 (0.3149)	loss 1.2367 (1.1667)	grad_norm 0.3168 (0.3396)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:27:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:10:50 lr 0.000003	 wd 0.0500	time 0.2439 (0.2953)	loss 1.3870 (1.1812)	grad_norm 0.3301 (0.3378)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:27:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:10:01 lr 0.000003	 wd 0.0500	time 0.2541 (0.2861)	loss 0.7319 (1.1809)	grad_norm 0.3403 (0.3370)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:27:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:09:21 lr 0.000003	 wd 0.0500	time 0.2611 (0.2803)	loss 1.4570 (1.1810)	grad_norm 0.3534 (0.3380)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:28:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:08:45 lr 0.000003	 wd 0.0500	time 0.2407 (0.2764)	loss 0.6901 (1.1761)	grad_norm 0.3275 (0.3372)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:28:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:08:13 lr 0.000003	 wd 0.0500	time 0.2390 (0.2739)	loss 1.2085 (1.1744)	grad_norm 0.3254 (nan)	loss_scale 2048.0000 (4049.2553)	mem 12226MB
[2024-07-31 14:29:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:07:42 lr 0.000003	 wd 0.0500	time 0.2577 (0.2718)	loss 1.2432 (1.1765)	grad_norm 0.3410 (nan)	loss_scale 2048.0000 (3799.4107)	mem 12226MB
[2024-07-31 14:29:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:07:15 lr 0.000003	 wd 0.0500	time 0.2839 (0.2716)	loss 0.9553 (1.1804)	grad_norm 0.3247 (nan)	loss_scale 2048.0000 (3605.0255)	mem 12226MB
[2024-07-31 14:30:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:06:46 lr 0.000003	 wd 0.0500	time 0.2949 (0.2704)	loss 0.8027 (1.1776)	grad_norm 0.3368 (nan)	loss_scale 2048.0000 (3449.4785)	mem 12226MB
[2024-07-31 14:30:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:06:17 lr 0.000003	 wd 0.0500	time 0.2708 (0.2695)	loss 0.8561 (1.1768)	grad_norm 0.3465 (nan)	loss_scale 2048.0000 (3322.1871)	mem 12226MB
[2024-07-31 14:30:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:05:50 lr 0.000002	 wd 0.0500	time 0.2497 (0.2688)	loss 0.9091 (1.1748)	grad_norm 0.3376 (nan)	loss_scale 2048.0000 (3216.0933)	mem 12226MB
[2024-07-31 14:31:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:05:22 lr 0.000002	 wd 0.0500	time 0.2584 (0.2683)	loss 0.7680 (1.1718)	grad_norm 0.3524 (nan)	loss_scale 2048.0000 (3126.3090)	mem 12226MB
[2024-07-31 14:31:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:04:55 lr 0.000002	 wd 0.0500	time 0.2574 (0.2678)	loss 1.4812 (1.1729)	grad_norm 0.3286 (nan)	loss_scale 2048.0000 (3049.3419)	mem 12226MB
[2024-07-31 14:32:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:04:28 lr 0.000002	 wd 0.0500	time 0.2776 (0.2676)	loss 1.2511 (1.1691)	grad_norm 0.3413 (nan)	loss_scale 2048.0000 (2982.6302)	mem 12226MB
[2024-07-31 14:32:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:04:01 lr 0.000002	 wd 0.0500	time 0.2492 (0.2672)	loss 1.3089 (1.1680)	grad_norm 0.3472 (nan)	loss_scale 2048.0000 (2924.2523)	mem 12226MB
[2024-07-31 14:33:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:03:34 lr 0.000002	 wd 0.0500	time 0.2774 (0.2669)	loss 1.5262 (1.1695)	grad_norm 0.3442 (nan)	loss_scale 2048.0000 (2872.7384)	mem 12226MB
[2024-07-31 14:33:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:07 lr 0.000002	 wd 0.0500	time 0.2401 (0.2667)	loss 1.1067 (1.1695)	grad_norm 0.3373 (nan)	loss_scale 2048.0000 (2826.9450)	mem 12226MB
[2024-07-31 14:34:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:40 lr 0.000002	 wd 0.0500	time 0.2438 (0.2667)	loss 0.9365 (1.1706)	grad_norm 0.3224 (nan)	loss_scale 2048.0000 (2785.9695)	mem 12226MB
[2024-07-31 14:34:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:13 lr 0.000002	 wd 0.0500	time 0.2415 (0.2666)	loss 1.2955 (1.1706)	grad_norm 0.3434 (nan)	loss_scale 2048.0000 (2749.0895)	mem 12226MB
[2024-07-31 14:34:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:47 lr 0.000002	 wd 0.0500	time 0.2712 (0.2665)	loss 1.0287 (1.1708)	grad_norm 0.3372 (nan)	loss_scale 2048.0000 (2715.7201)	mem 12226MB
[2024-07-31 14:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:20 lr 0.000002	 wd 0.0500	time 0.2973 (0.2665)	loss 1.0292 (1.1725)	grad_norm 0.3469 (nan)	loss_scale 2048.0000 (2685.3830)	mem 12226MB
[2024-07-31 14:35:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:53 lr 0.000002	 wd 0.0500	time 0.2496 (0.2664)	loss 1.3933 (1.1749)	grad_norm 0.3612 (nan)	loss_scale 2048.0000 (2657.6827)	mem 12226MB
[2024-07-31 14:36:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:27 lr 0.000002	 wd 0.0500	time 0.2896 (0.2661)	loss 1.2200 (1.1728)	grad_norm 0.3221 (nan)	loss_scale 2048.0000 (2632.2899)	mem 12226MB
[2024-07-31 14:36:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2374 (0.2656)	loss 0.9593 (1.1733)	grad_norm 0.3188 (nan)	loss_scale 2048.0000 (2608.9276)	mem 12226MB
[2024-07-31 14:36:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 24 training takes 0:11:08
[2024-07-31 14:36:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.846 (11.846)	Loss 0.5005 (0.5005)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 14:37:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.734 Acc@5 97.748
[2024-07-31 14:37:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 14:37:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 14:37:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:34:59 lr 0.000002	 wd 0.0500	time 10.9110 (10.9110)	loss 1.3008 (1.3008)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:37:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:14:44 lr 0.000002	 wd 0.0500	time 0.2539 (0.3682)	loss 1.2183 (1.2335)	grad_norm 0.3329 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:38:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:12:01 lr 0.000002	 wd 0.0500	time 0.2448 (0.3134)	loss 1.4456 (1.2017)	grad_norm 0.3339 (0.3341)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:38:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:10:53 lr 0.000002	 wd 0.0500	time 0.2780 (0.2966)	loss 1.1372 (1.1848)	grad_norm 0.3384 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:39:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:10:04 lr 0.000002	 wd 0.0500	time 0.2573 (0.2876)	loss 1.3411 (1.1857)	grad_norm 0.3193 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:39:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:09:25 lr 0.000002	 wd 0.0500	time 0.2585 (0.2824)	loss 1.4419 (1.1838)	grad_norm 0.3429 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:08:50 lr 0.000002	 wd 0.0500	time 0.2403 (0.2788)	loss 0.8357 (1.1737)	grad_norm 0.3451 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:40:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:08:17 lr 0.000002	 wd 0.0500	time 0.2406 (0.2759)	loss 0.8004 (1.1756)	grad_norm 0.3431 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:40:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:07:46 lr 0.000002	 wd 0.0500	time 0.2443 (0.2743)	loss 1.3625 (1.1745)	grad_norm 0.3533 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:41:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:07:16 lr 0.000002	 wd 0.0500	time 0.3008 (0.2728)	loss 1.4809 (1.1744)	grad_norm 0.3269 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:06:48 lr 0.000002	 wd 0.0500	time 0.2801 (0.2717)	loss 1.5242 (1.1757)	grad_norm 0.3352 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:42:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:06:19 lr 0.000002	 wd 0.0500	time 0.2470 (0.2708)	loss 1.5625 (1.1765)	grad_norm 0.3481 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:42:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:05:51 lr 0.000002	 wd 0.0500	time 0.2827 (0.2703)	loss 0.9669 (1.1729)	grad_norm 0.3236 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:43:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:05:24 lr 0.000002	 wd 0.0500	time 0.2502 (0.2698)	loss 0.8522 (1.1719)	grad_norm 0.3498 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:04:56 lr 0.000002	 wd 0.0500	time 0.2390 (0.2691)	loss 0.8363 (1.1714)	grad_norm 0.3445 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:44:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:04:30 lr 0.000002	 wd 0.0500	time 0.2432 (0.2700)	loss 0.8733 (1.1717)	grad_norm 0.3489 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:44:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:04:03 lr 0.000002	 wd 0.0500	time 0.2552 (0.2696)	loss 0.8143 (1.1707)	grad_norm 0.3365 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:44:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:03:36 lr 0.000002	 wd 0.0500	time 0.2760 (0.2695)	loss 0.7449 (1.1683)	grad_norm 0.3420 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:45:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:09 lr 0.000002	 wd 0.0500	time 0.2679 (0.2693)	loss 0.8894 (1.1703)	grad_norm 0.3368 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:45:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:42 lr 0.000002	 wd 0.0500	time 0.2641 (0.2693)	loss 1.6611 (1.1727)	grad_norm 0.3278 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:46:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:15 lr 0.000002	 wd 0.0500	time 0.2585 (0.2691)	loss 0.9997 (1.1731)	grad_norm 0.3232 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:46:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:48 lr 0.000002	 wd 0.0500	time 0.2792 (0.2687)	loss 1.3147 (1.1744)	grad_norm 0.3262 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 14:47:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:21 lr 0.000001	 wd 0.0500	time 0.2525 (0.2686)	loss 1.1202 (1.1747)	grad_norm 0.3326 (0.3352)	loss_scale 4096.0000 (2064.7488)	mem 12226MB
[2024-07-31 14:47:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:54 lr 0.000001	 wd 0.0500	time 0.2854 (0.2686)	loss 1.3518 (1.1741)	grad_norm 0.3523 (0.3352)	loss_scale 4096.0000 (2153.0256)	mem 12226MB
[2024-07-31 14:48:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:27 lr 0.000001	 wd 0.0500	time 0.2659 (0.2686)	loss 1.4004 (1.1733)	grad_norm 0.3063 (0.3352)	loss_scale 4096.0000 (2233.9492)	mem 12226MB
[2024-07-31 14:48:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2468 (0.2679)	loss 0.7551 (1.1721)	grad_norm 0.3684 (0.3352)	loss_scale 4096.0000 (2308.4014)	mem 12226MB
[2024-07-31 14:48:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 25 training takes 0:11:15
[2024-07-31 14:48:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.008 (11.008)	Loss 0.5225 (0.5225)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 14:49:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.756 Acc@5 97.736
[2024-07-31 14:49:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 14:49:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 14:49:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:58:31 lr 0.000001	 wd 0.0500	time 11.4753 (11.4753)	loss 1.1965 (1.1965)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:49:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:14:54 lr 0.000001	 wd 0.0500	time 0.2406 (0.3725)	loss 1.3315 (1.1854)	grad_norm 0.3233 (0.3380)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:50:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:12:08 lr 0.000001	 wd 0.0500	time 0.2488 (0.3164)	loss 1.1102 (1.1843)	grad_norm 0.3454 (0.3359)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:50:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:10:54 lr 0.000001	 wd 0.0500	time 0.2419 (0.2972)	loss 0.9181 (1.1660)	grad_norm 0.3249 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:50:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:10:04 lr 0.000001	 wd 0.0500	time 0.2803 (0.2876)	loss 1.3603 (1.1643)	grad_norm 0.3489 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:51:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:09:24 lr 0.000001	 wd 0.0500	time 0.2472 (0.2822)	loss 1.3155 (1.1732)	grad_norm 0.3262 (0.3353)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:51:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:08:49 lr 0.000001	 wd 0.0500	time 0.2597 (0.2782)	loss 0.8040 (1.1774)	grad_norm 0.3389 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:52:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:08:21 lr 0.000001	 wd 0.0500	time 0.2756 (0.2785)	loss 1.4627 (1.1734)	grad_norm 0.3154 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:52:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:07:50 lr 0.000001	 wd 0.0500	time 0.2610 (0.2767)	loss 1.2723 (1.1775)	grad_norm 0.3465 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:53:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:07:20 lr 0.000001	 wd 0.0500	time 0.2588 (0.2749)	loss 1.3000 (1.1767)	grad_norm 0.3246 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:53:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:06:50 lr 0.000001	 wd 0.0500	time 0.2654 (0.2733)	loss 1.1401 (1.1758)	grad_norm 0.3348 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:54:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:06:21 lr 0.000001	 wd 0.0500	time 0.2701 (0.2723)	loss 0.9580 (1.1757)	grad_norm 0.3256 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:54:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:05:53 lr 0.000001	 wd 0.0500	time 0.2428 (0.2714)	loss 1.1020 (1.1774)	grad_norm 0.3314 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:54:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:05:25 lr 0.000001	 wd 0.0500	time 0.2529 (0.2708)	loss 1.3121 (1.1790)	grad_norm 0.3461 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:55:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:04:57 lr 0.000001	 wd 0.0500	time 0.2511 (0.2703)	loss 1.3973 (1.1799)	grad_norm 0.3313 (0.3353)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:55:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:04:30 lr 0.000001	 wd 0.0500	time 0.2417 (0.2698)	loss 1.4236 (1.1816)	grad_norm 0.3246 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:04:03 lr 0.000001	 wd 0.0500	time 0.2561 (0.2696)	loss 1.2179 (1.1790)	grad_norm 0.3282 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:56:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:03:35 lr 0.000001	 wd 0.0500	time 0.3260 (0.2693)	loss 1.3057 (1.1770)	grad_norm 0.3306 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:57:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:08 lr 0.000001	 wd 0.0500	time 0.2689 (0.2691)	loss 0.8764 (1.1769)	grad_norm 0.3481 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:57:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:41 lr 0.000001	 wd 0.0500	time 0.2726 (0.2687)	loss 1.3011 (1.1780)	grad_norm 0.3393 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:58:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:14 lr 0.000001	 wd 0.0500	time 0.2453 (0.2685)	loss 1.1962 (1.1764)	grad_norm 0.3423 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:47 lr 0.000001	 wd 0.0500	time 0.2563 (0.2681)	loss 1.3723 (1.1754)	grad_norm 0.3430 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:58:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:20 lr 0.000001	 wd 0.0500	time 0.2880 (0.2681)	loss 1.3006 (1.1737)	grad_norm 0.3352 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:59:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:54 lr 0.000001	 wd 0.0500	time 0.2410 (0.2679)	loss 1.3070 (1.1739)	grad_norm 0.3167 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 14:59:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:27 lr 0.000001	 wd 0.0500	time 0.2429 (0.2676)	loss 0.7286 (1.1743)	grad_norm 0.3239 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:00:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2378 (0.2669)	loss 0.9352 (1.1748)	grad_norm 0.5148 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:00:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 26 training takes 0:11:12
[2024-07-31 15:00:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.761 (11.761)	Loss 0.5063 (0.5063)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 15:00:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.772 Acc@5 97.736
[2024-07-31 15:00:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 15:00:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 15:01:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:05:47 lr 0.000001	 wd 0.0500	time 11.6498 (11.6498)	loss 1.4230 (1.4230)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:01:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:14:56 lr 0.000001	 wd 0.0500	time 0.2458 (0.3731)	loss 1.3032 (1.1658)	grad_norm 0.3066 (0.3331)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:01:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:12:13 lr 0.000001	 wd 0.0500	time 0.2554 (0.3188)	loss 1.3848 (1.1777)	grad_norm 0.3377 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:02:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:10:55 lr 0.000001	 wd 0.0500	time 0.2438 (0.2978)	loss 1.3655 (1.1775)	grad_norm 0.3303 (0.3342)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:02:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:10:03 lr 0.000001	 wd 0.0500	time 0.2457 (0.2870)	loss 1.0204 (1.1736)	grad_norm 0.3408 (0.3340)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:03:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:09:21 lr 0.000001	 wd 0.0500	time 0.2629 (0.2807)	loss 0.6947 (1.1746)	grad_norm 0.3278 (0.3336)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:03:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:08:47 lr 0.000001	 wd 0.0500	time 0.2569 (0.2771)	loss 1.3324 (1.1746)	grad_norm 0.3416 (0.3335)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:04:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:08:15 lr 0.000001	 wd 0.0500	time 0.2654 (0.2752)	loss 0.9954 (1.1764)	grad_norm 0.3400 (0.3335)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:04:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:07:44 lr 0.000001	 wd 0.0500	time 0.2579 (0.2729)	loss 0.9371 (1.1781)	grad_norm 0.3483 (0.3343)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:04:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:07:14 lr 0.000001	 wd 0.0500	time 0.2503 (0.2713)	loss 1.2765 (1.1761)	grad_norm 0.3300 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:05:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:06:46 lr 0.000001	 wd 0.0500	time 0.2444 (0.2704)	loss 1.1609 (1.1727)	grad_norm 0.3034 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:05:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:06:18 lr 0.000001	 wd 0.0500	time 0.2578 (0.2697)	loss 1.1590 (1.1750)	grad_norm 0.3268 (0.3348)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:06:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:05:50 lr 0.000001	 wd 0.0500	time 0.2756 (0.2690)	loss 0.7253 (1.1756)	grad_norm 0.3227 (0.3347)	loss_scale 8192.0000 (4171.0308)	mem 12226MB
[2024-07-31 15:06:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:05:22 lr 0.000001	 wd 0.0500	time 0.2404 (0.2684)	loss 0.7972 (1.1723)	grad_norm 0.3338 (0.3352)	loss_scale 8192.0000 (4480.0984)	mem 12226MB
[2024-07-31 15:07:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:04:55 lr 0.000001	 wd 0.0500	time 0.2443 (0.2677)	loss 0.8400 (1.1736)	grad_norm 0.3221 (0.3360)	loss_scale 8192.0000 (4745.0450)	mem 12226MB
[2024-07-31 15:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:04:27 lr 0.000001	 wd 0.0500	time 0.2732 (0.2674)	loss 1.4905 (1.1745)	grad_norm 0.3141 (nan)	loss_scale 4096.0000 (4718.1772)	mem 12226MB
[2024-07-31 15:07:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:04:00 lr 0.000001	 wd 0.0500	time 0.2851 (0.2670)	loss 1.3317 (1.1762)	grad_norm 0.3351 (nan)	loss_scale 4096.0000 (4679.3154)	mem 12226MB
[2024-07-31 15:08:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:03:34 lr 0.000001	 wd 0.0500	time 0.2558 (0.2676)	loss 1.3704 (1.1778)	grad_norm 0.3401 (nan)	loss_scale 4096.0000 (4645.0229)	mem 12226MB
[2024-07-31 15:08:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:07 lr 0.000001	 wd 0.0500	time 0.2855 (0.2676)	loss 1.4269 (1.1773)	grad_norm 0.3303 (nan)	loss_scale 4096.0000 (4614.5386)	mem 12226MB
[2024-07-31 15:09:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:40 lr 0.000001	 wd 0.0500	time 0.2398 (0.2673)	loss 1.3986 (1.1781)	grad_norm 0.3290 (nan)	loss_scale 4096.0000 (4587.2614)	mem 12226MB
[2024-07-31 15:09:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:14 lr 0.000001	 wd 0.0500	time 0.2599 (0.2670)	loss 1.4195 (1.1788)	grad_norm 0.3264 (nan)	loss_scale 4096.0000 (4562.7106)	mem 12226MB
[2024-07-31 15:10:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:47 lr 0.000001	 wd 0.0500	time 0.2574 (0.2670)	loss 1.5226 (1.1778)	grad_norm 0.3408 (nan)	loss_scale 4096.0000 (4540.4969)	mem 12226MB
[2024-07-31 15:10:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:20 lr 0.000001	 wd 0.0500	time 0.2651 (0.2668)	loss 1.1976 (1.1792)	grad_norm 0.3351 (nan)	loss_scale 4096.0000 (4520.3017)	mem 12226MB
[2024-07-31 15:11:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:53 lr 0.000001	 wd 0.0500	time 0.2666 (0.2668)	loss 1.3207 (1.1789)	grad_norm 0.3262 (nan)	loss_scale 4096.0000 (4501.8618)	mem 12226MB
[2024-07-31 15:11:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:27 lr 0.000001	 wd 0.0500	time 0.2770 (0.2667)	loss 1.3730 (1.1806)	grad_norm 0.3538 (nan)	loss_scale 4096.0000 (4484.9579)	mem 12226MB
[2024-07-31 15:11:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2397 (0.2662)	loss 1.2890 (1.1797)	grad_norm 0.3410 (nan)	loss_scale 4096.0000 (4469.4058)	mem 12226MB
[2024-07-31 15:12:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 27 training takes 0:11:11
[2024-07-31 15:12:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.578 (12.578)	Loss 0.4949 (0.4949)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 15:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.752 Acc@5 97.758
[2024-07-31 15:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 15:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 15:12:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][0/2502]	eta 6:27:01 lr 0.000001	 wd 0.0500	time 9.2810 (9.2810)	loss 0.9751 (0.9751)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:13:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:14:38 lr 0.000000	 wd 0.0500	time 0.2458 (0.3656)	loss 0.9124 (1.2142)	grad_norm 0.3266 (0.3340)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:11:53 lr 0.000000	 wd 0.0500	time 0.2432 (0.3099)	loss 1.5501 (1.2011)	grad_norm 0.3267 (0.3337)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:14:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:10:44 lr 0.000000	 wd 0.0500	time 0.2470 (0.2925)	loss 1.4538 (1.1954)	grad_norm 0.3482 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:14:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:09:56 lr 0.000000	 wd 0.0500	time 0.2407 (0.2837)	loss 0.7526 (1.1866)	grad_norm 0.3287 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 12226MB
[2024-07-31 15:14:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:09:18 lr 0.000000	 wd 0.0500	time 0.2777 (0.2787)	loss 0.8208 (1.1917)	grad_norm 0.3247 (nan)	loss_scale 2048.0000 (3711.7445)	mem 12226MB
[2024-07-31 15:15:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.2488 (0.2758)	loss 0.9011 (1.1842)	grad_norm 0.3522 (nan)	loss_scale 2048.0000 (3434.9151)	mem 12226MB
[2024-07-31 15:15:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:08:13 lr 0.000000	 wd 0.0500	time 0.2586 (0.2736)	loss 1.3030 (1.1810)	grad_norm 0.3447 (nan)	loss_scale 2048.0000 (3237.0670)	mem 12226MB
[2024-07-31 15:16:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:07:42 lr 0.000000	 wd 0.0500	time 0.2496 (0.2720)	loss 0.8541 (1.1814)	grad_norm 0.3267 (nan)	loss_scale 2048.0000 (3088.6192)	mem 12226MB
[2024-07-31 15:16:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:07:13 lr 0.000000	 wd 0.0500	time 0.2447 (0.2703)	loss 0.8571 (1.1777)	grad_norm 0.3462 (nan)	loss_scale 2048.0000 (2973.1232)	mem 12226MB
[2024-07-31 15:17:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:06:44 lr 0.000000	 wd 0.0500	time 0.2425 (0.2695)	loss 1.0188 (1.1794)	grad_norm 0.3253 (nan)	loss_scale 2048.0000 (2880.7033)	mem 12226MB
[2024-07-31 15:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:06:16 lr 0.000000	 wd 0.0500	time 0.2432 (0.2688)	loss 1.2869 (1.1797)	grad_norm 0.3361 (nan)	loss_scale 2048.0000 (2805.0718)	mem 12226MB
[2024-07-31 15:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:05:49 lr 0.000000	 wd 0.0500	time 0.2424 (0.2683)	loss 1.4664 (1.1798)	grad_norm 0.3642 (nan)	loss_scale 2048.0000 (2742.0350)	mem 12226MB
[2024-07-31 15:18:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:05:21 lr 0.000000	 wd 0.0500	time 0.2488 (0.2678)	loss 1.5300 (1.1813)	grad_norm 0.3391 (nan)	loss_scale 2048.0000 (2688.6887)	mem 12226MB
[2024-07-31 15:18:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:04:55 lr 0.000000	 wd 0.0500	time 0.2653 (0.2678)	loss 0.9609 (1.1807)	grad_norm 0.3272 (nan)	loss_scale 2048.0000 (2642.9579)	mem 12226MB
[2024-07-31 15:19:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:04:27 lr 0.000000	 wd 0.0500	time 0.2510 (0.2673)	loss 1.1936 (1.1792)	grad_norm 0.3310 (nan)	loss_scale 2048.0000 (2603.3205)	mem 12226MB
[2024-07-31 15:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:04:00 lr 0.000000	 wd 0.0500	time 0.2574 (0.2670)	loss 1.4185 (1.1820)	grad_norm 0.3609 (nan)	loss_scale 2048.0000 (2568.6346)	mem 12226MB
[2024-07-31 15:20:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:03:33 lr 0.000000	 wd 0.0500	time 0.2584 (0.2666)	loss 1.2530 (1.1814)	grad_norm 0.3465 (nan)	loss_scale 2048.0000 (2538.0270)	mem 12226MB
[2024-07-31 15:20:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:07 lr 0.000000	 wd 0.0500	time 0.2545 (0.2664)	loss 0.8036 (1.1810)	grad_norm 0.3348 (nan)	loss_scale 2048.0000 (2510.8184)	mem 12226MB
[2024-07-31 15:20:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:40 lr 0.000000	 wd 0.0500	time 0.2520 (0.2662)	loss 1.1075 (1.1786)	grad_norm 0.3298 (nan)	loss_scale 2048.0000 (2486.4724)	mem 12226MB
[2024-07-31 15:21:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:13 lr 0.000000	 wd 0.0500	time 0.2405 (0.2661)	loss 1.1315 (1.1787)	grad_norm 0.3375 (nan)	loss_scale 2048.0000 (2464.5597)	mem 12226MB
[2024-07-31 15:21:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:46 lr 0.000000	 wd 0.0500	time 0.2673 (0.2661)	loss 0.9834 (1.1787)	grad_norm 0.3781 (nan)	loss_scale 2048.0000 (2444.7330)	mem 12226MB
[2024-07-31 15:22:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:20 lr 0.000000	 wd 0.0500	time 0.2720 (0.2659)	loss 1.3054 (1.1778)	grad_norm 0.3383 (nan)	loss_scale 2048.0000 (2426.7079)	mem 12226MB
[2024-07-31 15:22:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:53 lr 0.000000	 wd 0.0500	time 0.2449 (0.2657)	loss 1.2433 (1.1777)	grad_norm 0.3370 (nan)	loss_scale 2048.0000 (2410.2495)	mem 12226MB
[2024-07-31 15:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:27 lr 0.000000	 wd 0.0500	time 0.2412 (0.2654)	loss 1.5325 (1.1763)	grad_norm 0.3175 (nan)	loss_scale 2048.0000 (2395.1620)	mem 12226MB
[2024-07-31 15:23:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2371 (0.2647)	loss 1.0836 (1.1756)	grad_norm 0.3202 (nan)	loss_scale 2048.0000 (2381.2811)	mem 12226MB
[2024-07-31 15:23:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 28 training takes 0:11:08
[2024-07-31 15:23:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.030 (11.030)	Loss 0.5024 (0.5024)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 12226MB
[2024-07-31 15:24:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.756 Acc@5 97.720
[2024-07-31 15:24:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 15:24:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 15:24:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:24:50 lr 0.000000	 wd 0.0500	time 12.1063 (12.1063)	loss 1.2419 (1.2419)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:14:57 lr 0.000000	 wd 0.0500	time 0.2387 (0.3735)	loss 1.4386 (1.1878)	grad_norm 0.3359 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:25:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:12:07 lr 0.000000	 wd 0.0500	time 0.2605 (0.3159)	loss 0.8393 (1.1856)	grad_norm 0.3552 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:25:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:10:53 lr 0.000000	 wd 0.0500	time 0.2433 (0.2968)	loss 0.8959 (1.1817)	grad_norm 0.3423 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:26:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:10:13 lr 0.000000	 wd 0.0500	time 0.3003 (0.2917)	loss 1.1205 (1.1754)	grad_norm 0.3405 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:09:29 lr 0.000000	 wd 0.0500	time 0.2690 (0.2846)	loss 1.3223 (1.1706)	grad_norm 0.3447 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:27:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:08:54 lr 0.000000	 wd 0.0500	time 0.2419 (0.2812)	loss 0.7417 (1.1744)	grad_norm 0.3344 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:27:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:08:20 lr 0.000000	 wd 0.0500	time 0.2643 (0.2778)	loss 1.1461 (1.1750)	grad_norm 0.3305 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:27:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:07:49 lr 0.000000	 wd 0.0500	time 0.2701 (0.2758)	loss 0.7842 (1.1758)	grad_norm 0.3223 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:28:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:07:20 lr 0.000000	 wd 0.0500	time 0.2401 (0.2747)	loss 1.4971 (1.1772)	grad_norm 0.3430 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:28:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:06:49 lr 0.000000	 wd 0.0500	time 0.2474 (0.2729)	loss 1.4472 (1.1780)	grad_norm 0.3345 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:29:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:06:20 lr 0.000000	 wd 0.0500	time 0.3113 (0.2716)	loss 1.2537 (1.1778)	grad_norm 0.3272 (0.3376)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:29:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:05:52 lr 0.000000	 wd 0.0500	time 0.2468 (0.2710)	loss 1.0249 (1.1755)	grad_norm 0.3166 (0.3372)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:30:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:05:24 lr 0.000000	 wd 0.0500	time 0.2613 (0.2702)	loss 1.1609 (1.1790)	grad_norm 0.3545 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:30:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:04:56 lr 0.000000	 wd 0.0500	time 0.2921 (0.2692)	loss 1.1322 (1.1740)	grad_norm 0.3541 (0.3370)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:30:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:04:29 lr 0.000000	 wd 0.0500	time 0.2490 (0.2690)	loss 1.0354 (1.1742)	grad_norm 0.3317 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:31:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:04:02 lr 0.000000	 wd 0.0500	time 0.2733 (0.2686)	loss 0.8310 (1.1763)	grad_norm 0.3300 (0.3368)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:31:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:03:34 lr 0.000000	 wd 0.0500	time 0.2381 (0.2678)	loss 1.3732 (1.1790)	grad_norm 0.3385 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:07 lr 0.000000	 wd 0.0500	time 0.2410 (0.2675)	loss 1.2092 (1.1774)	grad_norm 0.3146 (0.3370)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:32:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:40 lr 0.000000	 wd 0.0500	time 0.2708 (0.2674)	loss 0.7863 (1.1777)	grad_norm 0.3517 (0.3368)	loss_scale 2048.0000 (2048.0000)	mem 12226MB
[2024-07-31 15:33:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:14 lr 0.000000	 wd 0.0500	time 0.2411 (0.2670)	loss 0.9105 (1.1795)	grad_norm 0.3393 (0.3367)	loss_scale 4096.0000 (2146.2549)	mem 12226MB
[2024-07-31 15:33:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:47 lr 0.000000	 wd 0.0500	time 0.2395 (0.2669)	loss 0.7785 (1.1804)	grad_norm 0.3054 (0.3366)	loss_scale 4096.0000 (2239.0557)	mem 12226MB
[2024-07-31 15:33:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:20 lr 0.000000	 wd 0.0500	time 0.2382 (0.2666)	loss 0.8498 (1.1810)	grad_norm 0.3445 (0.3365)	loss_scale 4096.0000 (2323.4239)	mem 12226MB
[2024-07-31 15:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:53 lr 0.000000	 wd 0.0500	time 0.2596 (0.2666)	loss 0.8578 (1.1780)	grad_norm 0.3461 (0.3366)	loss_scale 4096.0000 (2400.4589)	mem 12226MB
[2024-07-31 15:34:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:27 lr 0.000000	 wd 0.0500	time 0.2779 (0.2662)	loss 1.2746 (1.1781)	grad_norm 0.3362 (0.3367)	loss_scale 4096.0000 (2471.0771)	mem 12226MB
[2024-07-31 15:35:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2377 (0.2656)	loss 1.3474 (1.1773)	grad_norm 0.3210 (0.3365)	loss_scale 4096.0000 (2536.0480)	mem 12226MB
[2024-07-31 15:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 29 training takes 0:11:09
[2024-07-31 15:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_29.pth saving......
[2024-07-31 15:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_29.pth saved !!!
[2024-07-31 15:35:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.149 (11.149)	Loss 0.4922 (0.4922)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 12226MB
[2024-07-31 15:35:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.768 Acc@5 97.756
[2024-07-31 15:35:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 15:35:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-07-31 15:35:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 189): INFO Training time 5:49:06
