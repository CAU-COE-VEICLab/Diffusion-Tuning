[2024-07-03 09:03:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/config.json
[2024-07-03 09:03:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_large_224_22kto1k_efficient_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: adapter_smt_l_22kto1k_efficient_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-03 09:03:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_smt/adapter_smt_large_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "adapter_smt_l_22kto1k_efficient_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-03 09:03:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 108): INFO Creating model:adapter_smt/adapter_smt_large_224_22kto1k_efficient_finetune
[2024-07-03 09:03:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 110): INFO Adapter_SMT(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-03 09:03:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 113): INFO number of params: 4162792
[2024-07-03 09:03:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune, ignoring auto resume
[2024-07-03 09:03:38 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth for fine-tuning......
[2024-07-03 09:03:41 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-03 09:03:41 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['block1.0.adapter.down.weight', 'block1.0.adapter.down.bias', 'block1.0.adapter.up.weight', 'block1.0.adapter.up.bias', 'block1.1.adapter.down.weight', 'block1.1.adapter.down.bias', 'block1.1.adapter.up.weight', 'block1.1.adapter.up.bias', 'block1.2.adapter.down.weight', 'block1.2.adapter.down.bias', 'block1.2.adapter.up.weight', 'block1.2.adapter.up.bias', 'block1.3.adapter.down.weight', 'block1.3.adapter.down.bias', 'block1.3.adapter.up.weight', 'block1.3.adapter.up.bias', 'block2.0.adapter.down.weight', 'block2.0.adapter.down.bias', 'block2.0.adapter.up.weight', 'block2.0.adapter.up.bias', 'block2.1.adapter.down.weight', 'block2.1.adapter.down.bias', 'block2.1.adapter.up.weight', 'block2.1.adapter.up.bias', 'block2.2.adapter.down.weight', 'block2.2.adapter.down.bias', 'block2.2.adapter.up.weight', 'block2.2.adapter.up.bias', 'block2.3.adapter.down.weight', 'block2.3.adapter.down.bias', 'block2.3.adapter.up.weight', 'block2.3.adapter.up.bias', 'block2.4.adapter.down.weight', 'block2.4.adapter.down.bias', 'block2.4.adapter.up.weight', 'block2.4.adapter.up.bias', 'block2.5.adapter.down.weight', 'block2.5.adapter.down.bias', 'block2.5.adapter.up.weight', 'block2.5.adapter.up.bias', 'block3.0.adapter.down.weight', 'block3.0.adapter.down.bias', 'block3.0.adapter.up.weight', 'block3.0.adapter.up.bias', 'block3.1.adapter.down.weight', 'block3.1.adapter.down.bias', 'block3.1.adapter.up.weight', 'block3.1.adapter.up.bias', 'block3.2.adapter.down.weight', 'block3.2.adapter.down.bias', 'block3.2.adapter.up.weight', 'block3.2.adapter.up.bias', 'block3.3.adapter.down.weight', 'block3.3.adapter.down.bias', 'block3.3.adapter.up.weight', 'block3.3.adapter.up.bias', 'block3.4.adapter.down.weight', 'block3.4.adapter.down.bias', 'block3.4.adapter.up.weight', 'block3.4.adapter.up.bias', 'block3.5.adapter.down.weight', 'block3.5.adapter.down.bias', 'block3.5.adapter.up.weight', 'block3.5.adapter.up.bias', 'block3.6.adapter.down.weight', 'block3.6.adapter.down.bias', 'block3.6.adapter.up.weight', 'block3.6.adapter.up.bias', 'block3.7.adapter.down.weight', 'block3.7.adapter.down.bias', 'block3.7.adapter.up.weight', 'block3.7.adapter.up.bias', 'block3.8.adapter.down.weight', 'block3.8.adapter.down.bias', 'block3.8.adapter.up.weight', 'block3.8.adapter.up.bias', 'block3.9.adapter.down.weight', 'block3.9.adapter.down.bias', 'block3.9.adapter.up.weight', 'block3.9.adapter.up.bias', 'block3.10.adapter.down.weight', 'block3.10.adapter.down.bias', 'block3.10.adapter.up.weight', 'block3.10.adapter.up.bias', 'block3.11.adapter.down.weight', 'block3.11.adapter.down.bias', 'block3.11.adapter.up.weight', 'block3.11.adapter.up.bias', 'block3.12.adapter.down.weight', 'block3.12.adapter.down.bias', 'block3.12.adapter.up.weight', 'block3.12.adapter.up.bias', 'block3.13.adapter.down.weight', 'block3.13.adapter.down.bias', 'block3.13.adapter.up.weight', 'block3.13.adapter.up.bias', 'block3.14.adapter.down.weight', 'block3.14.adapter.down.bias', 'block3.14.adapter.up.weight', 'block3.14.adapter.up.bias', 'block3.15.adapter.down.weight', 'block3.15.adapter.down.bias', 'block3.15.adapter.up.weight', 'block3.15.adapter.up.bias', 'block3.16.adapter.down.weight', 'block3.16.adapter.down.bias', 'block3.16.adapter.up.weight', 'block3.16.adapter.up.bias', 'block3.17.adapter.down.weight', 'block3.17.adapter.down.bias', 'block3.17.adapter.up.weight', 'block3.17.adapter.up.bias', 'block3.18.adapter.down.weight', 'block3.18.adapter.down.bias', 'block3.18.adapter.up.weight', 'block3.18.adapter.up.bias', 'block3.19.adapter.down.weight', 'block3.19.adapter.down.bias', 'block3.19.adapter.up.weight', 'block3.19.adapter.up.bias', 'block3.20.adapter.down.weight', 'block3.20.adapter.down.bias', 'block3.20.adapter.up.weight', 'block3.20.adapter.up.bias', 'block3.21.adapter.down.weight', 'block3.21.adapter.down.bias', 'block3.21.adapter.up.weight', 'block3.21.adapter.up.bias', 'block3.22.adapter.down.weight', 'block3.22.adapter.down.bias', 'block3.22.adapter.up.weight', 'block3.22.adapter.up.bias', 'block3.23.adapter.down.weight', 'block3.23.adapter.down.bias', 'block3.23.adapter.up.weight', 'block3.23.adapter.up.bias', 'block3.24.adapter.down.weight', 'block3.24.adapter.down.bias', 'block3.24.adapter.up.weight', 'block3.24.adapter.up.bias', 'block3.25.adapter.down.weight', 'block3.25.adapter.down.bias', 'block3.25.adapter.up.weight', 'block3.25.adapter.up.bias', 'block3.26.adapter.down.weight', 'block3.26.adapter.down.bias', 'block3.26.adapter.up.weight', 'block3.26.adapter.up.bias', 'block3.27.adapter.down.weight', 'block3.27.adapter.down.bias', 'block3.27.adapter.up.weight', 'block3.27.adapter.up.bias', 'block4.0.adapter.down.weight', 'block4.0.adapter.down.bias', 'block4.0.adapter.up.weight', 'block4.0.adapter.up.bias', 'block4.1.adapter.down.weight', 'block4.1.adapter.down.bias', 'block4.1.adapter.up.weight', 'block4.1.adapter.up.bias', 'block4.2.adapter.down.weight', 'block4.2.adapter.down.bias', 'block4.2.adapter.up.weight', 'block4.2.adapter.up.bias', 'block4.3.adapter.down.weight', 'block4.3.adapter.down.bias', 'block4.3.adapter.up.weight', 'block4.3.adapter.up.bias'], unexpected_keys=['block1.0.gamma_1', 'block1.0.gamma_2', 'block1.1.gamma_1', 'block1.1.gamma_2', 'block1.2.gamma_1', 'block1.2.gamma_2', 'block1.3.gamma_1', 'block1.3.gamma_2', 'block2.0.gamma_1', 'block2.0.gamma_2', 'block2.1.gamma_1', 'block2.1.gamma_2', 'block2.2.gamma_1', 'block2.2.gamma_2', 'block2.3.gamma_1', 'block2.3.gamma_2', 'block2.4.gamma_1', 'block2.4.gamma_2', 'block2.5.gamma_1', 'block2.5.gamma_2', 'block3.0.gamma_1', 'block3.0.gamma_2', 'block3.1.gamma_1', 'block3.1.gamma_2', 'block3.2.gamma_1', 'block3.2.gamma_2', 'block3.3.gamma_1', 'block3.3.gamma_2', 'block3.4.gamma_1', 'block3.4.gamma_2', 'block3.5.gamma_1', 'block3.5.gamma_2', 'block3.6.gamma_1', 'block3.6.gamma_2', 'block3.7.gamma_1', 'block3.7.gamma_2', 'block3.8.gamma_1', 'block3.8.gamma_2', 'block3.9.gamma_1', 'block3.9.gamma_2', 'block3.10.gamma_1', 'block3.10.gamma_2', 'block3.11.gamma_1', 'block3.11.gamma_2', 'block3.12.gamma_1', 'block3.12.gamma_2', 'block3.13.gamma_1', 'block3.13.gamma_2', 'block3.14.gamma_1', 'block3.14.gamma_2', 'block3.15.gamma_1', 'block3.15.gamma_2', 'block3.16.gamma_1', 'block3.16.gamma_2', 'block3.17.gamma_1', 'block3.17.gamma_2', 'block3.18.gamma_1', 'block3.18.gamma_2', 'block3.19.gamma_1', 'block3.19.gamma_2', 'block3.20.gamma_1', 'block3.20.gamma_2', 'block3.21.gamma_1', 'block3.21.gamma_2', 'block3.22.gamma_1', 'block3.22.gamma_2', 'block3.23.gamma_1', 'block3.23.gamma_2', 'block3.24.gamma_1', 'block3.24.gamma_2', 'block3.25.gamma_1', 'block3.25.gamma_2', 'block3.26.gamma_1', 'block3.26.gamma_2', 'block3.27.gamma_1', 'block3.27.gamma_2', 'block4.0.gamma_1', 'block4.0.gamma_2', 'block4.1.gamma_1', 'block4.1.gamma_2', 'block4.2.gamma_1', 'block4.2.gamma_2', 'block4.3.gamma_1', 'block4.3.gamma_2'])
[2024-07-03 09:03:41 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth'
[2024-07-03 09:04:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 63.911 (63.911)	Loss 7.1133 (7.1133)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 2339MB
[2024-07-03 09:05:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 0.104 Acc@5 0.552
[2024-07-03 09:05:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 0.1%
[2024-07-03 09:05:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 168): INFO Start training
[2024-07-03 09:05:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 13:18:11 lr 0.000000	 wd 0.0500	time 19.1411 (19.1411)	loss 4.1284 (4.1284)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17016MB
[2024-07-03 09:06:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:24:11 lr 0.000000	 wd 0.0500	time 0.4171 (0.6043)	loss 4.1484 (4.1155)	grad_norm 2.5893 (2.5814)	loss_scale 65536.0000 (65536.0000)	mem 17016MB
[2024-07-03 09:06:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:18:52 lr 0.000000	 wd 0.0500	time 0.3646 (0.4920)	loss 4.2216 (4.1142)	grad_norm 2.4566 (2.5595)	loss_scale 65536.0000 (65536.0000)	mem 17016MB
[2024-07-03 09:07:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:16:46 lr 0.000000	 wd 0.0500	time 0.4055 (0.4570)	loss 4.1284 (4.1107)	grad_norm 2.4273 (2.5323)	loss_scale 65536.0000 (65536.0000)	mem 17016MB
[2024-07-03 09:08:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:15:30 lr 0.000001	 wd 0.0500	time 0.3544 (0.4426)	loss 4.0577 (4.1096)	grad_norm 2.3279 (2.4945)	loss_scale 65536.0000 (65536.0000)	mem 17016MB
[2024-07-03 09:08:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:14:22 lr 0.000001	 wd 0.0500	time 0.3442 (0.4308)	loss 4.0920 (4.1040)	grad_norm 2.2353 (2.4493)	loss_scale 65536.0000 (65536.0000)	mem 17016MB
[2024-07-03 14:11:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/config.json
[2024-07-03 14:11:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_large_224_22kto1k_efficient_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: adapter_smt_l_22kto1k_efficient_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-03 14:11:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_smt/adapter_smt_large_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "adapter_smt_l_22kto1k_efficient_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-03 14:11:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 108): INFO Creating model:adapter_smt/adapter_smt_large_224_22kto1k_efficient_finetune
[2024-07-03 14:11:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 110): INFO Adapter_SMT(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-03 14:11:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 113): INFO number of params: 4162792
[2024-07-03 14:11:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune, ignoring auto resume
[2024-07-03 14:11:48 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth for fine-tuning......
[2024-07-03 14:11:51 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-03 14:11:51 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['block1.0.adapter.down.weight', 'block1.0.adapter.down.bias', 'block1.0.adapter.up.weight', 'block1.0.adapter.up.bias', 'block1.1.adapter.down.weight', 'block1.1.adapter.down.bias', 'block1.1.adapter.up.weight', 'block1.1.adapter.up.bias', 'block1.2.adapter.down.weight', 'block1.2.adapter.down.bias', 'block1.2.adapter.up.weight', 'block1.2.adapter.up.bias', 'block1.3.adapter.down.weight', 'block1.3.adapter.down.bias', 'block1.3.adapter.up.weight', 'block1.3.adapter.up.bias', 'block2.0.adapter.down.weight', 'block2.0.adapter.down.bias', 'block2.0.adapter.up.weight', 'block2.0.adapter.up.bias', 'block2.1.adapter.down.weight', 'block2.1.adapter.down.bias', 'block2.1.adapter.up.weight', 'block2.1.adapter.up.bias', 'block2.2.adapter.down.weight', 'block2.2.adapter.down.bias', 'block2.2.adapter.up.weight', 'block2.2.adapter.up.bias', 'block2.3.adapter.down.weight', 'block2.3.adapter.down.bias', 'block2.3.adapter.up.weight', 'block2.3.adapter.up.bias', 'block2.4.adapter.down.weight', 'block2.4.adapter.down.bias', 'block2.4.adapter.up.weight', 'block2.4.adapter.up.bias', 'block2.5.adapter.down.weight', 'block2.5.adapter.down.bias', 'block2.5.adapter.up.weight', 'block2.5.adapter.up.bias', 'block3.0.adapter.down.weight', 'block3.0.adapter.down.bias', 'block3.0.adapter.up.weight', 'block3.0.adapter.up.bias', 'block3.1.adapter.down.weight', 'block3.1.adapter.down.bias', 'block3.1.adapter.up.weight', 'block3.1.adapter.up.bias', 'block3.2.adapter.down.weight', 'block3.2.adapter.down.bias', 'block3.2.adapter.up.weight', 'block3.2.adapter.up.bias', 'block3.3.adapter.down.weight', 'block3.3.adapter.down.bias', 'block3.3.adapter.up.weight', 'block3.3.adapter.up.bias', 'block3.4.adapter.down.weight', 'block3.4.adapter.down.bias', 'block3.4.adapter.up.weight', 'block3.4.adapter.up.bias', 'block3.5.adapter.down.weight', 'block3.5.adapter.down.bias', 'block3.5.adapter.up.weight', 'block3.5.adapter.up.bias', 'block3.6.adapter.down.weight', 'block3.6.adapter.down.bias', 'block3.6.adapter.up.weight', 'block3.6.adapter.up.bias', 'block3.7.adapter.down.weight', 'block3.7.adapter.down.bias', 'block3.7.adapter.up.weight', 'block3.7.adapter.up.bias', 'block3.8.adapter.down.weight', 'block3.8.adapter.down.bias', 'block3.8.adapter.up.weight', 'block3.8.adapter.up.bias', 'block3.9.adapter.down.weight', 'block3.9.adapter.down.bias', 'block3.9.adapter.up.weight', 'block3.9.adapter.up.bias', 'block3.10.adapter.down.weight', 'block3.10.adapter.down.bias', 'block3.10.adapter.up.weight', 'block3.10.adapter.up.bias', 'block3.11.adapter.down.weight', 'block3.11.adapter.down.bias', 'block3.11.adapter.up.weight', 'block3.11.adapter.up.bias', 'block3.12.adapter.down.weight', 'block3.12.adapter.down.bias', 'block3.12.adapter.up.weight', 'block3.12.adapter.up.bias', 'block3.13.adapter.down.weight', 'block3.13.adapter.down.bias', 'block3.13.adapter.up.weight', 'block3.13.adapter.up.bias', 'block3.14.adapter.down.weight', 'block3.14.adapter.down.bias', 'block3.14.adapter.up.weight', 'block3.14.adapter.up.bias', 'block3.15.adapter.down.weight', 'block3.15.adapter.down.bias', 'block3.15.adapter.up.weight', 'block3.15.adapter.up.bias', 'block3.16.adapter.down.weight', 'block3.16.adapter.down.bias', 'block3.16.adapter.up.weight', 'block3.16.adapter.up.bias', 'block3.17.adapter.down.weight', 'block3.17.adapter.down.bias', 'block3.17.adapter.up.weight', 'block3.17.adapter.up.bias', 'block3.18.adapter.down.weight', 'block3.18.adapter.down.bias', 'block3.18.adapter.up.weight', 'block3.18.adapter.up.bias', 'block3.19.adapter.down.weight', 'block3.19.adapter.down.bias', 'block3.19.adapter.up.weight', 'block3.19.adapter.up.bias', 'block3.20.adapter.down.weight', 'block3.20.adapter.down.bias', 'block3.20.adapter.up.weight', 'block3.20.adapter.up.bias', 'block3.21.adapter.down.weight', 'block3.21.adapter.down.bias', 'block3.21.adapter.up.weight', 'block3.21.adapter.up.bias', 'block3.22.adapter.down.weight', 'block3.22.adapter.down.bias', 'block3.22.adapter.up.weight', 'block3.22.adapter.up.bias', 'block3.23.adapter.down.weight', 'block3.23.adapter.down.bias', 'block3.23.adapter.up.weight', 'block3.23.adapter.up.bias', 'block3.24.adapter.down.weight', 'block3.24.adapter.down.bias', 'block3.24.adapter.up.weight', 'block3.24.adapter.up.bias', 'block3.25.adapter.down.weight', 'block3.25.adapter.down.bias', 'block3.25.adapter.up.weight', 'block3.25.adapter.up.bias', 'block3.26.adapter.down.weight', 'block3.26.adapter.down.bias', 'block3.26.adapter.up.weight', 'block3.26.adapter.up.bias', 'block3.27.adapter.down.weight', 'block3.27.adapter.down.bias', 'block3.27.adapter.up.weight', 'block3.27.adapter.up.bias', 'block4.0.adapter.down.weight', 'block4.0.adapter.down.bias', 'block4.0.adapter.up.weight', 'block4.0.adapter.up.bias', 'block4.1.adapter.down.weight', 'block4.1.adapter.down.bias', 'block4.1.adapter.up.weight', 'block4.1.adapter.up.bias', 'block4.2.adapter.down.weight', 'block4.2.adapter.down.bias', 'block4.2.adapter.up.weight', 'block4.2.adapter.up.bias', 'block4.3.adapter.down.weight', 'block4.3.adapter.down.bias', 'block4.3.adapter.up.weight', 'block4.3.adapter.up.bias'], unexpected_keys=[])
[2024-07-03 14:11:51 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth'
[2024-07-03 14:12:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 48.662 (48.662)	Loss 0.4038 (0.4038)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 2339MB
[2024-07-03 14:13:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.486 Acc@5 97.126
[2024-07-03 14:13:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-03 14:13:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 168): INFO Start training
[2024-07-03 14:13:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 17:12:27 lr 0.000000	 wd 0.0500	time 24.7590 (24.7590)	loss 1.6624 (1.6624)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17019MB
[2024-07-03 14:14:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:28:01 lr 0.000000	 wd 0.0500	time 0.3508 (0.6999)	loss 1.4310 (1.3244)	grad_norm 0.4366 (nan)	loss_scale 8192.0000 (12004.1188)	mem 17019MB
[2024-07-03 14:14:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:20:58 lr 0.000000	 wd 0.0500	time 0.3544 (0.5465)	loss 1.1041 (1.3133)	grad_norm 0.4389 (nan)	loss_scale 8192.0000 (10107.5423)	mem 17019MB
[2024-07-03 14:15:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:18:59 lr 0.000000	 wd 0.0500	time 0.3963 (0.5176)	loss 0.9622 (1.2708)	grad_norm 0.4435 (nan)	loss_scale 8192.0000 (9471.1495)	mem 17019MB
[2024-07-03 14:16:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:17:06 lr 0.000001	 wd 0.0500	time 0.3604 (0.4884)	loss 1.0342 (1.2784)	grad_norm 0.5423 (nan)	loss_scale 8192.0000 (9152.1596)	mem 17019MB
[2024-07-03 14:17:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:15:57 lr 0.000001	 wd 0.0500	time 0.4537 (0.4784)	loss 1.2558 (1.2833)	grad_norm 0.4061 (nan)	loss_scale 8192.0000 (8960.5110)	mem 17019MB
[2024-07-03 14:17:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:15:09 lr 0.000001	 wd 0.0500	time 0.3689 (0.4784)	loss 1.4431 (1.2827)	grad_norm 0.4218 (nan)	loss_scale 4096.0000 (8464.6123)	mem 17019MB
[2024-07-03 14:18:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:13:56 lr 0.000001	 wd 0.0500	time 0.3914 (0.4644)	loss 1.3682 (1.2847)	grad_norm 0.4616 (nan)	loss_scale 4096.0000 (7841.4151)	mem 17019MB
[2024-07-03 14:19:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:12:58 lr 0.000001	 wd 0.0500	time 0.4049 (0.4573)	loss 1.1736 (1.2834)	grad_norm 0.5950 (nan)	loss_scale 4096.0000 (7373.8227)	mem 17019MB
[2024-07-03 14:19:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:11:59 lr 0.000001	 wd 0.0500	time 0.3422 (0.4488)	loss 1.4709 (1.2822)	grad_norm 0.4513 (nan)	loss_scale 4096.0000 (7010.0244)	mem 17019MB
[2024-07-03 14:20:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:11:07 lr 0.000002	 wd 0.0500	time 0.3848 (0.4447)	loss 1.5886 (1.2796)	grad_norm 0.4398 (nan)	loss_scale 4096.0000 (6718.9131)	mem 17019MB
[2024-07-03 14:21:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:10:17 lr 0.000002	 wd 0.0500	time 0.3480 (0.4402)	loss 1.3633 (1.2804)	grad_norm 0.4276 (nan)	loss_scale 4096.0000 (6480.6830)	mem 17019MB
[2024-07-03 14:21:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:09:27 lr 0.000002	 wd 0.0500	time 0.3941 (0.4359)	loss 0.9683 (1.2798)	grad_norm 0.4237 (nan)	loss_scale 4096.0000 (6282.1249)	mem 17019MB
[2024-07-03 14:22:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:08:40 lr 0.000002	 wd 0.0500	time 0.3855 (0.4332)	loss 1.2937 (1.2808)	grad_norm 0.4762 (nan)	loss_scale 4096.0000 (6114.0907)	mem 17019MB
[2024-07-03 14:23:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:07:53 lr 0.000002	 wd 0.0500	time 0.3904 (0.4301)	loss 1.5573 (1.2823)	grad_norm 0.4217 (nan)	loss_scale 4096.0000 (5970.0443)	mem 17019MB
[2024-07-03 14:23:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:07:15 lr 0.000002	 wd 0.0500	time 0.6605 (0.4348)	loss 1.3081 (1.2858)	grad_norm 0.4266 (nan)	loss_scale 2048.0000 (5738.7662)	mem 17019MB
[2024-07-03 14:24:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:06:39 lr 0.000003	 wd 0.0500	time 0.4695 (0.4430)	loss 1.3017 (1.2821)	grad_norm 0.4049 (nan)	loss_scale 2048.0000 (5508.2374)	mem 17019MB
[2024-07-03 14:26:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:06:25 lr 0.000003	 wd 0.0500	time 1.0075 (0.4811)	loss 0.9588 (1.2808)	grad_norm 0.4313 (nan)	loss_scale 2048.0000 (5304.8136)	mem 17019MB
[2024-07-03 14:27:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:05:43 lr 0.000003	 wd 0.0500	time 0.3908 (0.4886)	loss 1.2038 (1.2799)	grad_norm 0.4132 (nan)	loss_scale 2048.0000 (5123.9800)	mem 17019MB
[2024-07-03 14:28:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:04:51 lr 0.000003	 wd 0.0500	time 0.4215 (0.4847)	loss 1.5413 (1.2785)	grad_norm 0.4133 (nan)	loss_scale 2048.0000 (4962.1715)	mem 17019MB
[2024-07-03 14:29:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:04:01 lr 0.000003	 wd 0.0500	time 0.4359 (0.4806)	loss 0.9028 (1.2764)	grad_norm 0.4080 (nan)	loss_scale 2048.0000 (4816.5357)	mem 17019MB
[2024-07-03 14:29:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:11 lr 0.000003	 wd 0.0500	time 0.3821 (0.4764)	loss 1.1465 (1.2742)	grad_norm 0.4201 (nan)	loss_scale 2048.0000 (4684.7634)	mem 17019MB
[2024-07-03 14:30:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:22 lr 0.000004	 wd 0.0500	time 0.3763 (0.4735)	loss 1.6027 (1.2727)	grad_norm 0.4067 (nan)	loss_scale 2048.0000 (4564.9650)	mem 17019MB
[2024-07-03 14:31:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:34 lr 0.000004	 wd 0.0500	time 0.3716 (0.4701)	loss 1.4052 (1.2722)	grad_norm 0.4237 (nan)	loss_scale 2048.0000 (4455.5793)	mem 17019MB
[2024-07-03 14:31:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:47 lr 0.000004	 wd 0.0500	time 0.3552 (0.4671)	loss 1.4209 (1.2711)	grad_norm 0.3982 (nan)	loss_scale 2048.0000 (4355.3053)	mem 17019MB
[2024-07-03 14:32:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.3312 (0.4663)	loss 0.9425 (1.2680)	grad_norm 0.4105 (nan)	loss_scale 2048.0000 (4263.0500)	mem 17019MB
[2024-07-03 14:32:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 0 training takes 0:19:38
[2024-07-03 14:32:40 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_0.pth saving......
[2024-07-03 14:32:41 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_0.pth saved !!!
[2024-07-03 14:34:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 115.504 (115.504)	Loss 0.4392 (0.4392)	Acc@1 92.773 (92.773)	Acc@5 98.047 (98.047)	Mem 17019MB
[2024-07-03 14:35:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.732 Acc@5 97.242
[2024-07-03 14:35:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 14:35:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.73%
[2024-07-03 14:35:09 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 14:35:10 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 14:35:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][0/2502]	eta 1 day, 0:52:28 lr 0.000004	 wd 0.0500	time 35.7907 (35.7907)	loss 1.1971 (1.1971)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:36:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:29:32 lr 0.000004	 wd 0.0500	time 0.3725 (0.7379)	loss 1.2450 (1.2248)	grad_norm 0.3934 (0.3963)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:37:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:21:53 lr 0.000004	 wd 0.0500	time 0.3597 (0.5707)	loss 0.7979 (1.2380)	grad_norm 0.4080 (0.4055)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:37:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:18:40 lr 0.000004	 wd 0.0500	time 0.3886 (0.5090)	loss 0.9432 (1.2369)	grad_norm 0.3792 (0.4042)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:38:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:16:46 lr 0.000005	 wd 0.0500	time 0.3934 (0.4790)	loss 1.0243 (1.2509)	grad_norm 0.3888 (0.4055)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:39:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:15:23 lr 0.000005	 wd 0.0500	time 0.3701 (0.4613)	loss 1.5460 (1.2463)	grad_norm 0.3689 (0.4085)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:39:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:14:15 lr 0.000005	 wd 0.0500	time 0.3949 (0.4499)	loss 1.5646 (1.2433)	grad_norm 0.3641 (0.4056)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:40:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:13:17 lr 0.000005	 wd 0.0500	time 0.3624 (0.4427)	loss 0.8999 (1.2380)	grad_norm 0.3606 (0.4012)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:40:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:12:21 lr 0.000005	 wd 0.0500	time 0.3503 (0.4354)	loss 1.4973 (1.2363)	grad_norm 0.3406 (0.4005)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:41:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:11:29 lr 0.000005	 wd 0.0500	time 0.4019 (0.4303)	loss 1.2054 (1.2339)	grad_norm 0.3864 (0.3982)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:42:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:11:09 lr 0.000006	 wd 0.0500	time 0.6639 (0.4460)	loss 1.4075 (1.2314)	grad_norm 0.3806 (0.3963)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:43:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:11:14 lr 0.000006	 wd 0.0500	time 0.6958 (0.4812)	loss 1.4693 (1.2287)	grad_norm 0.3537 (0.3959)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:45:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:11:07 lr 0.000006	 wd 0.0500	time 0.3872 (0.5127)	loss 1.0200 (1.2253)	grad_norm 0.3458 (0.3926)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:46:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:10:04 lr 0.000006	 wd 0.0500	time 0.3713 (0.5028)	loss 1.5937 (1.2254)	grad_norm 0.3666 (0.3918)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:46:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:09:05 lr 0.000006	 wd 0.0500	time 0.3939 (0.4954)	loss 0.9693 (1.2242)	grad_norm 0.3869 (0.3894)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:47:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:08:09 lr 0.000006	 wd 0.0500	time 0.3808 (0.4885)	loss 1.4807 (1.2241)	grad_norm 0.3383 (0.3869)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:48:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:07:15 lr 0.000007	 wd 0.0500	time 0.4033 (0.4829)	loss 1.3480 (1.2250)	grad_norm 0.3444 (0.3850)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:48:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:06:24 lr 0.000007	 wd 0.0500	time 0.3351 (0.4791)	loss 1.0271 (1.2245)	grad_norm 0.3779 (0.3831)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:49:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:05:33 lr 0.000007	 wd 0.0500	time 0.3775 (0.4747)	loss 1.7604 (1.2233)	grad_norm 0.3470 (0.3814)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:50:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:04:49 lr 0.000007	 wd 0.0500	time 0.4928 (0.4812)	loss 1.4815 (1.2238)	grad_norm 0.3516 (0.3809)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:51:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:04:02 lr 0.000007	 wd 0.0500	time 0.5304 (0.4837)	loss 1.4816 (1.2237)	grad_norm 0.3579 (0.3795)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:52:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:18 lr 0.000007	 wd 0.0500	time 0.5501 (0.4927)	loss 1.5447 (1.2253)	grad_norm 0.3644 (0.3782)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:53:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:30 lr 0.000008	 wd 0.0500	time 0.5743 (0.4983)	loss 1.2635 (1.2252)	grad_norm 0.3317 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:54:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:41 lr 0.000008	 wd 0.0500	time 0.4126 (0.5001)	loss 1.0130 (1.2270)	grad_norm 0.4480 (0.3759)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:55:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:51 lr 0.000008	 wd 0.0500	time 0.5697 (0.5039)	loss 1.2557 (1.2267)	grad_norm 0.3357 (0.3745)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:55:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.3561 (0.4996)	loss 0.9134 (1.2264)	grad_norm 0.3448 (0.3735)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:56:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 1 training takes 0:20:56
[2024-07-03 14:57:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 110.156 (110.156)	Loss 0.5254 (0.5254)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 14:58:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.072 Acc@5 97.328
[2024-07-03 14:58:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 14:58:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.07%
[2024-07-03 14:58:35 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 14:58:36 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 14:58:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][0/2502]	eta 14:03:14 lr 0.000008	 wd 0.0500	time 20.2216 (20.2216)	loss 1.2968 (1.2968)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 14:59:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:23:30 lr 0.000008	 wd 0.0500	time 0.4165 (0.5871)	loss 1.5049 (1.2190)	grad_norm 0.3354 (0.3455)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 15:00:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:18:57 lr 0.000008	 wd 0.0500	time 0.3844 (0.4943)	loss 1.4505 (1.2088)	grad_norm 0.3348 (0.3479)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 15:00:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:16:46 lr 0.000008	 wd 0.0500	time 0.3435 (0.4570)	loss 1.2099 (1.1988)	grad_norm 0.3399 (0.3462)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 15:01:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:15:28 lr 0.000009	 wd 0.0500	time 0.4212 (0.4416)	loss 1.4696 (1.2013)	grad_norm 0.3227 (0.3485)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 15:02:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:14:24 lr 0.000009	 wd 0.0500	time 0.4026 (0.4316)	loss 1.4856 (1.1965)	grad_norm 0.3475 (0.3477)	loss_scale 4096.0000 (2383.2016)	mem 17019MB
[2024-07-03 15:02:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:13:27 lr 0.000009	 wd 0.0500	time 0.3533 (0.4247)	loss 1.2968 (1.1976)	grad_norm 0.3387 (0.3499)	loss_scale 4096.0000 (2668.1930)	mem 17019MB
[2024-07-03 15:03:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:12:39 lr 0.000009	 wd 0.0500	time 0.3831 (0.4212)	loss 1.3584 (1.2025)	grad_norm 0.3416 (0.3484)	loss_scale 4096.0000 (2871.8745)	mem 17019MB
[2024-07-03 15:04:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:11:49 lr 0.000009	 wd 0.0500	time 0.3879 (0.4167)	loss 1.4292 (1.2056)	grad_norm 0.3381 (0.3476)	loss_scale 4096.0000 (3024.6991)	mem 17019MB
[2024-07-03 15:04:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:11:12 lr 0.000009	 wd 0.0500	time 0.9458 (0.4198)	loss 0.9549 (1.2105)	grad_norm 0.3516 (0.3475)	loss_scale 4096.0000 (3143.6004)	mem 17019MB
[2024-07-03 15:05:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:10:53 lr 0.000010	 wd 0.0500	time 0.5085 (0.4354)	loss 0.8970 (1.2075)	grad_norm 0.6211 (0.3478)	loss_scale 4096.0000 (3238.7453)	mem 17019MB
[2024-07-03 15:07:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:11:13 lr 0.000010	 wd 0.0500	time 0.7715 (0.4802)	loss 1.4108 (1.2049)	grad_norm 0.3550 (0.3477)	loss_scale 4096.0000 (3316.6067)	mem 17019MB
[2024-07-03 15:08:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:10:44 lr 0.000010	 wd 0.0500	time 0.3441 (0.4948)	loss 0.9181 (1.2051)	grad_norm 0.3913 (0.3480)	loss_scale 4096.0000 (3381.5021)	mem 17019MB
[2024-07-03 15:09:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:09:45 lr 0.000010	 wd 0.0500	time 0.3526 (0.4867)	loss 1.4509 (1.2059)	grad_norm 0.3496 (0.3474)	loss_scale 4096.0000 (3436.4212)	mem 17019MB
[2024-07-03 15:09:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:08:49 lr 0.000010	 wd 0.0500	time 0.3944 (0.4803)	loss 1.2638 (1.2075)	grad_norm 0.3293 (0.3471)	loss_scale 4096.0000 (3483.5004)	mem 17019MB
[2024-07-03 15:10:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:07:55 lr 0.000010	 wd 0.0500	time 0.3366 (0.4741)	loss 1.3836 (1.2079)	grad_norm 0.3315 (0.3468)	loss_scale 4096.0000 (3524.3065)	mem 17019MB
[2024-07-03 15:11:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:07:03 lr 0.000011	 wd 0.0500	time 0.3722 (0.4692)	loss 1.0283 (1.2081)	grad_norm 0.3260 (0.3477)	loss_scale 4096.0000 (3560.0150)	mem 17019MB
[2024-07-03 15:11:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:06:13 lr 0.000011	 wd 0.0500	time 0.3435 (0.4656)	loss 1.3150 (1.2084)	grad_norm 0.3435 (0.3499)	loss_scale 4096.0000 (3591.5250)	mem 17019MB
[2024-07-03 15:12:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:05:24 lr 0.000011	 wd 0.0500	time 0.3715 (0.4616)	loss 1.1099 (1.2090)	grad_norm 0.3616 (0.3515)	loss_scale 4096.0000 (3619.5358)	mem 17019MB
[2024-07-03 15:13:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:04:39 lr 0.000011	 wd 0.0500	time 0.5272 (0.4639)	loss 1.3705 (1.2086)	grad_norm 0.3620 (0.3511)	loss_scale 4096.0000 (3644.5997)	mem 17019MB
[2024-07-03 15:14:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:03:55 lr 0.000011	 wd 0.0500	time 0.4792 (0.4683)	loss 1.1425 (1.2085)	grad_norm 0.3320 (0.3508)	loss_scale 4096.0000 (3667.1584)	mem 17019MB
[2024-07-03 15:15:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:17 lr 0.000011	 wd 0.0500	time 0.7798 (0.4915)	loss 1.0479 (1.2069)	grad_norm 0.3307 (0.3503)	loss_scale 4096.0000 (3687.5697)	mem 17019MB
[2024-07-03 15:16:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:30 lr 0.000012	 wd 0.0500	time 0.3739 (0.4978)	loss 1.4211 (1.2065)	grad_norm 0.3413 (0.3502)	loss_scale 4096.0000 (3706.1263)	mem 17019MB
[2024-07-03 15:17:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:39 lr 0.000012	 wd 0.0500	time 0.3871 (0.4933)	loss 1.2525 (1.2065)	grad_norm 0.3284 (0.3503)	loss_scale 4096.0000 (3723.0700)	mem 17019MB
[2024-07-03 15:18:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:49 lr 0.000012	 wd 0.0500	time 0.3947 (0.4894)	loss 0.8065 (1.2071)	grad_norm 0.3333 (0.3498)	loss_scale 4096.0000 (3738.6022)	mem 17019MB
[2024-07-03 15:18:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.3450 (0.4847)	loss 1.2337 (1.2072)	grad_norm 0.3177 (0.3495)	loss_scale 4096.0000 (3752.8924)	mem 17019MB
[2024-07-03 15:18:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 2 training takes 0:20:17
[2024-07-03 15:19:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 38.783 (38.783)	Loss 0.5010 (0.5010)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 15:19:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.336 Acc@5 97.452
[2024-07-03 15:19:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-03 15:19:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.34%
[2024-07-03 15:19:53 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 15:19:55 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 15:20:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][0/2502]	eta 11:22:58 lr 0.000012	 wd 0.0500	time 16.3784 (16.3784)	loss 0.7599 (0.7599)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:20:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:21:35 lr 0.000012	 wd 0.0500	time 0.3480 (0.5395)	loss 1.2709 (1.2409)	grad_norm 0.3415 (0.3433)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:21:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:18:04 lr 0.000012	 wd 0.0500	time 0.3921 (0.4712)	loss 1.5497 (1.2140)	grad_norm 0.3461 (0.3416)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:22:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:16:14 lr 0.000012	 wd 0.0500	time 0.4536 (0.4427)	loss 1.4798 (1.2102)	grad_norm 0.3248 (0.3428)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:22:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:14:59 lr 0.000013	 wd 0.0500	time 0.3920 (0.4278)	loss 1.4582 (1.2020)	grad_norm 0.3398 (0.3489)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:23:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:14:00 lr 0.000013	 wd 0.0500	time 0.3782 (0.4198)	loss 1.0052 (1.2023)	grad_norm 0.3315 (0.3468)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:24:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:13:07 lr 0.000013	 wd 0.0500	time 0.3937 (0.4141)	loss 1.0695 (1.1967)	grad_norm 0.3475 (0.3499)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:24:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:12:46 lr 0.000013	 wd 0.0500	time 0.4529 (0.4255)	loss 1.5595 (1.1939)	grad_norm 0.3390 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:25:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:12:28 lr 0.000013	 wd 0.0500	time 0.5084 (0.4397)	loss 0.8020 (1.1895)	grad_norm 0.3427 (0.3514)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:27:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:12:42 lr 0.000013	 wd 0.0500	time 0.5749 (0.4759)	loss 1.5830 (1.1916)	grad_norm 0.3510 (0.3538)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:28:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:12:11 lr 0.000014	 wd 0.0500	time 0.8378 (0.4872)	loss 1.3271 (1.1923)	grad_norm 0.3100 (0.3527)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:29:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:11:35 lr 0.000014	 wd 0.0500	time 0.4833 (0.4957)	loss 1.0095 (1.1928)	grad_norm 0.3524 (0.3521)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:30:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:11:43 lr 0.000014	 wd 0.0500	time 0.9750 (0.5407)	loss 1.1768 (1.1924)	grad_norm 0.3327 (0.3521)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:31:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:10:53 lr 0.000014	 wd 0.0500	time 0.3669 (0.5439)	loss 1.3446 (1.1918)	grad_norm 0.3228 (0.3518)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:32:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:09:47 lr 0.000014	 wd 0.0500	time 0.3769 (0.5331)	loss 1.1380 (1.1927)	grad_norm 0.3224 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:33:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:08:44 lr 0.000014	 wd 0.0500	time 0.3449 (0.5238)	loss 1.4483 (1.1909)	grad_norm 0.3350 (0.3502)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:33:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:07:45 lr 0.000015	 wd 0.0500	time 0.3494 (0.5157)	loss 0.8102 (1.1892)	grad_norm 0.4806 (0.3500)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:34:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:06:48 lr 0.000015	 wd 0.0500	time 0.3773 (0.5089)	loss 1.0058 (1.1917)	grad_norm 0.3192 (0.3496)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:35:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:05:53 lr 0.000015	 wd 0.0500	time 0.3660 (0.5034)	loss 1.1825 (1.1927)	grad_norm 0.3530 (0.3493)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:35:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:04:59 lr 0.000015	 wd 0.0500	time 0.3721 (0.4978)	loss 1.2970 (1.1918)	grad_norm 0.3313 (0.3490)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 15:36:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:04:10 lr 0.000015	 wd 0.0500	time 0.3759 (0.4993)	loss 1.3541 (1.1922)	grad_norm 0.3308 (0.3487)	loss_scale 8192.0000 (4267.9460)	mem 17019MB
[2024-07-03 15:37:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:21 lr 0.000015	 wd 0.0500	time 0.4639 (0.5008)	loss 1.0008 (1.1925)	grad_norm 0.3399 (0.3484)	loss_scale 8192.0000 (4454.7168)	mem 17019MB
[2024-07-03 15:38:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:33 lr 0.000016	 wd 0.0500	time 0.6004 (0.5070)	loss 0.8276 (1.1916)	grad_norm 0.3336 (0.3478)	loss_scale 8192.0000 (4624.5161)	mem 17019MB
[2024-07-03 15:39:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:44 lr 0.000016	 wd 0.0500	time 0.9557 (0.5150)	loss 0.9435 (1.1917)	grad_norm 0.3603 (0.3475)	loss_scale 8192.0000 (4779.5567)	mem 17019MB
[2024-07-03 15:40:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:52 lr 0.000016	 wd 0.0500	time 0.4779 (0.5185)	loss 1.4400 (1.1933)	grad_norm 0.3528 (0.3473)	loss_scale 8192.0000 (4921.6826)	mem 17019MB
[2024-07-03 15:41:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.3377 (0.5162)	loss 1.3330 (1.1931)	grad_norm 0.3369 (0.3475)	loss_scale 8192.0000 (5052.4430)	mem 17019MB
[2024-07-03 15:41:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 3 training takes 0:21:41
[2024-07-03 15:42:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 71.036 (71.036)	Loss 0.5195 (0.5195)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 15:43:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.418 Acc@5 97.560
[2024-07-03 15:43:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-03 15:43:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.42%
[2024-07-03 15:43:06 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 15:43:07 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 15:43:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][0/2502]	eta 11:38:52 lr 0.000016	 wd 0.0500	time 16.7596 (16.7596)	loss 1.2938 (1.2938)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:44:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:22:14 lr 0.000016	 wd 0.0500	time 0.3874 (0.5558)	loss 0.9696 (1.2064)	grad_norm 0.3384 (0.3413)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:44:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:18:17 lr 0.000016	 wd 0.0500	time 0.3425 (0.4767)	loss 0.8777 (1.1941)	grad_norm 0.3383 (0.3461)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:45:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:16:21 lr 0.000016	 wd 0.0500	time 0.3363 (0.4458)	loss 0.8078 (1.1902)	grad_norm 0.3407 (0.3460)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:46:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:15:10 lr 0.000017	 wd 0.0500	time 0.3715 (0.4332)	loss 1.3224 (1.1857)	grad_norm 0.3379 (0.3457)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:46:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:14:09 lr 0.000017	 wd 0.0500	time 0.3771 (0.4243)	loss 1.2748 (1.1858)	grad_norm 0.3419 (0.3466)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:47:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:13:17 lr 0.000017	 wd 0.0500	time 0.4472 (0.4193)	loss 1.1946 (1.1905)	grad_norm 0.3494 (0.3452)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:48:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:12:58 lr 0.000017	 wd 0.0500	time 0.3584 (0.4322)	loss 0.8749 (1.1910)	grad_norm 0.3427 (0.3492)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-07-03 15:49:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:12:44 lr 0.000017	 wd 0.0500	time 0.6464 (0.4493)	loss 0.8003 (1.1935)	grad_norm nan (nan)	loss_scale 4096.0000 (8181.7728)	mem 17019MB
[2024-07-03 15:50:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:12:32 lr 0.000017	 wd 0.0500	time 0.5211 (0.4699)	loss 0.8456 (1.1946)	grad_norm 0.3297 (nan)	loss_scale 4096.0000 (7728.3019)	mem 17019MB
[2024-07-03 15:51:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:12:52 lr 0.000018	 wd 0.0500	time 0.9444 (0.5140)	loss 1.6121 (1.1945)	grad_norm 0.3192 (nan)	loss_scale 4096.0000 (7365.4346)	mem 17019MB
[2024-07-03 15:52:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:12:26 lr 0.000018	 wd 0.0500	time 0.3540 (0.5322)	loss 1.4679 (1.1955)	grad_norm 0.3335 (nan)	loss_scale 4096.0000 (7068.4832)	mem 17019MB
[2024-07-03 15:53:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:11:16 lr 0.000018	 wd 0.0500	time 0.3782 (0.5198)	loss 1.2995 (1.1941)	grad_norm 0.3456 (nan)	loss_scale 4096.0000 (6820.9825)	mem 17019MB
[2024-07-03 15:54:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:10:13 lr 0.000018	 wd 0.0500	time 0.3347 (0.5101)	loss 1.0466 (1.1948)	grad_norm 0.3488 (nan)	loss_scale 4096.0000 (6611.5296)	mem 17019MB
[2024-07-03 15:54:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:09:12 lr 0.000018	 wd 0.0500	time 0.3514 (0.5013)	loss 1.4389 (1.1946)	grad_norm 0.3482 (nan)	loss_scale 4096.0000 (6431.9772)	mem 17019MB
[2024-07-03 15:55:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:08:14 lr 0.000018	 wd 0.0500	time 0.4056 (0.4939)	loss 1.2281 (1.1949)	grad_norm 0.3299 (nan)	loss_scale 4096.0000 (6276.3491)	mem 17019MB
[2024-07-03 15:56:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:07:20 lr 0.000019	 wd 0.0500	time 0.3639 (0.4885)	loss 1.4969 (1.1934)	grad_norm 0.3415 (nan)	loss_scale 4096.0000 (6140.1624)	mem 17019MB
[2024-07-03 15:56:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:06:27 lr 0.000019	 wd 0.0500	time 0.3911 (0.4832)	loss 1.1954 (1.1939)	grad_norm 0.3401 (nan)	loss_scale 4096.0000 (6019.9882)	mem 17019MB
[2024-07-03 15:57:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:05:41 lr 0.000019	 wd 0.0500	time 0.5642 (0.4862)	loss 1.5590 (1.1959)	grad_norm 0.3501 (nan)	loss_scale 4096.0000 (5913.1594)	mem 17019MB
[2024-07-03 15:58:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:04:54 lr 0.000019	 wd 0.0500	time 0.4671 (0.4884)	loss 1.4204 (1.1962)	grad_norm 0.3429 (nan)	loss_scale 2048.0000 (5729.2288)	mem 17019MB
[2024-07-03 15:59:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:04:13 lr 0.000019	 wd 0.0500	time 0.5078 (0.5040)	loss 0.8732 (1.1939)	grad_norm 0.3425 (nan)	loss_scale 2048.0000 (5545.2594)	mem 17019MB
[2024-07-03 16:00:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:03:23 lr 0.000019	 wd 0.0500	time 0.6349 (0.5073)	loss 1.0219 (1.1926)	grad_norm 0.3262 (nan)	loss_scale 2048.0000 (5378.8025)	mem 17019MB
[2024-07-03 16:01:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:33 lr 0.000020	 wd 0.0500	time 0.3969 (0.5086)	loss 0.8902 (1.1921)	grad_norm 0.3157 (nan)	loss_scale 2048.0000 (5227.4711)	mem 17019MB
[2024-07-03 16:02:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:43 lr 0.000020	 wd 0.0500	time 0.3936 (0.5134)	loss 0.8093 (1.1913)	grad_norm 0.3420 (nan)	loss_scale 2048.0000 (5089.2934)	mem 17019MB
[2024-07-03 16:03:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.4559 (0.5154)	loss 0.8146 (1.1912)	grad_norm 0.3077 (nan)	loss_scale 2048.0000 (4962.6256)	mem 17019MB
[2024-07-03 16:04:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.3439 (0.5209)	loss 1.2603 (1.1917)	grad_norm 0.3571 (nan)	loss_scale 2048.0000 (4846.0872)	mem 17019MB
[2024-07-03 16:04:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 4 training takes 0:21:51
[2024-07-03 16:05:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 59.081 (59.081)	Loss 0.5088 (0.5088)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 16:06:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.548 Acc@5 97.620
[2024-07-03 16:06:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-03 16:06:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.55%
[2024-07-03 16:06:17 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 16:06:18 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 16:06:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][0/2502]	eta 11:25:00 lr 0.000020	 wd 0.0500	time 16.4269 (16.4269)	loss 1.4041 (1.4041)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:07:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:22:23 lr 0.000020	 wd 0.0500	time 0.3933 (0.5594)	loss 1.0095 (1.2210)	grad_norm 0.3478 (0.3461)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:07:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:18:03 lr 0.000020	 wd 0.0500	time 0.3563 (0.4707)	loss 1.3980 (1.1901)	grad_norm 0.3400 (0.3476)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:08:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:16:08 lr 0.000020	 wd 0.0500	time 0.3491 (0.4400)	loss 1.0334 (1.1794)	grad_norm 0.3292 (0.3485)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:09:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:14:57 lr 0.000020	 wd 0.0500	time 0.3383 (0.4268)	loss 1.2927 (1.1880)	grad_norm 0.3720 (0.3497)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:09:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:13:54 lr 0.000020	 wd 0.0500	time 0.3357 (0.4166)	loss 1.0466 (1.1865)	grad_norm 0.3410 (0.3486)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:10:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:13:01 lr 0.000020	 wd 0.0500	time 0.3626 (0.4109)	loss 1.0704 (1.1881)	grad_norm 0.3429 (0.3493)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:11:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:12:14 lr 0.000020	 wd 0.0500	time 0.3717 (0.4076)	loss 0.9296 (1.1864)	grad_norm 0.3212 (0.3485)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:11:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:11:27 lr 0.000020	 wd 0.0500	time 0.3687 (0.4042)	loss 1.5317 (1.1853)	grad_norm 0.3694 (0.3576)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:12:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:11:22 lr 0.000020	 wd 0.0500	time 0.4456 (0.4261)	loss 1.0870 (1.1839)	grad_norm 0.3310 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:13:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:10:59 lr 0.000020	 wd 0.0500	time 0.4952 (0.4388)	loss 1.2561 (1.1820)	grad_norm 0.3278 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:14:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:10:45 lr 0.000020	 wd 0.0500	time 0.5803 (0.4607)	loss 1.1957 (1.1829)	grad_norm 0.3589 (0.3536)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:15:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:10:28 lr 0.000020	 wd 0.0500	time 1.3633 (0.4825)	loss 1.4467 (1.1813)	grad_norm 0.3477 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:17:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:10:16 lr 0.000020	 wd 0.0500	time 0.3485 (0.5126)	loss 1.0065 (1.1820)	grad_norm 0.3316 (0.3525)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:18:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:09:15 lr 0.000020	 wd 0.0500	time 0.3934 (0.5040)	loss 1.4433 (1.1856)	grad_norm 0.3649 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:18:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:08:17 lr 0.000020	 wd 0.0500	time 0.3653 (0.4969)	loss 0.8687 (1.1867)	grad_norm 0.7227 (0.3525)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:19:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:07:22 lr 0.000020	 wd 0.0500	time 0.3563 (0.4901)	loss 1.4480 (1.1892)	grad_norm 0.3405 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:20:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:06:28 lr 0.000020	 wd 0.0500	time 0.3694 (0.4845)	loss 0.9829 (1.1894)	grad_norm 0.3243 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:20:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:05:37 lr 0.000020	 wd 0.0500	time 0.3442 (0.4801)	loss 0.7801 (1.1871)	grad_norm 0.3482 (0.3529)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:21:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:04:46 lr 0.000020	 wd 0.0500	time 0.3347 (0.4754)	loss 1.1413 (1.1866)	grad_norm 0.3365 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:22:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:03:56 lr 0.000020	 wd 0.0500	time 0.3555 (0.4714)	loss 1.5747 (1.1881)	grad_norm 0.3260 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:22:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:03:11 lr 0.000020	 wd 0.0500	time 0.6967 (0.4758)	loss 1.5072 (1.1876)	grad_norm 0.3461 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:24:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:25 lr 0.000020	 wd 0.0500	time 0.5383 (0.4832)	loss 1.2365 (1.1887)	grad_norm 0.3343 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:24:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:38 lr 0.000020	 wd 0.0500	time 0.5120 (0.4857)	loss 1.1538 (1.1872)	grad_norm 0.3400 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:25:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:49 lr 0.000020	 wd 0.0500	time 0.3867 (0.4897)	loss 0.7967 (1.1865)	grad_norm 0.3638 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:26:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.3286 (0.4868)	loss 1.3189 (1.1877)	grad_norm 0.3379 (0.3527)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:26:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 5 training takes 0:20:24
[2024-07-03 16:28:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 104.858 (104.858)	Loss 0.5093 (0.5093)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 16:29:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.652 Acc@5 97.662
[2024-07-03 16:29:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-03 16:29:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.65%
[2024-07-03 16:29:07 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 16:29:08 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 16:29:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][0/2502]	eta 15:00:53 lr 0.000020	 wd 0.0500	time 21.6043 (21.6043)	loss 1.2356 (1.2356)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:30:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:24:09 lr 0.000020	 wd 0.0500	time 0.4181 (0.6036)	loss 0.9014 (1.2082)	grad_norm 0.3456 (0.3423)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:30:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:19:10 lr 0.000020	 wd 0.0500	time 0.3836 (0.4998)	loss 0.8787 (1.1976)	grad_norm 0.3104 (0.3414)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:31:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:16:50 lr 0.000020	 wd 0.0500	time 0.3642 (0.4590)	loss 0.9769 (1.1894)	grad_norm 0.3393 (0.3408)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:32:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:15:31 lr 0.000020	 wd 0.0500	time 0.3348 (0.4429)	loss 0.8022 (1.1918)	grad_norm 0.3516 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:32:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:14:23 lr 0.000020	 wd 0.0500	time 0.3613 (0.4313)	loss 1.2278 (1.1921)	grad_norm 0.3287 (0.3436)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:33:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:13:27 lr 0.000020	 wd 0.0500	time 0.3678 (0.4244)	loss 0.9062 (1.1834)	grad_norm 0.3345 (0.3437)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:34:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:12:37 lr 0.000020	 wd 0.0500	time 0.3652 (0.4205)	loss 1.4965 (1.1860)	grad_norm 0.3097 (0.3440)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:34:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:11:47 lr 0.000020	 wd 0.0500	time 0.3578 (0.4157)	loss 0.7170 (1.1902)	grad_norm 0.3208 (0.3439)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:35:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:11:14 lr 0.000020	 wd 0.0500	time 1.4105 (0.4209)	loss 1.5251 (1.1866)	grad_norm 0.3238 (0.3438)	loss_scale 4096.0000 (2243.4806)	mem 17019MB
[2024-07-03 16:36:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:11:13 lr 0.000020	 wd 0.0500	time 0.4090 (0.4485)	loss 1.0483 (1.1863)	grad_norm 0.3266 (0.3447)	loss_scale 4096.0000 (2428.5475)	mem 17019MB
[2024-07-03 16:37:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:11:15 lr 0.000020	 wd 0.0500	time 0.4741 (0.4818)	loss 0.8687 (1.1829)	grad_norm 0.3374 (0.3449)	loss_scale 4096.0000 (2579.9964)	mem 17019MB
[2024-07-03 16:38:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:10:37 lr 0.000020	 wd 0.0500	time 0.6372 (0.4899)	loss 1.2600 (1.1798)	grad_norm 0.3527 (0.3449)	loss_scale 4096.0000 (2706.2248)	mem 17019MB
[2024-07-03 16:39:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:09:54 lr 0.000020	 wd 0.0500	time 0.4301 (0.4945)	loss 1.1180 (1.1784)	grad_norm 0.3288 (0.3446)	loss_scale 4096.0000 (2813.0484)	mem 17019MB
[2024-07-03 16:40:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:09:13 lr 0.000020	 wd 0.0500	time 0.5371 (0.5026)	loss 0.9650 (1.1786)	grad_norm 0.4192 (0.3446)	loss_scale 4096.0000 (2904.6224)	mem 17019MB
[2024-07-03 16:41:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:08:28 lr 0.000020	 wd 0.0500	time 0.8295 (0.5075)	loss 0.7549 (1.1795)	grad_norm 0.3316 (0.3442)	loss_scale 4096.0000 (2983.9947)	mem 17019MB
[2024-07-03 16:42:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:07:43 lr 0.000020	 wd 0.0500	time 0.5705 (0.5136)	loss 1.4683 (1.1769)	grad_norm 0.3158 (0.3443)	loss_scale 4096.0000 (3053.4516)	mem 17019MB
[2024-07-03 16:44:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:07:07 lr 0.000020	 wd 0.0500	time 0.8202 (0.5336)	loss 1.3875 (1.1762)	grad_norm 0.3331 (0.3443)	loss_scale 4096.0000 (3114.7419)	mem 17019MB
[2024-07-03 16:45:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:06:22 lr 0.000020	 wd 0.0500	time 0.3773 (0.5444)	loss 1.2295 (1.1766)	grad_norm 0.3327 (0.3451)	loss_scale 4096.0000 (3169.2260)	mem 17019MB
[2024-07-03 16:46:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:05:22 lr 0.000020	 wd 0.0500	time 0.4621 (0.5364)	loss 1.3613 (1.1778)	grad_norm 0.3336 (0.3455)	loss_scale 4096.0000 (3217.9779)	mem 17019MB
[2024-07-03 16:46:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:04:25 lr 0.000020	 wd 0.0500	time 0.3928 (0.5297)	loss 0.9871 (1.1786)	grad_norm 0.3373 (0.3457)	loss_scale 4096.0000 (3261.8571)	mem 17019MB
[2024-07-03 16:47:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:03:30 lr 0.000020	 wd 0.0500	time 0.3788 (0.5233)	loss 1.0181 (1.1787)	grad_norm 0.3491 (0.3466)	loss_scale 4096.0000 (3301.5593)	mem 17019MB
[2024-07-03 16:48:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:36 lr 0.000020	 wd 0.0500	time 0.3695 (0.5174)	loss 0.8279 (1.1780)	grad_norm 0.3715 (nan)	loss_scale 2048.0000 (3281.8246)	mem 17019MB
[2024-07-03 16:48:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:44 lr 0.000020	 wd 0.0500	time 0.4306 (0.5169)	loss 1.0474 (1.1805)	grad_norm 0.4148 (nan)	loss_scale 2048.0000 (3228.2034)	mem 17019MB
[2024-07-03 16:49:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.6085 (0.5172)	loss 1.3988 (1.1797)	grad_norm 0.3494 (nan)	loss_scale 2048.0000 (3179.0487)	mem 17019MB
[2024-07-03 16:50:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.3357 (0.5181)	loss 1.3543 (1.1793)	grad_norm 0.3385 (nan)	loss_scale 2048.0000 (3133.8249)	mem 17019MB
[2024-07-03 16:50:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 6 training takes 0:21:50
[2024-07-03 16:52:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 108.725 (108.725)	Loss 0.5190 (0.5190)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 16:53:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.612 Acc@5 97.676
[2024-07-03 16:53:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-03 16:53:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.65%
[2024-07-03 16:53:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][0/2502]	eta 1 day, 4:00:54 lr 0.000020	 wd 0.0500	time 40.3095 (40.3095)	loss 0.9586 (0.9586)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:54:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:31:15 lr 0.000020	 wd 0.0500	time 0.3515 (0.7810)	loss 1.2181 (1.1634)	grad_norm 0.3390 (0.3569)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:55:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:22:46 lr 0.000020	 wd 0.0500	time 0.3757 (0.5938)	loss 1.1382 (1.1843)	grad_norm 0.3424 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:55:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:19:08 lr 0.000020	 wd 0.0500	time 0.3353 (0.5218)	loss 1.3042 (1.1793)	grad_norm 0.3545 (0.3515)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:56:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:17:03 lr 0.000020	 wd 0.0500	time 0.3685 (0.4870)	loss 0.8432 (1.1844)	grad_norm 0.3462 (0.3516)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:57:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:15:39 lr 0.000020	 wd 0.0500	time 0.3414 (0.4694)	loss 1.3614 (1.1811)	grad_norm 0.3307 (0.3503)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:57:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:14:25 lr 0.000020	 wd 0.0500	time 0.3464 (0.4551)	loss 1.3210 (1.1855)	grad_norm 0.3452 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:58:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:13:23 lr 0.000020	 wd 0.0500	time 0.4249 (0.4460)	loss 0.9679 (1.1800)	grad_norm 0.3290 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:59:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:12:26 lr 0.000020	 wd 0.0500	time 0.3509 (0.4384)	loss 0.8873 (1.1773)	grad_norm 0.3510 (0.3521)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 16:59:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:11:32 lr 0.000020	 wd 0.0500	time 0.3966 (0.4323)	loss 1.4435 (1.1790)	grad_norm 0.3391 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:00:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:11:06 lr 0.000020	 wd 0.0500	time 0.3886 (0.4440)	loss 0.8157 (1.1790)	grad_norm 0.3512 (0.3536)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:01:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:10:37 lr 0.000020	 wd 0.0500	time 0.6030 (0.4545)	loss 1.3757 (1.1781)	grad_norm 0.3640 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:03:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:11:01 lr 0.000020	 wd 0.0500	time 0.4844 (0.5078)	loss 1.4188 (1.1748)	grad_norm 0.3420 (0.3546)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:03:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:09:59 lr 0.000020	 wd 0.0500	time 0.3725 (0.4991)	loss 1.3772 (1.1759)	grad_norm 0.3189 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:04:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:09:01 lr 0.000019	 wd 0.0500	time 0.4108 (0.4912)	loss 1.2889 (1.1755)	grad_norm 0.3382 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:05:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:08:05 lr 0.000019	 wd 0.0500	time 0.3833 (0.4847)	loss 0.8416 (1.1761)	grad_norm 0.3298 (0.3532)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:05:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:07:12 lr 0.000019	 wd 0.0500	time 0.3900 (0.4791)	loss 1.1274 (1.1767)	grad_norm 0.3436 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:06:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:06:20 lr 0.000019	 wd 0.0500	time 0.4074 (0.4746)	loss 1.3520 (1.1768)	grad_norm 0.3210 (0.3528)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:07:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:05:30 lr 0.000019	 wd 0.0500	time 0.3792 (0.4704)	loss 0.7731 (1.1791)	grad_norm 0.3304 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:07:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:04:40 lr 0.000019	 wd 0.0500	time 0.3581 (0.4665)	loss 1.1207 (1.1795)	grad_norm 0.5701 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:08:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:03:57 lr 0.000019	 wd 0.0500	time 0.5499 (0.4730)	loss 1.1495 (1.1800)	grad_norm 0.3375 (0.3526)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:09:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:03:12 lr 0.000019	 wd 0.0500	time 0.6940 (0.4780)	loss 1.4660 (1.1826)	grad_norm 0.3494 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:10:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:25 lr 0.000019	 wd 0.0500	time 0.4453 (0.4815)	loss 1.2400 (1.1807)	grad_norm 0.3402 (0.3528)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:11:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:38 lr 0.000019	 wd 0.0500	time 0.5930 (0.4870)	loss 1.4096 (1.1798)	grad_norm 0.3463 (0.3525)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:12:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:49 lr 0.000019	 wd 0.0500	time 0.4238 (0.4890)	loss 1.2606 (1.1802)	grad_norm 0.3335 (0.3531)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:13:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.3376 (0.4888)	loss 1.5921 (1.1803)	grad_norm 0.3202 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:13:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 7 training takes 0:20:34
[2024-07-03 17:15:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 90.778 (90.778)	Loss 0.5029 (0.5029)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 17:15:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.680 Acc@5 97.706
[2024-07-03 17:15:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-03 17:15:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-03 17:15:39 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 17:15:40 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 17:16:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][0/2502]	eta 1 day, 10:46:53 lr 0.000019	 wd 0.0500	time 50.0454 (50.0454)	loss 1.2673 (1.2673)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:17:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:35:08 lr 0.000019	 wd 0.0500	time 0.3441 (0.8778)	loss 0.9868 (1.2213)	grad_norm 0.3214 (0.3443)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:17:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:24:20 lr 0.000019	 wd 0.0500	time 0.4057 (0.6343)	loss 0.8340 (1.2066)	grad_norm 0.3522 (0.3518)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:18:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:20:20 lr 0.000019	 wd 0.0500	time 0.3555 (0.5544)	loss 1.4246 (1.1979)	grad_norm 0.3534 (0.3575)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:19:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:17:53 lr 0.000019	 wd 0.0500	time 0.3758 (0.5107)	loss 0.8475 (1.1966)	grad_norm 0.3458 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:19:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:16:17 lr 0.000019	 wd 0.0500	time 0.4869 (0.4882)	loss 0.8196 (1.1856)	grad_norm 0.3557 (0.3629)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:20:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:14:55 lr 0.000019	 wd 0.0500	time 0.3501 (0.4707)	loss 0.9188 (1.1827)	grad_norm 0.3764 (0.3654)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:21:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:13:45 lr 0.000019	 wd 0.0500	time 0.3793 (0.4581)	loss 1.3572 (1.1834)	grad_norm 0.3630 (0.3622)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:21:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:12:47 lr 0.000019	 wd 0.0500	time 0.3925 (0.4510)	loss 1.4740 (1.1851)	grad_norm 0.3404 (0.3601)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:22:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:11:49 lr 0.000019	 wd 0.0500	time 0.3819 (0.4431)	loss 0.9559 (1.1801)	grad_norm 0.3925 (0.3583)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:23:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:11:02 lr 0.000019	 wd 0.0500	time 0.8545 (0.4412)	loss 1.3482 (1.1834)	grad_norm 0.3383 (0.3574)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:23:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:10:32 lr 0.000019	 wd 0.0500	time 0.5836 (0.4512)	loss 1.2229 (1.1816)	grad_norm 0.3307 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 17:25:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:10:28 lr 0.000019	 wd 0.0500	time 0.8103 (0.4830)	loss 1.1318 (1.1811)	grad_norm 0.3470 (0.3551)	loss_scale 4096.0000 (2157.1357)	mem 17019MB
[2024-07-03 17:26:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:10:10 lr 0.000019	 wd 0.0500	time 0.3700 (0.5080)	loss 0.9151 (1.1804)	grad_norm 0.3475 (0.3547)	loss_scale 4096.0000 (2306.1645)	mem 17019MB
[2024-07-03 17:27:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:09:10 lr 0.000019	 wd 0.0500	time 0.3707 (0.4995)	loss 1.2832 (1.1813)	grad_norm 0.3432 (0.3546)	loss_scale 4096.0000 (2433.9186)	mem 17019MB
[2024-07-03 17:28:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:08:13 lr 0.000019	 wd 0.0500	time 0.3649 (0.4930)	loss 1.1794 (1.1815)	grad_norm 0.3572 (0.3540)	loss_scale 4096.0000 (2544.6502)	mem 17019MB
[2024-07-03 17:28:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:07:18 lr 0.000019	 wd 0.0500	time 0.3534 (0.4864)	loss 1.3853 (1.1842)	grad_norm 0.3618 (0.3534)	loss_scale 4096.0000 (2641.5490)	mem 17019MB
[2024-07-03 17:29:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:06:25 lr 0.000019	 wd 0.0500	time 0.3515 (0.4810)	loss 0.7745 (1.1866)	grad_norm 0.3551 (0.3585)	loss_scale 4096.0000 (2727.0547)	mem 17019MB
[2024-07-03 17:29:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:05:34 lr 0.000019	 wd 0.0500	time 0.3399 (0.4768)	loss 0.8001 (1.1851)	grad_norm 0.3510 (0.3577)	loss_scale 4096.0000 (2803.0650)	mem 17019MB
[2024-07-03 17:30:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:04:44 lr 0.000019	 wd 0.0500	time 0.3468 (0.4723)	loss 1.1892 (1.1869)	grad_norm 0.3339 (0.3570)	loss_scale 4096.0000 (2871.0784)	mem 17019MB
[2024-07-03 17:31:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:03:57 lr 0.000019	 wd 0.0500	time 0.4465 (0.4733)	loss 0.8214 (1.1856)	grad_norm 0.3380 (0.3564)	loss_scale 4096.0000 (2932.2939)	mem 17019MB
[2024-07-03 17:32:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:03:11 lr 0.000019	 wd 0.0500	time 0.5295 (0.4752)	loss 0.8738 (1.1861)	grad_norm 0.9801 (0.3607)	loss_scale 4096.0000 (2987.6821)	mem 17019MB
[2024-07-03 17:33:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:25 lr 0.000019	 wd 0.0500	time 0.4023 (0.4805)	loss 1.1231 (1.1844)	grad_norm 0.3583 (0.3599)	loss_scale 4096.0000 (3038.0373)	mem 17019MB
[2024-07-03 17:34:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:37 lr 0.000019	 wd 0.0500	time 0.4512 (0.4829)	loss 1.3530 (1.1830)	grad_norm 0.3248 (0.3591)	loss_scale 4096.0000 (3084.0156)	mem 17019MB
[2024-07-03 17:35:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:50 lr 0.000019	 wd 0.0500	time 0.7595 (0.4952)	loss 1.1828 (1.1828)	grad_norm 0.3662 (0.3585)	loss_scale 4096.0000 (3126.1641)	mem 17019MB
[2024-07-03 17:36:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.3531 (0.4926)	loss 0.9144 (1.1826)	grad_norm 0.3291 (0.3582)	loss_scale 4096.0000 (3164.9420)	mem 17019MB
[2024-07-03 17:36:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 8 training takes 0:20:50
[2024-07-03 17:37:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 81.357 (81.357)	Loss 0.5034 (0.5034)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-07-03 17:38:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.732 Acc@5 97.724
[2024-07-03 17:38:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-03 17:38:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.73%
[2024-07-03 17:38:12 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 17:38:13 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 17:38:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][0/2502]	eta 12:09:01 lr 0.000019	 wd 0.0500	time 17.4828 (17.4828)	loss 1.3458 (1.3458)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:39:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:22:24 lr 0.000019	 wd 0.0500	time 0.4152 (0.5597)	loss 1.2797 (1.1603)	grad_norm 0.3399 (0.3578)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:39:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:18:24 lr 0.000019	 wd 0.0500	time 0.3450 (0.4797)	loss 0.9097 (1.1835)	grad_norm 0.3362 (0.3561)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:40:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:16:20 lr 0.000019	 wd 0.0500	time 0.3798 (0.4454)	loss 1.5075 (1.1924)	grad_norm 0.4004 (0.3568)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:41:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:15:07 lr 0.000019	 wd 0.0500	time 0.3576 (0.4317)	loss 0.8493 (1.1753)	grad_norm 0.3360 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:41:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:14:07 lr 0.000019	 wd 0.0500	time 0.3568 (0.4232)	loss 0.7582 (1.1702)	grad_norm 0.3472 (0.3530)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:42:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:13:15 lr 0.000019	 wd 0.0500	time 0.3936 (0.4184)	loss 0.8543 (1.1745)	grad_norm 0.3324 (0.3525)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 17:43:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:12:29 lr 0.000019	 wd 0.0500	time 0.4213 (0.4160)	loss 1.3797 (1.1778)	grad_norm 0.3336 (nan)	loss_scale 2048.0000 (3809.6890)	mem 17019MB
[2024-07-03 17:43:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:11:42 lr 0.000019	 wd 0.0500	time 0.3891 (0.4127)	loss 1.3818 (1.1790)	grad_norm 0.3245 (nan)	loss_scale 2048.0000 (3589.7528)	mem 17019MB
[2024-07-03 17:44:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:10:58 lr 0.000019	 wd 0.0500	time 0.6569 (0.4109)	loss 1.3674 (1.1786)	grad_norm 0.4037 (nan)	loss_scale 2048.0000 (3418.6371)	mem 17019MB
[2024-07-03 17:45:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:10:45 lr 0.000019	 wd 0.0500	time 0.5305 (0.4298)	loss 1.1965 (1.1795)	grad_norm 0.3281 (nan)	loss_scale 2048.0000 (3281.7103)	mem 17019MB
[2024-07-03 17:46:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:10:56 lr 0.000018	 wd 0.0500	time 0.9698 (0.4682)	loss 1.3382 (1.1754)	grad_norm 0.3525 (nan)	loss_scale 2048.0000 (3169.6567)	mem 17019MB
[2024-07-03 17:48:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:10:37 lr 0.000018	 wd 0.0500	time 0.3824 (0.4894)	loss 0.8471 (1.1743)	grad_norm 0.3372 (nan)	loss_scale 2048.0000 (3076.2631)	mem 17019MB
[2024-07-03 17:48:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:09:38 lr 0.000018	 wd 0.0500	time 0.3359 (0.4814)	loss 1.2936 (1.1752)	grad_norm 0.3347 (nan)	loss_scale 2048.0000 (2997.2267)	mem 17019MB
[2024-07-03 17:49:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:08:43 lr 0.000018	 wd 0.0500	time 0.3790 (0.4752)	loss 1.0809 (1.1744)	grad_norm 0.3344 (nan)	loss_scale 2048.0000 (2929.4732)	mem 17019MB
[2024-07-03 17:49:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:07:50 lr 0.000018	 wd 0.0500	time 0.4282 (0.4694)	loss 0.8718 (1.1776)	grad_norm 0.3779 (nan)	loss_scale 2048.0000 (2870.7475)	mem 17019MB
[2024-07-03 17:50:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:06:58 lr 0.000018	 wd 0.0500	time 0.3439 (0.4643)	loss 0.9285 (1.1759)	grad_norm 0.3338 (nan)	loss_scale 2048.0000 (2819.3579)	mem 17019MB
[2024-07-03 17:51:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:06:09 lr 0.000018	 wd 0.0500	time 0.3616 (0.4608)	loss 1.4621 (1.1764)	grad_norm 0.3474 (nan)	loss_scale 2048.0000 (2774.0106)	mem 17019MB
[2024-07-03 17:51:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:05:20 lr 0.000018	 wd 0.0500	time 0.4018 (0.4572)	loss 1.2608 (1.1751)	grad_norm 0.3431 (nan)	loss_scale 2048.0000 (2733.6991)	mem 17019MB
[2024-07-03 17:52:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:04:34 lr 0.000018	 wd 0.0500	time 0.4217 (0.4568)	loss 1.4221 (1.1744)	grad_norm 0.3491 (nan)	loss_scale 2048.0000 (2697.6286)	mem 17019MB
[2024-07-03 17:54:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:03:58 lr 0.000018	 wd 0.0500	time 0.5055 (0.4742)	loss 1.4054 (1.1749)	grad_norm 0.3200 (nan)	loss_scale 2048.0000 (2665.1634)	mem 17019MB
[2024-07-03 17:55:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:03:14 lr 0.000018	 wd 0.0500	time 0.4943 (0.4849)	loss 1.4126 (1.1764)	grad_norm 0.3744 (nan)	loss_scale 2048.0000 (2635.7887)	mem 17019MB
[2024-07-03 17:56:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:28 lr 0.000018	 wd 0.0500	time 0.4342 (0.4911)	loss 1.1626 (1.1775)	grad_norm 0.3436 (nan)	loss_scale 2048.0000 (2609.0831)	mem 17019MB
[2024-07-03 17:57:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:39 lr 0.000018	 wd 0.0500	time 0.5418 (0.4922)	loss 0.7975 (1.1774)	grad_norm 0.3420 (nan)	loss_scale 2048.0000 (2584.6988)	mem 17019MB
[2024-07-03 17:58:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:50 lr 0.000018	 wd 0.0500	time 0.3816 (0.4963)	loss 1.0880 (1.1777)	grad_norm 0.3505 (nan)	loss_scale 2048.0000 (2562.3457)	mem 17019MB
[2024-07-03 17:58:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.3649 (0.4936)	loss 1.2681 (1.1772)	grad_norm 1.2895 (nan)	loss_scale 2048.0000 (2541.7801)	mem 17019MB
[2024-07-03 17:58:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 9 training takes 0:20:42
[2024-07-03 18:00:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 91.593 (91.593)	Loss 0.5127 (0.5127)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 18:00:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.758 Acc@5 97.732
[2024-07-03 18:00:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-03 18:00:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.76%
[2024-07-03 18:00:54 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 18:00:55 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 18:01:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][0/2502]	eta 22:29:58 lr 0.000018	 wd 0.0500	time 32.3734 (32.3734)	loss 1.6358 (1.6358)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:02:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:27:57 lr 0.000018	 wd 0.0500	time 0.3684 (0.6986)	loss 0.9044 (1.1878)	grad_norm 0.3732 (0.3435)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:02:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:21:11 lr 0.000018	 wd 0.0500	time 0.3888 (0.5522)	loss 1.4425 (1.1824)	grad_norm 0.3261 (0.3750)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:03:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:18:16 lr 0.000018	 wd 0.0500	time 0.3438 (0.4978)	loss 1.3335 (1.1775)	grad_norm 0.3620 (0.3732)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:04:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:16:32 lr 0.000018	 wd 0.0500	time 0.3682 (0.4723)	loss 0.9140 (1.1811)	grad_norm 0.3508 (0.3695)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:04:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:15:13 lr 0.000018	 wd 0.0500	time 0.4308 (0.4564)	loss 0.7116 (1.1766)	grad_norm 0.3363 (0.3661)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:05:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:14:05 lr 0.000018	 wd 0.0500	time 0.4161 (0.4444)	loss 1.1635 (1.1847)	grad_norm 0.3302 (0.3652)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:06:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:13:09 lr 0.000018	 wd 0.0500	time 0.3832 (0.4382)	loss 1.3104 (1.1827)	grad_norm 0.3153 (0.3644)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:06:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:12:15 lr 0.000018	 wd 0.0500	time 0.3966 (0.4319)	loss 1.2781 (1.1853)	grad_norm 0.3513 (0.3655)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:07:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:11:24 lr 0.000018	 wd 0.0500	time 0.3951 (0.4271)	loss 1.4611 (1.1828)	grad_norm 0.3305 (0.3633)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:08:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:11:08 lr 0.000018	 wd 0.0500	time 0.5893 (0.4451)	loss 1.2857 (1.1838)	grad_norm 0.3334 (0.3615)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:09:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:11:16 lr 0.000018	 wd 0.0500	time 0.9890 (0.4825)	loss 1.0130 (1.1896)	grad_norm 0.3626 (0.3606)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:11:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:11:09 lr 0.000018	 wd 0.0500	time 0.3527 (0.5139)	loss 1.4041 (1.1871)	grad_norm 0.3501 (0.3609)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:11:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:10:05 lr 0.000018	 wd 0.0500	time 0.4114 (0.5040)	loss 1.3799 (1.1846)	grad_norm 0.4509 (0.3601)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:12:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:09:07 lr 0.000018	 wd 0.0500	time 0.4103 (0.4972)	loss 1.3440 (1.1824)	grad_norm 0.3347 (0.3593)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:13:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:08:10 lr 0.000018	 wd 0.0500	time 0.3476 (0.4897)	loss 1.1770 (1.1799)	grad_norm 0.3524 (0.3627)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:13:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:07:16 lr 0.000018	 wd 0.0500	time 0.3795 (0.4837)	loss 1.1125 (1.1786)	grad_norm 0.3237 (0.3620)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:14:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:06:23 lr 0.000018	 wd 0.0500	time 0.3686 (0.4787)	loss 1.2434 (1.1806)	grad_norm 0.3538 (0.3616)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:15:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:05:32 lr 0.000018	 wd 0.0500	time 0.3344 (0.4739)	loss 1.1414 (1.1796)	grad_norm 0.3610 (0.3607)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:16:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:04:46 lr 0.000018	 wd 0.0500	time 0.4043 (0.4764)	loss 1.0904 (1.1786)	grad_norm 0.3406 (0.3598)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:16:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:04:00 lr 0.000017	 wd 0.0500	time 0.5852 (0.4785)	loss 1.3423 (1.1820)	grad_norm 0.3429 (0.3591)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:18:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:03:18 lr 0.000017	 wd 0.0500	time 0.6137 (0.4926)	loss 1.2227 (1.1834)	grad_norm 0.3423 (0.3585)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:19:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:29 lr 0.000017	 wd 0.0500	time 0.6002 (0.4958)	loss 1.3264 (1.1844)	grad_norm 0.3295 (0.3580)	loss_scale 4096.0000 (2141.0486)	mem 17019MB
[2024-07-03 18:20:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:40 lr 0.000017	 wd 0.0500	time 0.4940 (0.4982)	loss 1.2913 (1.1823)	grad_norm 0.3462 (0.3576)	loss_scale 4096.0000 (2226.0096)	mem 17019MB
[2024-07-03 18:21:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:51 lr 0.000017	 wd 0.0500	time 0.3820 (0.5024)	loss 1.3751 (1.1812)	grad_norm 0.3287 (0.3576)	loss_scale 4096.0000 (2303.8934)	mem 17019MB
[2024-07-03 18:21:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.3801 (0.4985)	loss 0.9004 (1.1807)	grad_norm 0.3347 (0.3587)	loss_scale 4096.0000 (2375.5490)	mem 17019MB
[2024-07-03 18:21:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 10 training takes 0:20:54
[2024-07-03 18:21:49 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_10.pth saving......
[2024-07-03 18:21:51 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_10.pth saved !!!
[2024-07-03 18:23:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 108.325 (108.325)	Loss 0.5054 (0.5054)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-07-03 18:24:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.806 Acc@5 97.754
[2024-07-03 18:24:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-03 18:24:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.81%
[2024-07-03 18:24:17 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 18:24:19 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 18:24:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][0/2502]	eta 12:59:19 lr 0.000017	 wd 0.0500	time 18.6890 (18.6890)	loss 0.9285 (0.9285)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:25:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:22:49 lr 0.000017	 wd 0.0500	time 0.4080 (0.5702)	loss 1.3920 (1.1647)	grad_norm 0.3319 (0.3478)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:25:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:18:33 lr 0.000017	 wd 0.0500	time 0.3661 (0.4835)	loss 1.3337 (1.1855)	grad_norm 0.3464 (0.3500)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:26:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:16:28 lr 0.000017	 wd 0.0500	time 0.3769 (0.4488)	loss 0.8703 (1.1758)	grad_norm 0.3328 (0.3488)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:27:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:15:14 lr 0.000017	 wd 0.0500	time 0.3773 (0.4353)	loss 1.4100 (1.1741)	grad_norm 0.3503 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:27:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:14:13 lr 0.000017	 wd 0.0500	time 0.3595 (0.4262)	loss 0.7337 (1.1736)	grad_norm 0.3279 (0.3475)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:28:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:13:17 lr 0.000017	 wd 0.0500	time 0.4016 (0.4193)	loss 1.4656 (1.1742)	grad_norm 0.3342 (0.3583)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:29:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:12:30 lr 0.000017	 wd 0.0500	time 0.3879 (0.4164)	loss 1.0138 (1.1698)	grad_norm 0.4144 (0.3570)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:29:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:11:41 lr 0.000017	 wd 0.0500	time 0.4048 (0.4122)	loss 1.4856 (1.1678)	grad_norm 0.3430 (0.3556)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:30:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:10:57 lr 0.000017	 wd 0.0500	time 0.5675 (0.4104)	loss 1.0143 (1.1707)	grad_norm 0.3701 (0.3561)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:31:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:10:57 lr 0.000017	 wd 0.0500	time 0.5666 (0.4374)	loss 1.1618 (1.1708)	grad_norm 0.3434 (0.3571)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:32:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:10:54 lr 0.000017	 wd 0.0500	time 0.5506 (0.4667)	loss 0.9142 (1.1693)	grad_norm 0.3714 (0.3563)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:33:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:10:15 lr 0.000017	 wd 0.0500	time 0.4349 (0.4728)	loss 0.9293 (1.1662)	grad_norm 0.3347 (0.3558)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:35:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:10:24 lr 0.000017	 wd 0.0500	time 0.4854 (0.5192)	loss 1.4003 (1.1666)	grad_norm 0.3548 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:36:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:09:21 lr 0.000017	 wd 0.0500	time 0.4451 (0.5099)	loss 0.8242 (1.1671)	grad_norm 0.3654 (0.3556)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:36:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:08:22 lr 0.000017	 wd 0.0500	time 0.3930 (0.5019)	loss 1.4024 (1.1678)	grad_norm 0.3470 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:37:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:07:26 lr 0.000017	 wd 0.0500	time 0.3644 (0.4954)	loss 1.2799 (1.1690)	grad_norm 0.3401 (0.3563)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:38:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:06:32 lr 0.000017	 wd 0.0500	time 0.3946 (0.4895)	loss 1.0909 (1.1681)	grad_norm 0.3403 (0.3555)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:38:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:05:40 lr 0.000017	 wd 0.0500	time 0.4582 (0.4851)	loss 0.8627 (1.1680)	grad_norm 0.4125 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:39:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:04:49 lr 0.000017	 wd 0.0500	time 0.3758 (0.4805)	loss 1.1463 (1.1671)	grad_norm 0.3497 (0.3548)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:40:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:03:59 lr 0.000017	 wd 0.0500	time 0.3812 (0.4763)	loss 1.0641 (1.1673)	grad_norm 0.3174 (0.3552)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:41:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:03:15 lr 0.000017	 wd 0.0500	time 0.4018 (0.4870)	loss 1.3649 (1.1665)	grad_norm 0.3544 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:42:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:28 lr 0.000017	 wd 0.0500	time 0.4480 (0.4927)	loss 0.9726 (1.1683)	grad_norm 0.3729 (0.3547)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:43:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:39 lr 0.000016	 wd 0.0500	time 0.5896 (0.4933)	loss 1.1696 (1.1665)	grad_norm 0.3514 (0.3544)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:44:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:50 lr 0.000016	 wd 0.0500	time 0.4218 (0.4976)	loss 1.3938 (1.1677)	grad_norm 0.3392 (0.3542)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 18:44:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.3299 (0.4952)	loss 0.8626 (1.1659)	grad_norm 0.3168 (nan)	loss_scale 2048.0000 (4068.1583)	mem 17019MB
[2024-07-03 18:45:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 11 training takes 0:20:46
[2024-07-03 18:46:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 92.320 (92.320)	Loss 0.5063 (0.5063)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 18:47:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.802 Acc@5 97.782
[2024-07-03 18:47:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-03 18:47:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.81%
[2024-07-03 18:47:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][0/2502]	eta 15:24:01 lr 0.000016	 wd 0.0500	time 22.1588 (22.1588)	loss 0.9639 (0.9639)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:48:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:24:27 lr 0.000016	 wd 0.0500	time 0.3852 (0.6109)	loss 1.3137 (1.1689)	grad_norm 0.3483 (0.3489)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:48:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:19:24 lr 0.000016	 wd 0.0500	time 0.3668 (0.5060)	loss 1.3139 (1.1660)	grad_norm 0.3367 (0.3609)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:49:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:17:02 lr 0.000016	 wd 0.0500	time 0.3698 (0.4642)	loss 0.7575 (1.1783)	grad_norm 0.3726 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:50:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:15:37 lr 0.000016	 wd 0.0500	time 0.3434 (0.4461)	loss 1.2350 (1.1703)	grad_norm 0.3472 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:50:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:14:29 lr 0.000016	 wd 0.0500	time 0.3906 (0.4343)	loss 1.4441 (1.1702)	grad_norm 0.3362 (0.3532)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:51:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:13:31 lr 0.000016	 wd 0.0500	time 0.4588 (0.4265)	loss 1.4225 (1.1688)	grad_norm 0.3654 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:52:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:12:41 lr 0.000016	 wd 0.0500	time 0.3562 (0.4225)	loss 1.2251 (1.1671)	grad_norm 0.3466 (0.3535)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:52:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:11:51 lr 0.000016	 wd 0.0500	time 0.3790 (0.4178)	loss 1.3730 (1.1681)	grad_norm 0.3532 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:53:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:11:15 lr 0.000016	 wd 0.0500	time 0.7372 (0.4215)	loss 1.5418 (1.1655)	grad_norm 0.3482 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:54:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:10:51 lr 0.000016	 wd 0.0500	time 0.6439 (0.4334)	loss 1.3985 (1.1675)	grad_norm 0.3508 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:55:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:10:59 lr 0.000016	 wd 0.0500	time 0.6376 (0.4705)	loss 0.8119 (1.1690)	grad_norm 0.3350 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:57:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:10:45 lr 0.000016	 wd 0.0500	time 0.6145 (0.4958)	loss 1.4338 (1.1671)	grad_norm 0.3453 (0.3544)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:58:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:10:07 lr 0.000016	 wd 0.0500	time 0.4899 (0.5058)	loss 1.0488 (1.1680)	grad_norm 0.3401 (0.3544)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 18:59:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:09:26 lr 0.000016	 wd 0.0500	time 0.6948 (0.5142)	loss 1.3282 (1.1677)	grad_norm 0.3353 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:00:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:08:57 lr 0.000016	 wd 0.0500	time 0.5598 (0.5366)	loss 0.8206 (1.1650)	grad_norm 1.1740 (0.3559)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:02:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:08:21 lr 0.000016	 wd 0.0500	time 0.3444 (0.5555)	loss 1.3742 (1.1617)	grad_norm 0.3337 (0.3552)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:02:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:07:17 lr 0.000016	 wd 0.0500	time 0.3849 (0.5454)	loss 1.0679 (1.1639)	grad_norm 0.3809 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:03:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:06:17 lr 0.000016	 wd 0.0500	time 0.4983 (0.5374)	loss 0.8178 (1.1624)	grad_norm 0.3709 (0.3559)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:04:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:05:19 lr 0.000016	 wd 0.0500	time 0.3300 (0.5303)	loss 1.3247 (1.1631)	grad_norm 0.3461 (0.3562)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:04:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:04:22 lr 0.000016	 wd 0.0500	time 0.3660 (0.5235)	loss 1.0421 (1.1639)	grad_norm 0.3370 (0.3563)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:05:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:03:28 lr 0.000016	 wd 0.0500	time 0.3588 (0.5181)	loss 0.8280 (1.1632)	grad_norm 0.3153 (0.3558)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:06:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:34 lr 0.000016	 wd 0.0500	time 0.3916 (0.5125)	loss 0.8164 (1.1646)	grad_norm 0.3549 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:06:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:43 lr 0.000015	 wd 0.0500	time 0.6175 (0.5104)	loss 1.1903 (1.1651)	grad_norm 0.3350 (0.3554)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:07:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:52 lr 0.000015	 wd 0.0500	time 0.4808 (0.5118)	loss 1.3672 (1.1653)	grad_norm 0.3341 (0.3551)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:08:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.3487 (0.5080)	loss 0.9456 (1.1646)	grad_norm 0.3467 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:08:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 12 training takes 0:21:27
[2024-07-03 19:10:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 79.858 (79.858)	Loss 0.5186 (0.5186)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 19:10:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.832 Acc@5 97.782
[2024-07-03 19:10:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-03 19:10:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.83%
[2024-07-03 19:10:25 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 19:10:26 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 19:10:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][0/2502]	eta 12:06:44 lr 0.000015	 wd 0.0500	time 17.4280 (17.4280)	loss 1.3330 (1.3330)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:11:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:22:47 lr 0.000015	 wd 0.0500	time 0.5003 (0.5693)	loss 1.3529 (1.2292)	grad_norm 0.3422 (0.3588)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:12:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:18:35 lr 0.000015	 wd 0.0500	time 0.3462 (0.4844)	loss 1.3978 (1.2168)	grad_norm 0.3293 (0.3551)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:12:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:16:35 lr 0.000015	 wd 0.0500	time 0.3646 (0.4519)	loss 1.4016 (1.2169)	grad_norm 0.3450 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:13:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:15:20 lr 0.000015	 wd 0.0500	time 0.3994 (0.4378)	loss 1.4677 (1.1974)	grad_norm 0.3478 (0.3513)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:14:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:14:14 lr 0.000015	 wd 0.0500	time 0.3961 (0.4268)	loss 1.2936 (1.1905)	grad_norm 0.3610 (0.3506)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:14:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:13:19 lr 0.000015	 wd 0.0500	time 0.3752 (0.4202)	loss 1.2099 (1.1844)	grad_norm 0.3375 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:15:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:12:31 lr 0.000015	 wd 0.0500	time 0.3764 (0.4173)	loss 1.1799 (1.1753)	grad_norm 0.3567 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:15:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:11:43 lr 0.000015	 wd 0.0500	time 0.3325 (0.4134)	loss 1.2377 (1.1746)	grad_norm 0.3360 (0.3543)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:16:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:11:08 lr 0.000015	 wd 0.0500	time 0.7380 (0.4171)	loss 1.3362 (1.1752)	grad_norm 0.3532 (0.3557)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:17:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:10:50 lr 0.000015	 wd 0.0500	time 0.5078 (0.4328)	loss 1.3938 (1.1740)	grad_norm 0.3310 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:18:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:10:38 lr 0.000015	 wd 0.0500	time 0.4146 (0.4551)	loss 1.4048 (1.1698)	grad_norm 0.3471 (0.3554)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:19:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:10:01 lr 0.000015	 wd 0.0500	time 0.5215 (0.4623)	loss 1.1966 (1.1738)	grad_norm 0.3698 (0.3576)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:21:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:09:52 lr 0.000015	 wd 0.0500	time 0.4263 (0.4926)	loss 0.7353 (1.1722)	grad_norm 0.3555 (0.3578)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:22:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:09:09 lr 0.000015	 wd 0.0500	time 0.4129 (0.4988)	loss 1.3621 (1.1709)	grad_norm 0.3410 (0.3571)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:22:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:08:21 lr 0.000015	 wd 0.0500	time 0.3903 (0.5001)	loss 1.2236 (1.1714)	grad_norm 0.3530 (0.3568)	loss_scale 4096.0000 (2099.8481)	mem 17019MB
[2024-07-03 19:24:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:07:47 lr 0.000015	 wd 0.0500	time 0.7583 (0.5183)	loss 0.9117 (1.1703)	grad_norm 0.3696 (0.3583)	loss_scale 4096.0000 (2224.5297)	mem 17019MB
[2024-07-03 19:25:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:07:01 lr 0.000015	 wd 0.0500	time 0.3993 (0.5257)	loss 1.0372 (1.1716)	grad_norm 0.3490 (0.3583)	loss_scale 4096.0000 (2334.5514)	mem 17019MB
[2024-07-03 19:26:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:06:08 lr 0.000015	 wd 0.0500	time 0.3972 (0.5255)	loss 1.2404 (1.1720)	grad_norm 0.3374 (0.3609)	loss_scale 4096.0000 (2432.3554)	mem 17019MB
[2024-07-03 19:27:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:05:26 lr 0.000015	 wd 0.0500	time 0.4416 (0.5422)	loss 1.0863 (1.1721)	grad_norm 0.3400 (0.3606)	loss_scale 4096.0000 (2519.8695)	mem 17019MB
[2024-07-03 19:29:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:04:41 lr 0.000015	 wd 0.0500	time 0.8431 (0.5615)	loss 1.3959 (1.1719)	grad_norm 0.3251 (0.3603)	loss_scale 4096.0000 (2598.6367)	mem 17019MB
[2024-07-03 19:30:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:03:47 lr 0.000014	 wd 0.0500	time 0.3533 (0.5649)	loss 1.3729 (1.1719)	grad_norm 0.3460 (0.3605)	loss_scale 4096.0000 (2669.9058)	mem 17019MB
[2024-07-03 19:30:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:48 lr 0.000014	 wd 0.0500	time 0.3337 (0.5570)	loss 1.2109 (1.1704)	grad_norm 0.3440 (0.3604)	loss_scale 4096.0000 (2734.6988)	mem 17019MB
[2024-07-03 19:31:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:51 lr 0.000014	 wd 0.0500	time 0.4120 (0.5503)	loss 1.3493 (1.1694)	grad_norm 0.3671 (0.3600)	loss_scale 4096.0000 (2793.8601)	mem 17019MB
[2024-07-03 19:32:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:55 lr 0.000014	 wd 0.0500	time 0.3866 (0.5437)	loss 0.9578 (1.1721)	grad_norm 0.3646 (0.3603)	loss_scale 4096.0000 (2848.0933)	mem 17019MB
[2024-07-03 19:32:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.3418 (0.5368)	loss 0.7922 (1.1708)	grad_norm 0.3390 (0.3610)	loss_scale 4096.0000 (2897.9896)	mem 17019MB
[2024-07-03 19:32:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 13 training takes 0:22:27
[2024-07-03 19:33:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 32.097 (32.097)	Loss 0.4915 (0.4915)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 19:33:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.890 Acc@5 97.810
[2024-07-03 19:33:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-03 19:33:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.89%
[2024-07-03 19:33:45 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 19:33:47 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 19:34:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][0/2502]	eta 10:26:40 lr 0.000014	 wd 0.0500	time 15.0282 (15.0282)	loss 1.4300 (1.4300)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:34:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:21:54 lr 0.000014	 wd 0.0500	time 0.3703 (0.5473)	loss 1.1889 (1.1881)	grad_norm 0.3389 (0.3543)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:35:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:17:53 lr 0.000014	 wd 0.0500	time 0.4003 (0.4662)	loss 1.4523 (1.1861)	grad_norm 0.5262 (0.3594)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:35:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:16:04 lr 0.000014	 wd 0.0500	time 0.3683 (0.4382)	loss 0.9377 (1.1889)	grad_norm 0.3501 (0.3567)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:36:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:14:59 lr 0.000014	 wd 0.0500	time 0.4425 (0.4279)	loss 1.6121 (1.1881)	grad_norm 0.3335 (0.3561)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:37:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:13:58 lr 0.000014	 wd 0.0500	time 0.3425 (0.4187)	loss 0.9838 (1.1836)	grad_norm 0.3385 (0.3574)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:37:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:13:08 lr 0.000014	 wd 0.0500	time 0.4456 (0.4145)	loss 0.9739 (1.1826)	grad_norm 0.3583 (0.3602)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:38:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:12:18 lr 0.000014	 wd 0.0500	time 0.3983 (0.4100)	loss 1.1404 (1.1832)	grad_norm 0.3535 (0.3599)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:39:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:11:31 lr 0.000014	 wd 0.0500	time 0.3587 (0.4064)	loss 0.8111 (1.1773)	grad_norm 0.3294 (0.3590)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:40:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:11:31 lr 0.000014	 wd 0.0500	time 0.5074 (0.4314)	loss 0.8669 (1.1763)	grad_norm 0.3733 (0.3597)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:41:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:11:03 lr 0.000014	 wd 0.0500	time 0.5968 (0.4419)	loss 1.0054 (1.1736)	grad_norm 0.3629 (0.3594)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:42:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:10:48 lr 0.000014	 wd 0.0500	time 0.4562 (0.4625)	loss 1.3919 (1.1730)	grad_norm 0.3616 (0.3585)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:43:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:10:18 lr 0.000014	 wd 0.0500	time 0.4577 (0.4753)	loss 1.4763 (1.1727)	grad_norm 0.3561 (0.3589)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:44:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:09:37 lr 0.000014	 wd 0.0500	time 0.4121 (0.4802)	loss 1.2495 (1.1759)	grad_norm 0.3524 (0.3588)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:45:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:09:13 lr 0.000014	 wd 0.0500	time 0.6355 (0.5019)	loss 1.2477 (1.1754)	grad_norm 0.3541 (0.3583)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:46:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:08:29 lr 0.000014	 wd 0.0500	time 0.5235 (0.5085)	loss 1.4007 (1.1745)	grad_norm 0.3396 (0.3576)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:47:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:07:40 lr 0.000014	 wd 0.0500	time 0.5421 (0.5103)	loss 0.7753 (1.1733)	grad_norm 0.3448 (0.3573)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:48:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:07:06 lr 0.000014	 wd 0.0500	time 0.5011 (0.5319)	loss 0.9544 (1.1722)	grad_norm 0.3348 (0.3576)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:50:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:06:19 lr 0.000013	 wd 0.0500	time 0.4421 (0.5402)	loss 0.9064 (1.1728)	grad_norm 0.3590 (0.3582)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:50:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:05:25 lr 0.000013	 wd 0.0500	time 0.4221 (0.5401)	loss 1.1687 (1.1731)	grad_norm 0.4146 (0.3578)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 19:51:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:04:33 lr 0.000013	 wd 0.0500	time 0.4826 (0.5441)	loss 1.2275 (1.1724)	grad_norm 0.3424 (nan)	loss_scale 2048.0000 (4010.0270)	mem 17019MB
[2024-07-03 19:52:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:03:39 lr 0.000013	 wd 0.0500	time 0.6577 (0.5466)	loss 1.3540 (1.1718)	grad_norm 0.3441 (nan)	loss_scale 2048.0000 (3916.6416)	mem 17019MB
[2024-07-03 19:53:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:44 lr 0.000013	 wd 0.0500	time 0.3840 (0.5463)	loss 1.3159 (1.1709)	grad_norm 0.3577 (nan)	loss_scale 2048.0000 (3831.7419)	mem 17019MB
[2024-07-03 19:54:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:50 lr 0.000013	 wd 0.0500	time 0.4181 (0.5487)	loss 0.8571 (1.1698)	grad_norm 0.3547 (nan)	loss_scale 2048.0000 (3754.2216)	mem 17019MB
[2024-07-03 19:55:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:55 lr 0.000013	 wd 0.0500	time 0.6455 (0.5478)	loss 0.8720 (1.1689)	grad_norm 0.3645 (nan)	loss_scale 2048.0000 (3683.1587)	mem 17019MB
[2024-07-03 19:56:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.3301 (0.5430)	loss 1.2509 (1.1678)	grad_norm 0.3476 (nan)	loss_scale 2048.0000 (3617.7785)	mem 17019MB
[2024-07-03 19:56:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 14 training takes 0:22:52
[2024-07-03 19:57:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 74.910 (74.910)	Loss 0.4817 (0.4817)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-07-03 19:58:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.928 Acc@5 97.840
[2024-07-03 19:58:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-03 19:58:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.93%
[2024-07-03 19:58:15 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 19:58:16 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 19:58:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][0/2502]	eta 11:48:38 lr 0.000013	 wd 0.0500	time 16.9937 (16.9937)	loss 1.2673 (1.2673)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:59:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:22:23 lr 0.000013	 wd 0.0500	time 0.3699 (0.5591)	loss 0.7802 (1.1736)	grad_norm 0.3504 (0.3580)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 19:59:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:18:18 lr 0.000013	 wd 0.0500	time 0.3726 (0.4772)	loss 1.4582 (1.1596)	grad_norm 0.3521 (0.3568)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:00:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:16:20 lr 0.000013	 wd 0.0500	time 0.3470 (0.4451)	loss 0.9378 (1.1594)	grad_norm 0.3487 (0.3554)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:01:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:15:06 lr 0.000013	 wd 0.0500	time 0.3743 (0.4315)	loss 0.8164 (1.1579)	grad_norm 0.3400 (0.3563)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:01:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:14:05 lr 0.000013	 wd 0.0500	time 0.3924 (0.4223)	loss 0.9490 (1.1692)	grad_norm 0.3529 (0.3642)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:02:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:13:13 lr 0.000013	 wd 0.0500	time 0.3783 (0.4172)	loss 1.3943 (1.1719)	grad_norm 0.3759 (0.3641)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:03:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:12:28 lr 0.000013	 wd 0.0500	time 0.3909 (0.4154)	loss 1.2418 (1.1719)	grad_norm 0.3914 (0.3624)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:03:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:11:42 lr 0.000013	 wd 0.0500	time 0.3663 (0.4125)	loss 1.4031 (1.1736)	grad_norm 0.3362 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:04:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:11:22 lr 0.000013	 wd 0.0500	time 6.3620 (0.4259)	loss 1.2731 (1.1732)	grad_norm 0.3537 (0.3635)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:05:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:10:54 lr 0.000013	 wd 0.0500	time 0.5038 (0.4355)	loss 0.8533 (1.1731)	grad_norm 0.3412 (0.3624)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:06:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:10:37 lr 0.000013	 wd 0.0500	time 0.4867 (0.4545)	loss 0.8690 (1.1739)	grad_norm 0.3460 (0.3645)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:07:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:10:01 lr 0.000013	 wd 0.0500	time 0.6278 (0.4617)	loss 0.8371 (1.1740)	grad_norm 0.4059 (0.3631)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:08:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:09:34 lr 0.000013	 wd 0.0500	time 0.5211 (0.4777)	loss 0.8808 (1.1751)	grad_norm 0.3433 (0.3653)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:09:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:08:55 lr 0.000012	 wd 0.0500	time 0.5138 (0.4863)	loss 0.9490 (1.1752)	grad_norm 0.3538 (0.3646)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:10:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:08:09 lr 0.000012	 wd 0.0500	time 0.5668 (0.4884)	loss 1.0252 (1.1714)	grad_norm 0.3504 (0.3641)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:11:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:07:26 lr 0.000012	 wd 0.0500	time 0.4590 (0.4949)	loss 1.0228 (1.1721)	grad_norm 0.3717 (0.3639)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:12:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:06:38 lr 0.000012	 wd 0.0500	time 0.4513 (0.4967)	loss 0.8279 (1.1721)	grad_norm 0.3559 (0.3631)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:13:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:05:55 lr 0.000012	 wd 0.0500	time 0.4787 (0.5069)	loss 1.6804 (1.1702)	grad_norm 0.3681 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:14:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:05:12 lr 0.000012	 wd 0.0500	time 0.4397 (0.5194)	loss 0.9984 (1.1684)	grad_norm 0.3820 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 20:15:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:04:21 lr 0.000012	 wd 0.0500	time 0.5209 (0.5204)	loss 0.8618 (1.1697)	grad_norm 0.3486 (nan)	loss_scale 1024.0000 (2035.7181)	mem 17019MB
[2024-07-03 20:16:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:03:32 lr 0.000012	 wd 0.0500	time 0.6306 (0.5286)	loss 0.8012 (1.1685)	grad_norm 0.3464 (nan)	loss_scale 1024.0000 (1987.5640)	mem 17019MB
[2024-07-03 20:17:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:40 lr 0.000012	 wd 0.0500	time 0.5217 (0.5312)	loss 1.0906 (1.1680)	grad_norm 0.3330 (nan)	loss_scale 1024.0000 (1943.7856)	mem 17019MB
[2024-07-03 20:18:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:47 lr 0.000012	 wd 0.0500	time 0.4366 (0.5309)	loss 1.0309 (1.1683)	grad_norm 0.3614 (nan)	loss_scale 1024.0000 (1903.8123)	mem 17019MB
[2024-07-03 20:19:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:54 lr 0.000012	 wd 0.0500	time 0.4987 (0.5332)	loss 1.4808 (1.1687)	grad_norm 0.3277 (nan)	loss_scale 1024.0000 (1867.1687)	mem 17019MB
[2024-07-03 20:20:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.3503 (0.5279)	loss 1.5216 (1.1692)	grad_norm 0.3541 (nan)	loss_scale 1024.0000 (1833.4554)	mem 17019MB
[2024-07-03 20:20:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 15 training takes 0:22:08
[2024-07-03 20:22:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 130.137 (130.137)	Loss 0.5361 (0.5361)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 20:23:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.852 Acc@5 97.802
[2024-07-03 20:23:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-03 20:23:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.93%
[2024-07-03 20:23:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][0/2502]	eta 14:05:56 lr 0.000012	 wd 0.0500	time 20.2863 (20.2863)	loss 1.2180 (1.2180)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:24:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:24:15 lr 0.000012	 wd 0.0500	time 0.4220 (0.6058)	loss 1.0601 (1.1798)	grad_norm 0.3521 (0.3542)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:24:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:19:05 lr 0.000012	 wd 0.0500	time 0.3449 (0.4978)	loss 1.2596 (1.1693)	grad_norm 0.3468 (0.3539)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:25:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:16:51 lr 0.000012	 wd 0.0500	time 0.3885 (0.4595)	loss 1.0124 (1.1623)	grad_norm 0.3418 (0.3555)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:25:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:15:31 lr 0.000012	 wd 0.0500	time 0.3424 (0.4430)	loss 0.9349 (1.1508)	grad_norm 0.3517 (0.3548)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:26:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:14:22 lr 0.000012	 wd 0.0500	time 0.3951 (0.4307)	loss 1.5374 (1.1538)	grad_norm 0.3582 (0.3554)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:27:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:13:25 lr 0.000012	 wd 0.0500	time 0.3864 (0.4235)	loss 1.4330 (1.1544)	grad_norm 0.3209 (0.3556)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:27:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:12:35 lr 0.000012	 wd 0.0500	time 0.3550 (0.4195)	loss 1.0778 (1.1568)	grad_norm 0.3607 (0.3591)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:28:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:11:47 lr 0.000012	 wd 0.0500	time 0.3689 (0.4154)	loss 1.3243 (1.1583)	grad_norm 0.4122 (0.3597)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:29:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:11:38 lr 0.000012	 wd 0.0500	time 0.4023 (0.4358)	loss 1.1430 (1.1594)	grad_norm 0.3518 (0.3614)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:30:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:11:08 lr 0.000011	 wd 0.0500	time 0.4803 (0.4449)	loss 1.3586 (1.1593)	grad_norm 0.3415 (0.3612)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:31:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:11:01 lr 0.000011	 wd 0.0500	time 0.5649 (0.4717)	loss 0.8457 (1.1633)	grad_norm 0.3721 (0.3610)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:32:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:10:31 lr 0.000011	 wd 0.0500	time 0.4986 (0.4852)	loss 0.8765 (1.1611)	grad_norm 0.3447 (0.3605)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:33:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:09:49 lr 0.000011	 wd 0.0500	time 0.4451 (0.4904)	loss 1.3101 (1.1640)	grad_norm 0.3593 (0.3614)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:34:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:09:08 lr 0.000011	 wd 0.0500	time 0.3821 (0.4974)	loss 1.2431 (1.1640)	grad_norm 0.3539 (0.3608)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:35:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:08:19 lr 0.000011	 wd 0.0500	time 0.5162 (0.4985)	loss 1.2971 (1.1663)	grad_norm 0.3354 (0.3606)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:36:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:07:37 lr 0.000011	 wd 0.0500	time 0.5895 (0.5074)	loss 1.1314 (1.1669)	grad_norm 0.3669 (0.3603)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:37:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:06:51 lr 0.000011	 wd 0.0500	time 0.5633 (0.5132)	loss 1.0889 (1.1660)	grad_norm 0.3515 (0.3601)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:38:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:06:01 lr 0.000011	 wd 0.0500	time 0.4951 (0.5149)	loss 0.8761 (1.1649)	grad_norm 0.3216 (0.3601)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:39:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:05:12 lr 0.000011	 wd 0.0500	time 0.4495 (0.5193)	loss 1.0367 (1.1656)	grad_norm 0.3362 (0.3596)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:40:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:04:20 lr 0.000011	 wd 0.0500	time 0.4568 (0.5192)	loss 1.3672 (1.1663)	grad_norm 0.3298 (0.3597)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:41:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:03:29 lr 0.000011	 wd 0.0500	time 0.5311 (0.5222)	loss 1.3673 (1.1671)	grad_norm 0.3296 (0.3598)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:42:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:38 lr 0.000011	 wd 0.0500	time 0.5248 (0.5255)	loss 1.0146 (1.1667)	grad_norm 0.3551 (0.3625)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:43:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:46 lr 0.000011	 wd 0.0500	time 0.5399 (0.5271)	loss 0.8991 (1.1648)	grad_norm 0.3692 (0.3624)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:44:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:53 lr 0.000011	 wd 0.0500	time 0.6966 (0.5292)	loss 1.2234 (1.1638)	grad_norm 0.3364 (0.3629)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:44:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.3659 (0.5241)	loss 0.8491 (1.1641)	grad_norm 0.3517 (0.3627)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:45:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 16 training takes 0:22:00
[2024-07-03 20:46:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 110.603 (110.603)	Loss 0.5039 (0.5039)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-07-03 20:47:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.930 Acc@5 97.830
[2024-07-03 20:47:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-03 20:47:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 85.93%
[2024-07-03 20:47:32 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 20:47:33 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 20:47:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][0/2502]	eta 14:45:36 lr 0.000011	 wd 0.0500	time 21.2377 (21.2377)	loss 1.4224 (1.4224)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:48:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:24:04 lr 0.000011	 wd 0.0500	time 0.4179 (0.6015)	loss 1.2395 (1.1477)	grad_norm 0.3373 (0.3656)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:49:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:19:15 lr 0.000011	 wd 0.0500	time 0.3468 (0.5020)	loss 1.2919 (1.1474)	grad_norm 0.3485 (0.3667)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:49:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:16:57 lr 0.000011	 wd 0.0500	time 0.3564 (0.4620)	loss 1.2463 (1.1627)	grad_norm 0.3571 (0.3633)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:50:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:15:34 lr 0.000011	 wd 0.0500	time 0.4233 (0.4444)	loss 1.4302 (1.1607)	grad_norm 0.3490 (0.3755)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:51:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:14:27 lr 0.000010	 wd 0.0500	time 0.4485 (0.4333)	loss 1.3720 (1.1651)	grad_norm 0.3556 (0.3718)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:51:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:13:29 lr 0.000010	 wd 0.0500	time 0.3634 (0.4253)	loss 0.8097 (1.1661)	grad_norm 0.3494 (0.3682)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:52:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:12:40 lr 0.000010	 wd 0.0500	time 0.3554 (0.4218)	loss 1.2903 (1.1638)	grad_norm 0.3544 (0.3700)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:53:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:11:52 lr 0.000010	 wd 0.0500	time 0.4474 (0.4185)	loss 1.1912 (1.1620)	grad_norm 0.3595 (0.3699)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:53:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:11:09 lr 0.000010	 wd 0.0500	time 1.0617 (0.4179)	loss 1.3023 (1.1637)	grad_norm 0.3532 (0.3684)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-07-03 20:54:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:11:03 lr 0.000010	 wd 0.0500	time 0.5325 (0.4418)	loss 1.0353 (1.1629)	grad_norm 0.3606 (0.3684)	loss_scale 2048.0000 (1052.6434)	mem 17019MB
[2024-07-03 20:56:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:11:04 lr 0.000010	 wd 0.0500	time 0.6956 (0.4736)	loss 1.0360 (1.1675)	grad_norm 0.3379 (0.3673)	loss_scale 2048.0000 (1143.0481)	mem 17019MB
[2024-07-03 20:57:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:10:32 lr 0.000010	 wd 0.0500	time 0.7409 (0.4859)	loss 0.7689 (1.1674)	grad_norm 0.3442 (0.3697)	loss_scale 2048.0000 (1218.3980)	mem 17019MB
[2024-07-03 20:58:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:10:01 lr 0.000010	 wd 0.0500	time 0.4952 (0.5007)	loss 1.4264 (1.1645)	grad_norm 0.3623 (0.3687)	loss_scale 2048.0000 (1282.1645)	mem 17019MB
[2024-07-03 20:59:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:09:19 lr 0.000010	 wd 0.0500	time 0.4508 (0.5074)	loss 1.3331 (1.1646)	grad_norm 0.3266 (0.3697)	loss_scale 2048.0000 (1336.8280)	mem 17019MB
[2024-07-03 21:00:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:08:33 lr 0.000010	 wd 0.0500	time 3.1786 (0.5125)	loss 1.4704 (1.1651)	grad_norm 0.3441 (0.3686)	loss_scale 2048.0000 (1384.2079)	mem 17019MB
[2024-07-03 21:01:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:07:58 lr 0.000010	 wd 0.0500	time 0.3687 (0.5301)	loss 1.3932 (1.1650)	grad_norm 0.3552 (0.3682)	loss_scale 2048.0000 (1425.6690)	mem 17019MB
[2024-07-03 21:02:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:07:13 lr 0.000010	 wd 0.0500	time 0.8185 (0.5404)	loss 1.5335 (1.1668)	grad_norm 0.3376 (0.3676)	loss_scale 2048.0000 (1462.2551)	mem 17019MB
[2024-07-03 21:03:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:06:22 lr 0.000010	 wd 0.0500	time 0.7457 (0.5448)	loss 0.9764 (1.1662)	grad_norm 0.4498 (0.3668)	loss_scale 2048.0000 (1494.7785)	mem 17019MB
[2024-07-03 21:04:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:05:28 lr 0.000010	 wd 0.0500	time 0.4652 (0.5452)	loss 1.2511 (1.1652)	grad_norm 0.3380 (0.3660)	loss_scale 2048.0000 (1523.8801)	mem 17019MB
[2024-07-03 21:05:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:04:37 lr 0.000010	 wd 0.0500	time 0.4801 (0.5525)	loss 1.2015 (1.1654)	grad_norm 0.3483 (0.3654)	loss_scale 2048.0000 (1550.0730)	mem 17019MB
[2024-07-03 21:06:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:03:42 lr 0.000010	 wd 0.0500	time 0.8110 (0.5523)	loss 1.4773 (1.1653)	grad_norm 0.3763 (0.3648)	loss_scale 2048.0000 (1573.7725)	mem 17019MB
[2024-07-03 21:07:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:47 lr 0.000010	 wd 0.0500	time 0.6087 (0.5546)	loss 1.3757 (1.1652)	grad_norm 0.3406 (0.3642)	loss_scale 2048.0000 (1595.3185)	mem 17019MB
[2024-07-03 21:08:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:52 lr 0.000010	 wd 0.0500	time 0.5032 (0.5566)	loss 0.8092 (1.1645)	grad_norm 0.3932 (0.3641)	loss_scale 2048.0000 (1614.9917)	mem 17019MB
[2024-07-03 21:09:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:56 lr 0.000010	 wd 0.0500	time 0.4821 (0.5555)	loss 0.9379 (1.1633)	grad_norm 0.4016 (0.3637)	loss_scale 2048.0000 (1633.0262)	mem 17019MB
[2024-07-03 21:10:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.3491 (0.5495)	loss 1.4606 (1.1629)	grad_norm 0.3392 (0.3637)	loss_scale 2048.0000 (1649.6186)	mem 17019MB
[2024-07-03 21:10:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 17 training takes 0:23:18
[2024-07-03 21:12:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 73.418 (73.418)	Loss 0.4939 (0.4939)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 21:12:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.996 Acc@5 97.826
[2024-07-03 21:12:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-03 21:12:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 21:12:28 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 21:12:30 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 21:12:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][0/2502]	eta 11:48:03 lr 0.000009	 wd 0.0500	time 16.9796 (16.9796)	loss 1.4500 (1.4500)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:13:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:22:17 lr 0.000009	 wd 0.0500	time 0.3921 (0.5570)	loss 1.4888 (1.1974)	grad_norm 0.3549 (0.3571)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:14:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:18:10 lr 0.000009	 wd 0.0500	time 0.3636 (0.4738)	loss 1.0344 (1.1884)	grad_norm 0.3778 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:14:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:16:11 lr 0.000009	 wd 0.0500	time 0.3568 (0.4412)	loss 1.5126 (1.1876)	grad_norm 0.3412 (0.3562)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:15:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:15:03 lr 0.000009	 wd 0.0500	time 0.4196 (0.4298)	loss 0.7590 (1.1862)	grad_norm 0.3581 (0.3554)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:16:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:14:03 lr 0.000009	 wd 0.0500	time 0.3853 (0.4213)	loss 1.2909 (1.1778)	grad_norm 0.3368 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:16:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:13:09 lr 0.000009	 wd 0.0500	time 0.3815 (0.4149)	loss 1.4977 (1.1702)	grad_norm 0.3892 (0.3543)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:17:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:12:23 lr 0.000009	 wd 0.0500	time 0.3634 (0.4128)	loss 1.4568 (1.1751)	grad_norm 0.3440 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:17:57 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:11:35 lr 0.000009	 wd 0.0500	time 0.3482 (0.4087)	loss 1.3361 (1.1781)	grad_norm 0.3433 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:18:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:11:08 lr 0.000009	 wd 0.0500	time 0.7060 (0.4173)	loss 0.9246 (1.1779)	grad_norm 0.3550 (0.3560)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:19:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:10:43 lr 0.000009	 wd 0.0500	time 0.4790 (0.4282)	loss 1.4393 (1.1737)	grad_norm 0.3530 (0.3579)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:20:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:10:40 lr 0.000009	 wd 0.0500	time 0.6807 (0.4570)	loss 1.0055 (1.1756)	grad_norm 0.3454 (0.3578)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:21:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:10:08 lr 0.000009	 wd 0.0500	time 0.7876 (0.4676)	loss 1.0633 (1.1689)	grad_norm 0.3511 (0.3616)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:22:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:09:38 lr 0.000009	 wd 0.0500	time 0.5239 (0.4811)	loss 1.4775 (1.1692)	grad_norm 0.3576 (0.3614)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:24:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:09:10 lr 0.000009	 wd 0.0500	time 0.6119 (0.4994)	loss 1.1558 (1.1695)	grad_norm 0.3367 (0.3615)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:25:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:08:24 lr 0.000009	 wd 0.0500	time 0.6865 (0.5032)	loss 1.3176 (1.1691)	grad_norm 0.3369 (0.3610)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:26:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:07:39 lr 0.000009	 wd 0.0500	time 0.4669 (0.5096)	loss 1.4540 (1.1711)	grad_norm 0.3751 (0.3624)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:27:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:06:55 lr 0.000009	 wd 0.0500	time 0.4774 (0.5177)	loss 1.3341 (1.1729)	grad_norm 0.3597 (0.3629)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:28:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:06:03 lr 0.000009	 wd 0.0500	time 0.4554 (0.5181)	loss 0.7260 (1.1732)	grad_norm 0.3609 (0.3624)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:29:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:05:16 lr 0.000009	 wd 0.0500	time 0.5974 (0.5259)	loss 1.4317 (1.1728)	grad_norm 0.3453 (0.3623)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:30:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:04:27 lr 0.000008	 wd 0.0500	time 0.6203 (0.5329)	loss 1.0688 (1.1725)	grad_norm 0.3465 (0.3620)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:31:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:03:34 lr 0.000008	 wd 0.0500	time 0.4873 (0.5331)	loss 1.3385 (1.1714)	grad_norm 0.3519 (0.3648)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:32:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:41 lr 0.000008	 wd 0.0500	time 0.3960 (0.5354)	loss 1.0282 (1.1715)	grad_norm 0.3288 (0.3664)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:33:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:48 lr 0.000008	 wd 0.0500	time 0.3950 (0.5347)	loss 1.3221 (1.1721)	grad_norm 0.3333 (0.3659)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:34:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.6703 (0.5381)	loss 1.4328 (1.1727)	grad_norm 0.3613 (0.3655)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 21:34:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.3554 (0.5346)	loss 1.2710 (1.1730)	grad_norm 0.3440 (0.3650)	loss_scale 4096.0000 (2072.5662)	mem 17019MB
[2024-07-03 21:35:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 18 training takes 0:22:30
[2024-07-03 21:36:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 82.553 (82.553)	Loss 0.5054 (0.5054)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 21:37:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.940 Acc@5 97.816
[2024-07-03 21:37:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-03 21:37:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 21:37:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][0/2502]	eta 15:22:07 lr 0.000008	 wd 0.0500	time 22.1134 (22.1134)	loss 0.8049 (0.8049)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:38:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:24:16 lr 0.000008	 wd 0.0500	time 0.4415 (0.6066)	loss 1.6067 (1.2188)	grad_norm 0.3342 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:38:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:19:13 lr 0.000008	 wd 0.0500	time 0.3456 (0.5012)	loss 1.1781 (1.1884)	grad_norm 0.4582 (0.3724)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:39:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:16:54 lr 0.000008	 wd 0.0500	time 0.3479 (0.4608)	loss 1.4079 (1.1751)	grad_norm 0.3553 (0.3662)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:40:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:15:34 lr 0.000008	 wd 0.0500	time 0.4397 (0.4448)	loss 0.9724 (1.1684)	grad_norm 0.3378 (0.3697)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:40:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:14:28 lr 0.000008	 wd 0.0500	time 0.3750 (0.4336)	loss 1.4570 (1.1726)	grad_norm 0.3558 (0.3664)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:41:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:13:28 lr 0.000008	 wd 0.0500	time 0.4135 (0.4252)	loss 0.8541 (1.1622)	grad_norm 0.3569 (0.3655)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:41:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:12:39 lr 0.000008	 wd 0.0500	time 0.3823 (0.4213)	loss 1.2449 (1.1602)	grad_norm 0.3439 (0.3788)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:42:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:11:50 lr 0.000008	 wd 0.0500	time 0.3917 (0.4173)	loss 1.5250 (1.1620)	grad_norm 0.3654 (0.3766)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:43:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:11:41 lr 0.000008	 wd 0.0500	time 0.6290 (0.4376)	loss 1.2859 (1.1625)	grad_norm 0.3603 (0.3745)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:44:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:11:12 lr 0.000008	 wd 0.0500	time 0.4641 (0.4475)	loss 1.4540 (1.1639)	grad_norm 0.3395 (0.3741)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:46:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:11:47 lr 0.000008	 wd 0.0500	time 1.1548 (0.5050)	loss 1.1853 (1.1629)	grad_norm 0.3625 (0.3722)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:47:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:10:59 lr 0.000008	 wd 0.0500	time 0.3752 (0.5068)	loss 1.3375 (1.1625)	grad_norm 0.3532 (0.3718)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:47:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:09:58 lr 0.000008	 wd 0.0500	time 0.4061 (0.4978)	loss 0.7778 (1.1614)	grad_norm 0.3628 (0.3715)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:48:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:09:00 lr 0.000008	 wd 0.0500	time 0.3569 (0.4904)	loss 0.8110 (1.1606)	grad_norm 0.3644 (0.3708)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:49:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:08:04 lr 0.000008	 wd 0.0500	time 0.3397 (0.4834)	loss 1.3659 (1.1618)	grad_norm 0.3327 (0.3703)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:49:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:07:11 lr 0.000007	 wd 0.0500	time 0.3794 (0.4783)	loss 1.4257 (1.1636)	grad_norm 0.3471 (0.3694)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:50:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:06:19 lr 0.000007	 wd 0.0500	time 0.4214 (0.4732)	loss 0.9000 (1.1641)	grad_norm 0.3622 (0.3685)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:51:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:05:29 lr 0.000007	 wd 0.0500	time 0.4475 (0.4689)	loss 1.3339 (1.1665)	grad_norm 0.3648 (0.3689)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:52:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:04:44 lr 0.000007	 wd 0.0500	time 0.3908 (0.4722)	loss 1.1050 (1.1655)	grad_norm 0.3731 (0.3682)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:52:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:03:58 lr 0.000007	 wd 0.0500	time 0.4666 (0.4749)	loss 1.4820 (1.1666)	grad_norm 0.3688 (0.3682)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:53:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:03:14 lr 0.000007	 wd 0.0500	time 0.7592 (0.4831)	loss 1.5859 (1.1680)	grad_norm 0.3475 (0.3691)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:55:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:27 lr 0.000007	 wd 0.0500	time 0.6190 (0.4898)	loss 1.1499 (1.1672)	grad_norm 0.3841 (0.3688)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:55:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:39 lr 0.000007	 wd 0.0500	time 0.4485 (0.4919)	loss 1.0187 (1.1674)	grad_norm 0.3502 (0.3683)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:57:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:51 lr 0.000007	 wd 0.0500	time 0.5404 (0.5031)	loss 1.1103 (1.1665)	grad_norm 0.3580 (0.3681)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:57:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.3333 (0.5001)	loss 0.8137 (1.1667)	grad_norm 0.3529 (0.3681)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 21:58:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 19 training takes 0:21:15
[2024-07-03 21:59:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 89.365 (89.365)	Loss 0.4897 (0.4897)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 22:00:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.958 Acc@5 97.854
[2024-07-03 22:00:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-03 22:00:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 22:00:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][0/2502]	eta 13:33:15 lr 0.000007	 wd 0.0500	time 19.5025 (19.5025)	loss 1.5420 (1.5420)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:01:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:23:37 lr 0.000007	 wd 0.0500	time 0.3786 (0.5903)	loss 1.0707 (1.1765)	grad_norm 0.3580 (0.3701)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:01:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:18:39 lr 0.000007	 wd 0.0500	time 0.3396 (0.4862)	loss 1.3215 (1.1601)	grad_norm 0.3677 (0.3611)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:02:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:16:32 lr 0.000007	 wd 0.0500	time 0.3450 (0.4507)	loss 0.7040 (1.1723)	grad_norm 0.3715 (0.3606)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:03:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:15:19 lr 0.000007	 wd 0.0500	time 0.3753 (0.4374)	loss 1.3209 (1.1715)	grad_norm 0.3473 (0.3624)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:03:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:14:15 lr 0.000007	 wd 0.0500	time 0.3565 (0.4271)	loss 1.2838 (1.1694)	grad_norm 0.3821 (0.3697)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:04:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:13:19 lr 0.000007	 wd 0.0500	time 0.3848 (0.4204)	loss 1.2556 (1.1743)	grad_norm 1.0085 (0.3695)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:05:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:12:30 lr 0.000007	 wd 0.0500	time 0.3465 (0.4163)	loss 1.2759 (1.1727)	grad_norm 0.3446 (0.3720)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:05:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:11:41 lr 0.000007	 wd 0.0500	time 0.3791 (0.4122)	loss 1.1748 (1.1731)	grad_norm 0.3428 (0.3734)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:06:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:11:16 lr 0.000007	 wd 0.0500	time 0.5375 (0.4225)	loss 1.2576 (1.1724)	grad_norm 0.3564 (0.3724)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:07:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:10:53 lr 0.000007	 wd 0.0500	time 0.6177 (0.4349)	loss 1.3998 (1.1744)	grad_norm 0.3438 (0.3705)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:08:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:10:43 lr 0.000007	 wd 0.0500	time 0.5735 (0.4592)	loss 1.0795 (1.1722)	grad_norm 0.3595 (0.3702)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:09:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:10:32 lr 0.000006	 wd 0.0500	time 0.4435 (0.4858)	loss 1.2307 (1.1682)	grad_norm 0.3385 (0.3693)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-03 22:11:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:10:11 lr 0.000006	 wd 0.0500	time 0.4882 (0.5090)	loss 1.3449 (1.1677)	grad_norm 0.3500 (nan)	loss_scale 2048.0000 (4073.9616)	mem 17019MB
[2024-07-03 22:11:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:09:11 lr 0.000006	 wd 0.0500	time 0.3926 (0.5003)	loss 1.0760 (1.1691)	grad_norm 0.3770 (nan)	loss_scale 2048.0000 (3929.3533)	mem 17019MB
[2024-07-03 22:12:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:08:13 lr 0.000006	 wd 0.0500	time 0.3665 (0.4930)	loss 1.3254 (1.1718)	grad_norm 0.3369 (nan)	loss_scale 2048.0000 (3804.0133)	mem 17019MB
[2024-07-03 22:13:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:07:19 lr 0.000006	 wd 0.0500	time 0.4178 (0.4868)	loss 1.3031 (1.1739)	grad_norm 0.4049 (nan)	loss_scale 2048.0000 (3694.3310)	mem 17019MB
[2024-07-03 22:13:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:06:25 lr 0.000006	 wd 0.0500	time 0.3433 (0.4813)	loss 1.6165 (1.1751)	grad_norm 0.3432 (nan)	loss_scale 2048.0000 (3597.5450)	mem 17019MB
[2024-07-03 22:14:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:05:35 lr 0.000006	 wd 0.0500	time 0.4159 (0.4772)	loss 0.7673 (1.1728)	grad_norm 0.3444 (nan)	loss_scale 2048.0000 (3511.5069)	mem 17019MB
[2024-07-03 22:15:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:04:44 lr 0.000006	 wd 0.0500	time 0.3447 (0.4726)	loss 0.8396 (1.1741)	grad_norm 0.3482 (nan)	loss_scale 2048.0000 (3434.5208)	mem 17019MB
[2024-07-03 22:15:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:03:55 lr 0.000006	 wd 0.0500	time 0.3653 (0.4686)	loss 1.0994 (1.1732)	grad_norm 0.3735 (nan)	loss_scale 2048.0000 (3365.2294)	mem 17019MB
[2024-07-03 22:16:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:03:09 lr 0.000006	 wd 0.0500	time 0.6201 (0.4720)	loss 1.2515 (1.1731)	grad_norm 0.3298 (nan)	loss_scale 2048.0000 (3302.5340)	mem 17019MB
[2024-07-03 22:17:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:25 lr 0.000006	 wd 0.0500	time 0.4776 (0.4834)	loss 1.3907 (1.1742)	grad_norm 0.3574 (nan)	loss_scale 2048.0000 (3245.5357)	mem 17019MB
[2024-07-03 22:19:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:41 lr 0.000006	 wd 0.0500	time 0.3756 (0.5046)	loss 0.8772 (1.1754)	grad_norm 0.3487 (nan)	loss_scale 2048.0000 (3193.4915)	mem 17019MB
[2024-07-03 22:20:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:50 lr 0.000006	 wd 0.0500	time 0.3782 (0.4997)	loss 0.8023 (1.1748)	grad_norm 0.3643 (nan)	loss_scale 2048.0000 (3145.7826)	mem 17019MB
[2024-07-03 22:20:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.3448 (0.4945)	loss 1.0575 (1.1766)	grad_norm 0.3415 (nan)	loss_scale 2048.0000 (3101.8888)	mem 17019MB
[2024-07-03 22:21:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 20 training takes 0:20:46
[2024-07-03 22:21:02 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_20.pth saving......
[2024-07-03 22:21:03 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_20.pth saved !!!
[2024-07-03 22:21:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 34.615 (34.615)	Loss 0.4990 (0.4990)	Acc@1 92.383 (92.383)	Acc@5 98.047 (98.047)	Mem 17019MB
[2024-07-03 22:22:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.980 Acc@5 97.834
[2024-07-03 22:22:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-03 22:22:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 22:22:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][0/2502]	eta 23:45:34 lr 0.000006	 wd 0.0500	time 34.1864 (34.1864)	loss 1.0071 (1.0071)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:23:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:29:17 lr 0.000006	 wd 0.0500	time 0.4132 (0.7318)	loss 1.3661 (1.1973)	grad_norm 0.3738 (0.3623)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:23:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:21:26 lr 0.000006	 wd 0.0500	time 0.4134 (0.5590)	loss 1.3248 (1.1915)	grad_norm 0.3530 (0.3606)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:24:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:18:31 lr 0.000006	 wd 0.0500	time 0.3667 (0.5046)	loss 1.1854 (1.1809)	grad_norm 0.3768 (0.3655)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:25:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:16:36 lr 0.000006	 wd 0.0500	time 0.3679 (0.4740)	loss 1.3754 (1.1729)	grad_norm 0.3455 (0.3635)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:25:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:15:17 lr 0.000006	 wd 0.0500	time 0.4807 (0.4585)	loss 0.9103 (1.1677)	grad_norm 0.3594 (0.3657)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:26:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:14:11 lr 0.000006	 wd 0.0500	time 0.3810 (0.4476)	loss 1.3064 (1.1629)	grad_norm 0.3449 (0.3645)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:27:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:13:11 lr 0.000006	 wd 0.0500	time 0.3567 (0.4391)	loss 1.2459 (1.1605)	grad_norm 0.3466 (0.3637)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:28:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:12:44 lr 0.000006	 wd 0.0500	time 0.4438 (0.4491)	loss 1.2191 (1.1590)	grad_norm 0.3542 (0.3637)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:28:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:12:16 lr 0.000005	 wd 0.0500	time 0.4441 (0.4600)	loss 1.4482 (1.1621)	grad_norm 0.3961 (0.3640)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:30:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:13:03 lr 0.000005	 wd 0.0500	time 0.8719 (0.5219)	loss 1.5023 (1.1655)	grad_norm 0.3474 (0.3637)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:31:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:12:04 lr 0.000005	 wd 0.0500	time 0.4401 (0.5171)	loss 0.8159 (1.1687)	grad_norm 0.3470 (0.3675)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:32:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:10:59 lr 0.000005	 wd 0.0500	time 0.4165 (0.5062)	loss 1.4045 (1.1657)	grad_norm 0.3542 (0.3668)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:32:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:09:58 lr 0.000005	 wd 0.0500	time 0.3406 (0.4977)	loss 1.2070 (1.1661)	grad_norm 0.3417 (0.3657)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:33:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:08:59 lr 0.000005	 wd 0.0500	time 0.3513 (0.4895)	loss 0.8286 (1.1634)	grad_norm 0.3664 (0.3656)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:34:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:08:04 lr 0.000005	 wd 0.0500	time 0.5225 (0.4834)	loss 0.8486 (1.1651)	grad_norm 0.3504 (0.3654)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:34:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:07:10 lr 0.000005	 wd 0.0500	time 0.3914 (0.4775)	loss 1.0514 (1.1616)	grad_norm 0.3742 (0.3656)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:35:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:06:18 lr 0.000005	 wd 0.0500	time 0.3685 (0.4722)	loss 1.3515 (1.1624)	grad_norm 0.3359 (0.3652)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:36:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:05:34 lr 0.000005	 wd 0.0500	time 0.4205 (0.4771)	loss 0.7797 (1.1628)	grad_norm 0.3914 (0.3652)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:37:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:04:48 lr 0.000005	 wd 0.0500	time 0.5484 (0.4797)	loss 0.8553 (1.1636)	grad_norm 0.3471 (0.3647)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:38:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:04:03 lr 0.000005	 wd 0.0500	time 0.6022 (0.4849)	loss 1.0707 (1.1627)	grad_norm 0.4365 (0.3653)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:39:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:03:16 lr 0.000005	 wd 0.0500	time 0.5895 (0.4897)	loss 1.1489 (1.1624)	grad_norm 0.3517 (0.3650)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:40:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:28 lr 0.000005	 wd 0.0500	time 0.5681 (0.4932)	loss 1.4555 (1.1610)	grad_norm 0.3682 (0.3649)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:41:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:40 lr 0.000005	 wd 0.0500	time 0.4480 (0.4982)	loss 1.5426 (1.1626)	grad_norm 0.4006 (0.3655)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:42:03 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:50 lr 0.000005	 wd 0.0500	time 0.4300 (0.4998)	loss 1.2296 (1.1644)	grad_norm 0.3613 (0.3665)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:42:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.3384 (0.4982)	loss 1.4900 (1.1649)	grad_norm 0.3523 (0.3668)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:43:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 21 training takes 0:21:07
[2024-07-03 22:45:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 111.560 (111.560)	Loss 0.5166 (0.5166)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 22:45:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.950 Acc@5 97.836
[2024-07-03 22:45:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-03 22:45:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 22:46:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][0/2502]	eta 1 day, 8:52:51 lr 0.000005	 wd 0.0500	time 47.3106 (47.3106)	loss 0.9957 (0.9957)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:46:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:33:56 lr 0.000005	 wd 0.0500	time 0.4401 (0.8480)	loss 1.4795 (1.1811)	grad_norm 0.3737 (0.3578)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:47:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:24:02 lr 0.000005	 wd 0.0500	time 0.3550 (0.6264)	loss 0.7577 (1.1739)	grad_norm 0.3442 (0.3603)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 22:48:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:19:57 lr 0.000005	 wd 0.0500	time 0.3456 (0.5439)	loss 1.3828 (1.1794)	grad_norm 0.3616 (0.3639)	loss_scale 4096.0000 (2170.4718)	mem 17019MB
[2024-07-03 22:48:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:17:41 lr 0.000005	 wd 0.0500	time 0.3629 (0.5052)	loss 0.7604 (1.1854)	grad_norm 0.3456 (0.3645)	loss_scale 4096.0000 (2650.6534)	mem 17019MB
[2024-07-03 22:49:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:16:03 lr 0.000005	 wd 0.0500	time 0.4050 (0.4815)	loss 1.1964 (1.1828)	grad_norm 0.5425 (0.3639)	loss_scale 4096.0000 (2939.1457)	mem 17019MB
[2024-07-03 22:50:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:14:46 lr 0.000005	 wd 0.0500	time 0.3688 (0.4662)	loss 0.8818 (1.1843)	grad_norm 0.3374 (0.3648)	loss_scale 4096.0000 (3131.6339)	mem 17019MB
[2024-07-03 22:50:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:13:42 lr 0.000005	 wd 0.0500	time 0.3873 (0.4565)	loss 1.3590 (1.1789)	grad_norm 0.3591 (0.3762)	loss_scale 4096.0000 (3269.2040)	mem 17019MB
[2024-07-03 22:51:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:12:41 lr 0.000004	 wd 0.0500	time 0.3926 (0.4475)	loss 0.9314 (1.1777)	grad_norm 0.3367 (0.3737)	loss_scale 4096.0000 (3372.4245)	mem 17019MB
[2024-07-03 22:52:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:11:46 lr 0.000004	 wd 0.0500	time 0.4054 (0.4412)	loss 1.1124 (1.1765)	grad_norm 0.4717 (0.3739)	loss_scale 4096.0000 (3452.7325)	mem 17019MB
[2024-07-03 22:53:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:11:48 lr 0.000004	 wd 0.0500	time 0.5292 (0.4716)	loss 1.2762 (1.1769)	grad_norm 0.3418 (0.3773)	loss_scale 4096.0000 (3516.9950)	mem 17019MB
[2024-07-03 22:54:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:11:32 lr 0.000004	 wd 0.0500	time 0.7965 (0.4937)	loss 0.8108 (1.1741)	grad_norm 0.3617 (0.3756)	loss_scale 4096.0000 (3569.5840)	mem 17019MB
[2024-07-03 22:55:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:10:55 lr 0.000004	 wd 0.0500	time 0.6352 (0.5034)	loss 1.3830 (1.1749)	grad_norm 0.3455 (nan)	loss_scale 2048.0000 (3497.4588)	mem 17019MB
[2024-07-03 22:56:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:10:26 lr 0.000004	 wd 0.0500	time 0.5180 (0.5214)	loss 0.8606 (1.1687)	grad_norm 0.3326 (nan)	loss_scale 2048.0000 (3386.0477)	mem 17019MB
[2024-07-03 22:58:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:10:14 lr 0.000004	 wd 0.0500	time 0.9464 (0.5580)	loss 1.0148 (1.1632)	grad_norm 0.3531 (nan)	loss_scale 2048.0000 (3290.5410)	mem 17019MB
[2024-07-03 22:59:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:09:17 lr 0.000004	 wd 0.0500	time 0.3453 (0.5566)	loss 1.2076 (1.1628)	grad_norm 0.3291 (nan)	loss_scale 2048.0000 (3207.7602)	mem 17019MB
[2024-07-03 23:00:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:08:12 lr 0.000004	 wd 0.0500	time 0.3537 (0.5463)	loss 1.5333 (1.1629)	grad_norm 0.3472 (nan)	loss_scale 2048.0000 (3135.3204)	mem 17019MB
[2024-07-03 23:00:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:07:11 lr 0.000004	 wd 0.0500	time 0.3662 (0.5378)	loss 0.9072 (1.1611)	grad_norm 0.3538 (nan)	loss_scale 2048.0000 (3071.3980)	mem 17019MB
[2024-07-03 23:01:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:06:11 lr 0.000004	 wd 0.0500	time 0.3917 (0.5298)	loss 1.4250 (1.1597)	grad_norm 0.3329 (nan)	loss_scale 2048.0000 (3014.5741)	mem 17019MB
[2024-07-03 23:02:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:05:14 lr 0.000004	 wd 0.0500	time 0.3885 (0.5230)	loss 0.9460 (1.1605)	grad_norm 0.3437 (nan)	loss_scale 2048.0000 (2963.7286)	mem 17019MB
[2024-07-03 23:02:47 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:04:19 lr 0.000004	 wd 0.0500	time 0.3755 (0.5169)	loss 0.9234 (1.1615)	grad_norm 0.3356 (nan)	loss_scale 2048.0000 (2917.9650)	mem 17019MB
[2024-07-03 23:03:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:03:25 lr 0.000004	 wd 0.0500	time 0.3983 (0.5110)	loss 1.3019 (1.1630)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (2876.5578)	mem 17019MB
[2024-07-03 23:04:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:37 lr 0.000004	 wd 0.0500	time 0.5033 (0.5207)	loss 1.0908 (1.1629)	grad_norm 0.4374 (nan)	loss_scale 2048.0000 (2838.9132)	mem 17019MB
[2024-07-03 23:05:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:45 lr 0.000004	 wd 0.0500	time 0.8484 (0.5236)	loss 1.1426 (1.1630)	grad_norm 0.3416 (nan)	loss_scale 2048.0000 (2804.5406)	mem 17019MB
[2024-07-03 23:06:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:53 lr 0.000004	 wd 0.0500	time 0.4630 (0.5236)	loss 1.3514 (1.1640)	grad_norm 0.3652 (nan)	loss_scale 2048.0000 (2773.0312)	mem 17019MB
[2024-07-03 23:07:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.3644 (0.5192)	loss 1.1112 (1.1636)	grad_norm 0.3396 (nan)	loss_scale 2048.0000 (2744.0416)	mem 17019MB
[2024-07-03 23:07:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 22 training takes 0:22:04
[2024-07-03 23:08:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 78.353 (78.353)	Loss 0.5020 (0.5020)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 23:09:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.986 Acc@5 97.846
[2024-07-03 23:09:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-03 23:09:22 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 23:09:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][0/2502]	eta 12:00:45 lr 0.000004	 wd 0.0500	time 17.2845 (17.2845)	loss 0.7706 (0.7706)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:10:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:22:53 lr 0.000004	 wd 0.0500	time 0.4189 (0.5720)	loss 1.4824 (1.1535)	grad_norm 0.3852 (0.3653)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:10:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:18:26 lr 0.000004	 wd 0.0500	time 0.3804 (0.4808)	loss 0.8596 (1.1581)	grad_norm 0.3683 (0.3711)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:11:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:16:25 lr 0.000004	 wd 0.0500	time 0.3482 (0.4475)	loss 1.4233 (1.1492)	grad_norm 0.3599 (0.3745)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:12:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:15:08 lr 0.000004	 wd 0.0500	time 0.3684 (0.4324)	loss 0.9092 (1.1546)	grad_norm 0.3685 (0.3703)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:12:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:14:05 lr 0.000004	 wd 0.0500	time 0.3701 (0.4222)	loss 0.8185 (1.1577)	grad_norm 0.4321 (0.3696)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:13:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:13:13 lr 0.000004	 wd 0.0500	time 0.3760 (0.4174)	loss 1.4467 (1.1570)	grad_norm 0.3721 (0.3685)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:14:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:12:25 lr 0.000004	 wd 0.0500	time 0.3608 (0.4137)	loss 1.1354 (1.1604)	grad_norm 0.3614 (0.3682)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:14:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:11:38 lr 0.000003	 wd 0.0500	time 0.3688 (0.4106)	loss 1.4512 (1.1620)	grad_norm 0.3381 (0.3682)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:15:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:11:16 lr 0.000003	 wd 0.0500	time 0.4985 (0.4225)	loss 1.1503 (1.1625)	grad_norm 0.3454 (0.3669)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:16:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:10:49 lr 0.000003	 wd 0.0500	time 0.6468 (0.4324)	loss 1.1414 (1.1581)	grad_norm 0.3210 (0.3691)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:18:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:11:09 lr 0.000003	 wd 0.0500	time 0.6143 (0.4776)	loss 1.0380 (1.1609)	grad_norm 0.3660 (0.3706)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:19:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:10:34 lr 0.000003	 wd 0.0500	time 0.3753 (0.4872)	loss 1.4169 (1.1639)	grad_norm 0.3450 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:19:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:09:36 lr 0.000003	 wd 0.0500	time 0.3850 (0.4793)	loss 1.2606 (1.1646)	grad_norm 0.3421 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:20:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:08:41 lr 0.000003	 wd 0.0500	time 0.4266 (0.4736)	loss 1.2867 (1.1648)	grad_norm 0.3744 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:21:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:07:48 lr 0.000003	 wd 0.0500	time 0.3690 (0.4678)	loss 1.2993 (1.1632)	grad_norm 0.3532 (0.3694)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:21:43 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:06:57 lr 0.000003	 wd 0.0500	time 0.3806 (0.4628)	loss 1.1491 (1.1664)	grad_norm 0.3396 (0.3689)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:22:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:06:08 lr 0.000003	 wd 0.0500	time 0.4169 (0.4589)	loss 0.7243 (1.1638)	grad_norm 0.3581 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:23:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:05:19 lr 0.000003	 wd 0.0500	time 0.3900 (0.4554)	loss 1.3102 (1.1630)	grad_norm 0.4073 (0.3683)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:24:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:04:38 lr 0.000003	 wd 0.0500	time 0.4241 (0.4629)	loss 1.2960 (1.1623)	grad_norm 0.3401 (0.3677)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:24:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:03:54 lr 0.000003	 wd 0.0500	time 0.5127 (0.4668)	loss 0.9325 (1.1620)	grad_norm 0.3636 (0.3671)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:26:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:03:10 lr 0.000003	 wd 0.0500	time 0.6914 (0.4751)	loss 1.3904 (1.1623)	grad_norm 0.3520 (0.3668)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:27:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:27 lr 0.000003	 wd 0.0500	time 0.7331 (0.4878)	loss 1.3329 (1.1609)	grad_norm 0.3657 (0.3665)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:28:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:41 lr 0.000003	 wd 0.0500	time 0.3499 (0.5036)	loss 1.3691 (1.1609)	grad_norm 0.3432 (0.3669)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:29:19 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:50 lr 0.000003	 wd 0.0500	time 0.3672 (0.4987)	loss 0.8230 (1.1608)	grad_norm 0.3469 (0.3669)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:29:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.3546 (0.4936)	loss 0.8698 (1.1600)	grad_norm 0.3454 (0.3667)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:30:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 23 training takes 0:20:46
[2024-07-03 23:30:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 42.102 (42.102)	Loss 0.5186 (0.5186)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-07-03 23:31:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.974 Acc@5 97.842
[2024-07-03 23:31:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-03 23:31:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-03 23:31:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][0/2502]	eta 1 day, 2:38:23 lr 0.000003	 wd 0.0500	time 38.3307 (38.3307)	loss 1.2259 (1.2259)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:32:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:30:28 lr 0.000003	 wd 0.0500	time 0.3279 (0.7612)	loss 0.9543 (1.1672)	grad_norm 0.3396 (0.3554)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:33:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:22:13 lr 0.000003	 wd 0.0500	time 0.4178 (0.5791)	loss 1.2378 (1.1564)	grad_norm 0.3336 (0.3693)	loss_scale 4096.0000 (2781.6119)	mem 17019MB
[2024-07-03 23:33:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:18:59 lr 0.000003	 wd 0.0500	time 0.3637 (0.5176)	loss 1.3845 (1.1705)	grad_norm 0.3476 (0.3717)	loss_scale 4096.0000 (3218.2857)	mem 17019MB
[2024-07-03 23:34:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:16:57 lr 0.000003	 wd 0.0500	time 0.4449 (0.4842)	loss 0.7305 (1.1703)	grad_norm 0.3811 (0.3817)	loss_scale 4096.0000 (3437.1671)	mem 17019MB
[2024-07-03 23:35:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:15:31 lr 0.000003	 wd 0.0500	time 0.3820 (0.4651)	loss 1.4418 (1.1706)	grad_norm 0.3759 (0.3793)	loss_scale 4096.0000 (3568.6707)	mem 17019MB
[2024-07-03 23:35:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:14:18 lr 0.000003	 wd 0.0500	time 0.3416 (0.4516)	loss 0.6969 (1.1656)	grad_norm 0.3496 (0.3779)	loss_scale 4096.0000 (3656.4126)	mem 17019MB
[2024-07-03 23:36:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:13:16 lr 0.000003	 wd 0.0500	time 0.3644 (0.4420)	loss 1.2057 (1.1642)	grad_norm 0.3430 (0.3797)	loss_scale 4096.0000 (3719.1213)	mem 17019MB
[2024-07-03 23:37:04 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:12:21 lr 0.000003	 wd 0.0500	time 0.3515 (0.4359)	loss 1.2021 (1.1663)	grad_norm 0.3584 (0.3765)	loss_scale 4096.0000 (3766.1723)	mem 17019MB
[2024-07-03 23:37:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:11:28 lr 0.000003	 wd 0.0500	time 0.4200 (0.4298)	loss 0.9590 (1.1700)	grad_norm 0.3450 (0.3785)	loss_scale 4096.0000 (3802.7791)	mem 17019MB
[2024-07-03 23:38:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:11:09 lr 0.000003	 wd 0.0500	time 0.4059 (0.4458)	loss 0.8121 (1.1671)	grad_norm 0.3583 (nan)	loss_scale 2048.0000 (3737.9580)	mem 17019MB
[2024-07-03 23:39:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:10:37 lr 0.000003	 wd 0.0500	time 0.4742 (0.4549)	loss 0.8659 (1.1664)	grad_norm 0.3622 (nan)	loss_scale 2048.0000 (3584.4650)	mem 17019MB
[2024-07-03 23:40:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:10:09 lr 0.000002	 wd 0.0500	time 0.7127 (0.4679)	loss 0.9077 (1.1644)	grad_norm 0.3589 (nan)	loss_scale 2048.0000 (3456.5329)	mem 17019MB
[2024-07-03 23:41:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:09:38 lr 0.000002	 wd 0.0500	time 0.6460 (0.4816)	loss 0.7763 (1.1614)	grad_norm 0.3693 (nan)	loss_scale 2048.0000 (3348.2675)	mem 17019MB
[2024-07-03 23:42:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:08:59 lr 0.000002	 wd 0.0500	time 0.5027 (0.4891)	loss 1.4501 (1.1624)	grad_norm 0.3434 (nan)	loss_scale 2048.0000 (3255.4575)	mem 17019MB
[2024-07-03 23:43:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:08:18 lr 0.000002	 wd 0.0500	time 0.3641 (0.4979)	loss 1.2536 (1.1585)	grad_norm 0.3676 (nan)	loss_scale 2048.0000 (3175.0140)	mem 17019MB
[2024-07-03 23:44:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:07:30 lr 0.000002	 wd 0.0500	time 0.3642 (0.5000)	loss 1.2978 (1.1575)	grad_norm 0.3562 (nan)	loss_scale 2048.0000 (3104.6196)	mem 17019MB
[2024-07-03 23:45:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:06:48 lr 0.000002	 wd 0.0500	time 0.6808 (0.5090)	loss 1.5135 (1.1589)	grad_norm 0.4324 (nan)	loss_scale 2048.0000 (3042.5021)	mem 17019MB
[2024-07-03 23:46:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:06:04 lr 0.000002	 wd 0.0500	time 0.4538 (0.5195)	loss 1.0913 (1.1589)	grad_norm 0.3562 (nan)	loss_scale 2048.0000 (2987.2826)	mem 17019MB
[2024-07-03 23:47:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:05:13 lr 0.000002	 wd 0.0500	time 0.4401 (0.5208)	loss 0.9165 (1.1601)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (2937.8727)	mem 17019MB
[2024-07-03 23:49:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:04:27 lr 0.000002	 wd 0.0500	time 0.5470 (0.5329)	loss 1.2869 (1.1600)	grad_norm 0.3748 (nan)	loss_scale 2048.0000 (2893.4013)	mem 17019MB
[2024-07-03 23:50:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:03:36 lr 0.000002	 wd 0.0500	time 0.4395 (0.5378)	loss 1.0058 (1.1603)	grad_norm 0.8519 (nan)	loss_scale 2048.0000 (2853.1633)	mem 17019MB
[2024-07-03 23:50:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:42 lr 0.000002	 wd 0.0500	time 0.4198 (0.5367)	loss 1.0150 (1.1618)	grad_norm 0.3697 (nan)	loss_scale 2048.0000 (2816.5816)	mem 17019MB
[2024-07-03 23:51:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:48 lr 0.000002	 wd 0.0500	time 0.3728 (0.5384)	loss 1.3797 (1.1642)	grad_norm 0.3861 (nan)	loss_scale 2048.0000 (2783.1795)	mem 17019MB
[2024-07-03 23:52:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:54 lr 0.000002	 wd 0.0500	time 0.4263 (0.5372)	loss 1.2267 (1.1620)	grad_norm 0.3523 (nan)	loss_scale 2048.0000 (2752.5598)	mem 17019MB
[2024-07-03 23:53:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.3504 (0.5346)	loss 0.9466 (1.1626)	grad_norm 0.3334 (nan)	loss_scale 2048.0000 (2724.3886)	mem 17019MB
[2024-07-03 23:53:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 24 training takes 0:22:32
[2024-07-03 23:55:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 87.667 (87.667)	Loss 0.4998 (0.4998)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-03 23:55:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 86.022 Acc@5 97.826
[2024-07-03 23:55:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-03 23:55:45 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.02%
[2024-07-03 23:55:45 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-03 23:55:47 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 23:56:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][0/2502]	eta 1 day, 10:28:49 lr 0.000002	 wd 0.0500	time 49.6121 (49.6121)	loss 1.2783 (1.2783)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:57:15 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:34:57 lr 0.000002	 wd 0.0500	time 0.3376 (0.8733)	loss 1.2034 (1.2208)	grad_norm 0.3567 (0.3789)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:57:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:24:24 lr 0.000002	 wd 0.0500	time 0.3766 (0.6361)	loss 1.4386 (1.1898)	grad_norm 0.3513 (0.3949)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:58:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:20:19 lr 0.000002	 wd 0.0500	time 0.3776 (0.5539)	loss 1.1457 (1.1734)	grad_norm 0.3557 (0.3836)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:59:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:17:53 lr 0.000002	 wd 0.0500	time 0.3541 (0.5107)	loss 1.3083 (1.1744)	grad_norm 0.3401 (0.3786)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-03 23:59:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:16:18 lr 0.000002	 wd 0.0500	time 0.3518 (0.4886)	loss 1.4185 (1.1726)	grad_norm 0.3580 (0.3782)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:00:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:14:54 lr 0.000002	 wd 0.0500	time 0.3469 (0.4703)	loss 0.8255 (1.1625)	grad_norm 0.3813 (0.3743)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:01:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:13:46 lr 0.000002	 wd 0.0500	time 0.3616 (0.4586)	loss 0.8056 (1.1642)	grad_norm 0.3717 (0.3740)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:01:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:12:47 lr 0.000002	 wd 0.0500	time 0.3405 (0.4508)	loss 1.3636 (1.1631)	grad_norm 0.3888 (0.3726)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:02:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:11:50 lr 0.000002	 wd 0.0500	time 0.3710 (0.4436)	loss 1.4445 (1.1630)	grad_norm 0.3513 (0.3710)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:03:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:11:15 lr 0.000002	 wd 0.0500	time 0.3985 (0.4500)	loss 1.5215 (1.1645)	grad_norm 0.3610 (0.3696)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:04:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:10:43 lr 0.000002	 wd 0.0500	time 0.4109 (0.4586)	loss 1.5540 (1.1653)	grad_norm 0.3705 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:06:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:11:06 lr 0.000002	 wd 0.0500	time 0.9966 (0.5122)	loss 0.9844 (1.1618)	grad_norm 0.3411 (0.3693)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:06:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:10:17 lr 0.000002	 wd 0.0500	time 0.3717 (0.5136)	loss 0.8318 (1.1607)	grad_norm 0.3697 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:07:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:09:16 lr 0.000002	 wd 0.0500	time 0.3543 (0.5047)	loss 0.8105 (1.1602)	grad_norm 0.3650 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:08:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:08:18 lr 0.000002	 wd 0.0500	time 0.4422 (0.4972)	loss 0.8625 (1.1604)	grad_norm 0.3591 (0.3686)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:08:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:07:22 lr 0.000002	 wd 0.0500	time 0.3651 (0.4905)	loss 0.8139 (1.1596)	grad_norm 0.3403 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:09:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:06:29 lr 0.000002	 wd 0.0500	time 0.3588 (0.4858)	loss 0.7586 (1.1573)	grad_norm 0.3532 (0.3689)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:10:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:05:37 lr 0.000002	 wd 0.0500	time 0.3798 (0.4806)	loss 0.8896 (1.1592)	grad_norm 0.3705 (0.3686)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:10:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:04:46 lr 0.000002	 wd 0.0500	time 0.3853 (0.4756)	loss 1.6515 (1.1616)	grad_norm 0.3475 (0.3684)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:12:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:04:15 lr 0.000002	 wd 0.0500	time 1.9209 (0.5083)	loss 0.9899 (1.1621)	grad_norm 0.3474 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:13:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:03:23 lr 0.000002	 wd 0.0500	time 0.4493 (0.5060)	loss 1.2847 (1.1633)	grad_norm 0.3398 (0.3685)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:14:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:31 lr 0.000001	 wd 0.0500	time 0.3609 (0.5006)	loss 1.1114 (1.1636)	grad_norm 0.3503 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:14:50 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:40 lr 0.000001	 wd 0.0500	time 0.4368 (0.4967)	loss 1.3482 (1.1630)	grad_norm 0.3450 (0.3686)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:15:29 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:50 lr 0.000001	 wd 0.0500	time 0.4135 (0.4924)	loss 1.3621 (1.1621)	grad_norm 0.3190 (0.3690)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:16:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.3367 (0.4873)	loss 0.7493 (1.1609)	grad_norm 0.3909 (0.3691)	loss_scale 4096.0000 (2087.3059)	mem 17019MB
[2024-07-04 00:16:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 25 training takes 0:20:34
[2024-07-04 00:16:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 33.732 (33.732)	Loss 0.5244 (0.5244)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-04 00:17:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.946 Acc@5 97.846
[2024-07-04 00:17:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-04 00:17:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.02%
[2024-07-04 00:18:01 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][0/2502]	eta 1 day, 4:08:15 lr 0.000001	 wd 0.0500	time 40.4858 (40.4858)	loss 1.1769 (1.1769)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 00:18:40 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:31:20 lr 0.000001	 wd 0.0500	time 0.3400 (0.7830)	loss 1.3141 (1.1735)	grad_norm 0.3415 (0.3656)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 00:19:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:22:38 lr 0.000001	 wd 0.0500	time 0.3620 (0.5903)	loss 1.0877 (1.1727)	grad_norm 0.3632 (0.3714)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 00:19:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:19:11 lr 0.000001	 wd 0.0500	time 0.3651 (0.5229)	loss 0.9172 (1.1549)	grad_norm 0.3430 (0.3669)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 00:20:37 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:17:07 lr 0.000001	 wd 0.0500	time 0.3782 (0.4888)	loss 1.3559 (1.1533)	grad_norm 0.3666 (0.3724)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 00:21:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:15:42 lr 0.000001	 wd 0.0500	time 0.3982 (0.4709)	loss 1.3202 (1.1622)	grad_norm 0.3495 (nan)	loss_scale 2048.0000 (3768.9741)	mem 17019MB
[2024-07-04 00:21:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:14:28 lr 0.000001	 wd 0.0500	time 0.3916 (0.4565)	loss 0.7907 (1.1662)	grad_norm 0.3574 (nan)	loss_scale 2048.0000 (3482.6223)	mem 17019MB
[2024-07-04 00:22:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:13:25 lr 0.000001	 wd 0.0500	time 0.3771 (0.4468)	loss 1.4133 (1.1623)	grad_norm 0.3377 (nan)	loss_scale 2048.0000 (3277.9686)	mem 17019MB
[2024-07-04 00:23:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:12:29 lr 0.000001	 wd 0.0500	time 0.3341 (0.4404)	loss 1.2502 (1.1662)	grad_norm 0.3654 (nan)	loss_scale 2048.0000 (3124.4145)	mem 17019MB
[2024-07-04 00:23:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:11:35 lr 0.000001	 wd 0.0500	time 0.4014 (0.4339)	loss 1.2765 (1.1653)	grad_norm 0.3687 (nan)	loss_scale 2048.0000 (3004.9456)	mem 17019MB
[2024-07-04 00:24:41 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:11:00 lr 0.000001	 wd 0.0500	time 0.4474 (0.4401)	loss 1.1141 (1.1642)	grad_norm 0.3502 (nan)	loss_scale 2048.0000 (2909.3467)	mem 17019MB
[2024-07-04 00:25:35 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:10:29 lr 0.000001	 wd 0.0500	time 0.4905 (0.4489)	loss 0.9381 (1.1642)	grad_norm 0.3486 (nan)	loss_scale 2048.0000 (2831.1135)	mem 17019MB
[2024-07-04 00:26:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:10:20 lr 0.000001	 wd 0.0500	time 0.6228 (0.4767)	loss 1.0804 (1.1659)	grad_norm 0.3522 (nan)	loss_scale 2048.0000 (2765.9084)	mem 17019MB
[2024-07-04 00:28:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:10:05 lr 0.000001	 wd 0.0500	time 0.9113 (0.5039)	loss 1.2828 (1.1674)	grad_norm 0.4582 (nan)	loss_scale 2048.0000 (2710.7271)	mem 17019MB
[2024-07-04 00:29:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:09:38 lr 0.000001	 wd 0.0500	time 0.3628 (0.5246)	loss 1.3825 (1.1683)	grad_norm 0.3586 (nan)	loss_scale 2048.0000 (2663.4233)	mem 17019MB
[2024-07-04 00:30:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:08:36 lr 0.000001	 wd 0.0500	time 0.3676 (0.5153)	loss 1.4177 (1.1700)	grad_norm 0.3467 (nan)	loss_scale 2048.0000 (2622.4224)	mem 17019MB
[2024-07-04 00:30:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:07:38 lr 0.000001	 wd 0.0500	time 0.3800 (0.5080)	loss 1.2093 (1.1673)	grad_norm 0.3488 (nan)	loss_scale 2048.0000 (2586.5434)	mem 17019MB
[2024-07-04 00:31:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:06:41 lr 0.000001	 wd 0.0500	time 0.3883 (0.5007)	loss 1.3044 (1.1655)	grad_norm 0.3483 (nan)	loss_scale 2048.0000 (2554.8830)	mem 17019MB
[2024-07-04 00:32:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:05:47 lr 0.000001	 wd 0.0500	time 0.3921 (0.4949)	loss 0.8914 (1.1655)	grad_norm 0.3717 (nan)	loss_scale 2048.0000 (2526.7385)	mem 17019MB
[2024-07-04 00:32:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:04:55 lr 0.000001	 wd 0.0500	time 0.3385 (0.4906)	loss 1.3028 (1.1666)	grad_norm 0.3649 (nan)	loss_scale 2048.0000 (2501.5550)	mem 17019MB
[2024-07-04 00:33:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:04:03 lr 0.000001	 wd 0.0500	time 0.3792 (0.4858)	loss 1.1906 (1.1651)	grad_norm 0.3605 (nan)	loss_scale 2048.0000 (2478.8886)	mem 17019MB
[2024-07-04 00:34:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:03:16 lr 0.000001	 wd 0.0500	time 0.5423 (0.4881)	loss 1.3700 (1.1640)	grad_norm 0.3879 (nan)	loss_scale 2048.0000 (2458.3798)	mem 17019MB
[2024-07-04 00:35:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:28 lr 0.000001	 wd 0.0500	time 0.5098 (0.4904)	loss 1.2902 (1.1624)	grad_norm 0.3536 (nan)	loss_scale 2048.0000 (2439.7347)	mem 17019MB
[2024-07-04 00:36:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:40 lr 0.000001	 wd 0.0500	time 0.5986 (0.4980)	loss 1.2908 (1.1626)	grad_norm 0.3350 (nan)	loss_scale 2048.0000 (2422.7101)	mem 17019MB
[2024-07-04 00:37:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:50 lr 0.000001	 wd 0.0500	time 0.5659 (0.5000)	loss 0.7206 (1.1631)	grad_norm 0.3602 (nan)	loss_scale 2048.0000 (2407.1037)	mem 17019MB
[2024-07-04 00:38:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.3545 (0.4999)	loss 0.9406 (1.1636)	grad_norm 0.3653 (nan)	loss_scale 2048.0000 (2392.7453)	mem 17019MB
[2024-07-04 00:38:26 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 26 training takes 0:21:05
[2024-07-04 00:40:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 106.076 (106.076)	Loss 0.5063 (0.5063)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-04 00:40:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 86.006 Acc@5 97.842
[2024-07-04 00:40:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-04 00:40:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.02%
[2024-07-04 00:41:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][0/2502]	eta 1 day, 2:05:45 lr 0.000001	 wd 0.0500	time 37.5482 (37.5482)	loss 1.4087 (1.4087)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:41:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:30:15 lr 0.000001	 wd 0.0500	time 0.3538 (0.7559)	loss 1.2835 (1.1543)	grad_norm 0.3236 (0.3569)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:42:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:22:13 lr 0.000001	 wd 0.0500	time 0.4171 (0.5794)	loss 1.3897 (1.1665)	grad_norm 0.3553 (0.3584)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:43:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:18:50 lr 0.000001	 wd 0.0500	time 0.3929 (0.5135)	loss 1.3546 (1.1659)	grad_norm 0.3520 (0.3599)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:43:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:16:53 lr 0.000001	 wd 0.0500	time 0.4112 (0.4823)	loss 1.0012 (1.1624)	grad_norm 0.3544 (0.3620)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:44:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:15:29 lr 0.000001	 wd 0.0500	time 0.3514 (0.4641)	loss 0.6786 (1.1635)	grad_norm 0.3576 (0.3616)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:45:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:14:15 lr 0.000001	 wd 0.0500	time 0.3658 (0.4499)	loss 1.3193 (1.1633)	grad_norm 0.3640 (0.3630)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:45:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:13:18 lr 0.000001	 wd 0.0500	time 0.3865 (0.4429)	loss 0.9837 (1.1650)	grad_norm 0.3834 (0.3624)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:46:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:12:21 lr 0.000001	 wd 0.0500	time 0.3456 (0.4358)	loss 0.9330 (1.1666)	grad_norm 0.3643 (0.3639)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:47:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:11:29 lr 0.000001	 wd 0.0500	time 0.4181 (0.4303)	loss 1.2469 (1.1647)	grad_norm 0.3386 (0.3632)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:48:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:11:12 lr 0.000001	 wd 0.0500	time 0.6309 (0.4478)	loss 1.1197 (1.1614)	grad_norm 0.3234 (0.3661)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:49:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:11:08 lr 0.000001	 wd 0.0500	time 0.6566 (0.4767)	loss 1.1415 (1.1635)	grad_norm 0.3498 (0.3682)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:50:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:11:07 lr 0.000001	 wd 0.0500	time 0.3480 (0.5130)	loss 0.7225 (1.1641)	grad_norm 0.3442 (0.3681)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:51:36 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:10:04 lr 0.000001	 wd 0.0500	time 0.3747 (0.5029)	loss 0.8112 (1.1609)	grad_norm 0.3618 (0.3681)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:52:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:09:05 lr 0.000001	 wd 0.0500	time 0.3781 (0.4952)	loss 0.8333 (1.1621)	grad_norm 0.3424 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:52:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:08:08 lr 0.000001	 wd 0.0500	time 0.3645 (0.4874)	loss 1.4586 (1.1631)	grad_norm 0.3422 (0.3666)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:53:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:07:14 lr 0.000001	 wd 0.0500	time 0.3695 (0.4813)	loss 1.3260 (1.1648)	grad_norm 0.3601 (0.3678)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:54:13 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:06:22 lr 0.000001	 wd 0.0500	time 0.3638 (0.4765)	loss 1.3482 (1.1665)	grad_norm 0.3546 (0.3725)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:54:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:05:30 lr 0.000001	 wd 0.0500	time 0.3837 (0.4714)	loss 1.4119 (1.1661)	grad_norm 0.3447 (0.3718)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:55:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:04:42 lr 0.000001	 wd 0.0500	time 0.9647 (0.4691)	loss 1.3783 (1.1669)	grad_norm 0.3579 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 00:56:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:03:57 lr 0.000001	 wd 0.0500	time 0.6026 (0.4723)	loss 1.4092 (1.1677)	grad_norm 0.3395 (0.3717)	loss_scale 4096.0000 (2131.9260)	mem 17019MB
[2024-07-04 00:57:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:13 lr 0.000001	 wd 0.0500	time 0.4740 (0.4803)	loss 1.5002 (1.1668)	grad_norm 0.3642 (0.3712)	loss_scale 4096.0000 (2225.4089)	mem 17019MB
[2024-07-04 00:58:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:25 lr 0.000001	 wd 0.0500	time 0.4990 (0.4827)	loss 1.1898 (1.1681)	grad_norm 0.3574 (0.3706)	loss_scale 4096.0000 (2310.3971)	mem 17019MB
[2024-07-04 00:59:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:39 lr 0.000001	 wd 0.0500	time 0.5952 (0.4940)	loss 1.3061 (1.1678)	grad_norm 0.3452 (0.3709)	loss_scale 4096.0000 (2387.9983)	mem 17019MB
[2024-07-04 01:00:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:51 lr 0.000001	 wd 0.0500	time 1.1438 (0.5023)	loss 1.3501 (1.1694)	grad_norm 0.3805 (0.3708)	loss_scale 4096.0000 (2459.1354)	mem 17019MB
[2024-07-04 01:02:05 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.3458 (0.5130)	loss 1.2759 (1.1685)	grad_norm 0.3616 (0.3710)	loss_scale 4096.0000 (2524.5838)	mem 17019MB
[2024-07-04 01:02:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 27 training takes 0:21:38
[2024-07-04 01:03:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 39.386 (39.386)	Loss 0.4944 (0.4944)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-04 01:03:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 86.026 Acc@5 97.840
[2024-07-04 01:03:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-04 01:03:25 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.03%
[2024-07-04 01:03:25 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-04 01:03:26 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-04 01:04:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][0/2502]	eta 1 day, 5:42:44 lr 0.000001	 wd 0.0500	time 42.7515 (42.7515)	loss 0.9614 (0.9614)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:04:48 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:32:13 lr 0.000000	 wd 0.0500	time 0.3939 (0.8048)	loss 0.9089 (1.2037)	grad_norm 0.3691 (0.3639)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:05:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:23:04 lr 0.000000	 wd 0.0500	time 0.3809 (0.6016)	loss 1.5443 (1.1902)	grad_norm 0.3467 (0.3627)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:06:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:19:24 lr 0.000000	 wd 0.0500	time 0.3446 (0.5286)	loss 1.4399 (1.1839)	grad_norm 0.3816 (0.3664)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:06:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:17:14 lr 0.000000	 wd 0.0500	time 0.3492 (0.4920)	loss 0.7363 (1.1755)	grad_norm 0.3445 (0.3661)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:07:24 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:15:47 lr 0.000000	 wd 0.0500	time 0.3944 (0.4730)	loss 0.8033 (1.1806)	grad_norm 0.3568 (0.3763)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:08:02 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:14:32 lr 0.000000	 wd 0.0500	time 0.3740 (0.4590)	loss 0.8892 (1.1732)	grad_norm 0.3942 (0.3753)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:08:42 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:13:31 lr 0.000000	 wd 0.0500	time 0.3462 (0.4504)	loss 1.2865 (1.1701)	grad_norm 0.3630 (0.3745)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:09:21 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:12:32 lr 0.000000	 wd 0.0500	time 0.3838 (0.4422)	loss 0.8515 (1.1705)	grad_norm 0.3617 (0.3747)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:09:59 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:11:38 lr 0.000000	 wd 0.0500	time 0.3904 (0.4358)	loss 0.8649 (1.1671)	grad_norm 0.3646 (0.3735)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:10:51 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:11:06 lr 0.000000	 wd 0.0500	time 0.3952 (0.4437)	loss 1.0124 (1.1687)	grad_norm 0.3578 (0.3742)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:11:44 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:10:32 lr 0.000000	 wd 0.0500	time 0.4060 (0.4515)	loss 1.2640 (1.1688)	grad_norm 0.3573 (0.3726)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:13:07 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:10:28 lr 0.000000	 wd 0.0500	time 0.4483 (0.4830)	loss 1.4395 (1.1688)	grad_norm 0.3793 (0.3721)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:14:06 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:09:51 lr 0.000000	 wd 0.0500	time 0.4130 (0.4918)	loss 1.5215 (1.1701)	grad_norm 0.3570 (0.3713)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:14:58 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:09:04 lr 0.000000	 wd 0.0500	time 0.4172 (0.4939)	loss 0.9598 (1.1696)	grad_norm 0.3471 (0.3705)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:16:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:08:34 lr 0.000000	 wd 0.0500	time 0.5069 (0.5134)	loss 1.1897 (1.1680)	grad_norm 0.3502 (0.3700)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:17:18 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:07:48 lr 0.000000	 wd 0.0500	time 0.5036 (0.5193)	loss 1.4008 (1.1708)	grad_norm 0.3830 (0.3694)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:18:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:06:57 lr 0.000000	 wd 0.0500	time 0.6086 (0.5208)	loss 1.2473 (1.1703)	grad_norm 0.3623 (0.3688)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:19:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:06:10 lr 0.000000	 wd 0.0500	time 0.4306 (0.5274)	loss 0.7932 (1.1698)	grad_norm 0.3501 (0.3681)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-07-04 01:20:10 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:05:17 lr 0.000000	 wd 0.0500	time 0.7587 (0.5279)	loss 1.0980 (1.1674)	grad_norm 0.3508 (nan)	loss_scale 2048.0000 (4011.9684)	mem 17019MB
[2024-07-04 01:21:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:04:27 lr 0.000000	 wd 0.0500	time 0.4923 (0.5336)	loss 1.1140 (1.1675)	grad_norm 0.3935 (nan)	loss_scale 2048.0000 (3913.8191)	mem 17019MB
[2024-07-04 01:22:17 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:03:36 lr 0.000000	 wd 0.0500	time 0.4185 (0.5379)	loss 0.9740 (1.1674)	grad_norm 0.4133 (nan)	loss_scale 2048.0000 (3825.0129)	mem 17019MB
[2024-07-04 01:23:08 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:42 lr 0.000000	 wd 0.0500	time 0.4933 (0.5370)	loss 1.2781 (1.1665)	grad_norm 0.3654 (nan)	loss_scale 2048.0000 (3744.2762)	mem 17019MB
[2024-07-04 01:24:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:50 lr 0.000000	 wd 0.0500	time 0.5142 (0.5484)	loss 1.2483 (1.1664)	grad_norm 0.3824 (nan)	loss_scale 2048.0000 (3670.5571)	mem 17019MB
[2024-07-04 01:25:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:56 lr 0.000000	 wd 0.0500	time 0.5179 (0.5519)	loss 1.5165 (1.1651)	grad_norm 0.3336 (nan)	loss_scale 2048.0000 (3602.9788)	mem 17019MB
[2024-07-04 01:26:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.3491 (0.5468)	loss 1.0855 (1.1643)	grad_norm 0.3319 (nan)	loss_scale 2048.0000 (3540.8045)	mem 17019MB
[2024-07-04 01:26:27 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 28 training takes 0:23:00
[2024-07-04 01:28:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 101.102 (101.102)	Loss 0.5020 (0.5020)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-04 01:28:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 85.990 Acc@5 97.842
[2024-07-04 01:28:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-04 01:28:55 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.03%
[2024-07-04 01:29:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][0/2502]	eta 17:14:45 lr 0.000000	 wd 0.0500	time 24.8145 (24.8145)	loss 1.2429 (1.2429)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:30:00 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:25:36 lr 0.000000	 wd 0.0500	time 0.4007 (0.6397)	loss 1.4188 (1.1758)	grad_norm 0.3648 (0.3840)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:30:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:19:42 lr 0.000000	 wd 0.0500	time 0.3528 (0.5137)	loss 0.8231 (1.1744)	grad_norm 0.3848 (0.3802)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:31:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:17:12 lr 0.000000	 wd 0.0500	time 0.3755 (0.4688)	loss 0.8775 (1.1709)	grad_norm 0.3610 (0.3732)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:31:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:15:45 lr 0.000000	 wd 0.0500	time 0.4349 (0.4499)	loss 1.1260 (1.1645)	grad_norm 0.3594 (0.3773)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:32:34 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:14:34 lr 0.000000	 wd 0.0500	time 0.3898 (0.4367)	loss 1.3073 (1.1601)	grad_norm 0.3616 (0.3737)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:33:12 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:13:33 lr 0.000000	 wd 0.0500	time 0.3852 (0.4278)	loss 0.7232 (1.1633)	grad_norm 0.3583 (0.3745)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:33:53 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:12:45 lr 0.000000	 wd 0.0500	time 0.3439 (0.4251)	loss 1.1252 (1.1639)	grad_norm 0.3695 (0.3772)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:34:32 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:11:55 lr 0.000000	 wd 0.0500	time 0.3698 (0.4207)	loss 0.7758 (1.1648)	grad_norm 0.3420 (0.3758)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:35:23 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:11:30 lr 0.000000	 wd 0.0500	time 0.4417 (0.4308)	loss 1.4951 (1.1661)	grad_norm 0.3651 (0.3744)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:36:16 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:11:01 lr 0.000000	 wd 0.0500	time 0.5062 (0.4407)	loss 1.4488 (1.1668)	grad_norm 0.3745 (0.3742)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:37:56 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:11:28 lr 0.000000	 wd 0.0500	time 0.6569 (0.4908)	loss 1.2463 (1.1667)	grad_norm 0.3492 (0.3728)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:38:54 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:10:49 lr 0.000000	 wd 0.0500	time 0.3533 (0.4988)	loss 1.0230 (1.1645)	grad_norm 0.3508 (0.3713)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:39:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:09:49 lr 0.000000	 wd 0.0500	time 0.3573 (0.4906)	loss 1.1372 (1.1679)	grad_norm 0.3879 (0.3748)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:40:14 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:08:53 lr 0.000000	 wd 0.0500	time 0.3580 (0.4843)	loss 1.1238 (1.1629)	grad_norm 0.3704 (0.3744)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:40:52 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:07:58 lr 0.000000	 wd 0.0500	time 0.3878 (0.4777)	loss 1.0242 (1.1632)	grad_norm 0.3559 (0.3789)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:41:31 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:07:05 lr 0.000000	 wd 0.0500	time 0.3612 (0.4721)	loss 0.8241 (1.1652)	grad_norm 0.3452 (0.3777)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:42:11 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:06:15 lr 0.000000	 wd 0.0500	time 0.3925 (0.4676)	loss 1.3590 (1.1679)	grad_norm 0.8373 (0.3773)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:42:49 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:05:25 lr 0.000000	 wd 0.0500	time 0.3744 (0.4632)	loss 1.1822 (1.1663)	grad_norm 0.3353 (0.3764)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:43:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:04:39 lr 0.000000	 wd 0.0500	time 0.5906 (0.4647)	loss 0.7635 (1.1666)	grad_norm 0.3609 (0.3766)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:44:30 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:03:54 lr 0.000000	 wd 0.0500	time 0.5240 (0.4672)	loss 0.9186 (1.1683)	grad_norm 0.3544 (0.3759)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:45:28 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:03:10 lr 0.000000	 wd 0.0500	time 0.4350 (0.4727)	loss 0.7529 (1.1692)	grad_norm 0.3247 (0.3800)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:46:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:23 lr 0.000000	 wd 0.0500	time 0.5498 (0.4747)	loss 0.8397 (1.1698)	grad_norm 0.3637 (0.3790)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:47:39 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:38 lr 0.000000	 wd 0.0500	time 0.4378 (0.4884)	loss 0.8608 (1.1668)	grad_norm 0.3701 (0.3790)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:48:38 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:50 lr 0.000000	 wd 0.0500	time 0.4295 (0.4925)	loss 1.2696 (1.1669)	grad_norm 0.3629 (0.3791)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:49:20 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.3586 (0.4897)	loss 1.3183 (1.1662)	grad_norm 0.3421 (0.3784)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-07-04 01:49:33 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 29 training takes 0:20:38
[2024-07-04 01:49:33 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_29.pth saving......
[2024-07-04 01:49:34 adapter_smt_large_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_smt_large_224_22kto1k_efficient_finetune/adapter_smt_l_22kto1k_efficient_finetune/ckpt_epoch_29.pth saved !!!
[2024-07-04 01:51:09 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 94.511 (94.511)	Loss 0.4917 (0.4917)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-07-04 01:51:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 86.006 Acc@5 97.858
[2024-07-04 01:51:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-04 01:51:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 86.03%
[2024-07-04 01:51:46 adapter_smt_large_224_22kto1k_efficient_finetune] (main.py 189): INFO Training time 11:38:44
