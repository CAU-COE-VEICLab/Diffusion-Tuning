[2024-08-03 09:05:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/config.json
[2024-08-03 09:05:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_step_cross2
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-03 09:05:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_step_cross2.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_step_cross2", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-03 09:05:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft
[2024-08-03 09:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-03 09:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 113): INFO number of params: 4162792
[2024-08-03 09:05:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2, ignoring auto resume
[2024-08-03 09:05:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth for fine-tuning......
[2024-08-03 09:05:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-03 09:05:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth'
[2024-08-03 09:05:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 15.474 (15.474)	Loss 0.5063 (0.5063)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2339MB
[2024-08-03 09:06:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.088 Acc@5 97.844
[2024-08-03 09:06:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 09:06:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 168): INFO Start training
[2024-08-03 09:06:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:38:07 lr 0.000000	 wd 0.0500	time 12.4251 (12.4251)	loss 1.5269 (1.5269)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17019MB
[2024-08-03 09:06:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:18:25 lr 0.000000	 wd 0.0500	time 0.3221 (0.4601)	loss 1.2719 (1.1994)	grad_norm 0.3668 (nan)	loss_scale 16384.0000 (24332.6733)	mem 17019MB
[2024-08-03 09:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:15:08 lr 0.000000	 wd 0.0500	time 0.2949 (0.3948)	loss 1.0169 (1.1839)	grad_norm 0.3575 (nan)	loss_scale 8192.0000 (16628.5373)	mem 17019MB
[2024-08-03 09:08:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:44 lr 0.000000	 wd 0.0500	time 0.3126 (0.3746)	loss 0.8959 (1.1477)	grad_norm 0.3709 (nan)	loss_scale 8192.0000 (13825.7010)	mem 17019MB
[2024-08-03 09:08:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:12:43 lr 0.000001	 wd 0.0500	time 0.3172 (0.3633)	loss 0.9647 (1.1552)	grad_norm 0.3712 (nan)	loss_scale 8192.0000 (12420.7880)	mem 17019MB
[2024-08-03 09:09:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:54 lr 0.000001	 wd 0.0500	time 0.3325 (0.3571)	loss 1.1422 (1.1600)	grad_norm 0.3401 (nan)	loss_scale 8192.0000 (11576.7186)	mem 17019MB
[2024-08-03 09:09:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:11:11 lr 0.000001	 wd 0.0500	time 0.3381 (0.3532)	loss 1.3122 (1.1599)	grad_norm 0.3543 (nan)	loss_scale 8192.0000 (11013.5374)	mem 17019MB
[2024-08-03 09:10:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:10:31 lr 0.000001	 wd 0.0500	time 0.2953 (0.3502)	loss 1.2244 (1.1618)	grad_norm 0.3917 (nan)	loss_scale 8192.0000 (10611.0357)	mem 17019MB
[2024-08-03 09:10:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:52 lr 0.000001	 wd 0.0500	time 0.3338 (0.3484)	loss 1.0507 (1.1610)	grad_norm 0.3467 (nan)	loss_scale 8192.0000 (10309.0337)	mem 17019MB
[2024-08-03 09:11:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:09:15 lr 0.000001	 wd 0.0500	time 0.3014 (0.3465)	loss 1.3507 (1.1607)	grad_norm 0.3680 (nan)	loss_scale 8192.0000 (10074.0688)	mem 17019MB
[2024-08-03 09:11:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:38 lr 0.000002	 wd 0.0500	time 0.3024 (0.3452)	loss 1.4224 (1.1584)	grad_norm 0.3632 (nan)	loss_scale 8192.0000 (9886.0500)	mem 17019MB
[2024-08-03 09:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:08:01 lr 0.000002	 wd 0.0500	time 0.3079 (0.3436)	loss 1.2234 (1.1594)	grad_norm 0.3864 (nan)	loss_scale 8192.0000 (9732.1853)	mem 17019MB
[2024-08-03 09:13:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:25 lr 0.000002	 wd 0.0500	time 0.3474 (0.3424)	loss 0.9162 (1.1591)	grad_norm 0.3552 (nan)	loss_scale 8192.0000 (9603.9434)	mem 17019MB
[2024-08-03 09:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:50 lr 0.000002	 wd 0.0500	time 0.3327 (0.3419)	loss 1.2312 (1.1609)	grad_norm 0.3703 (nan)	loss_scale 8192.0000 (9495.4158)	mem 17019MB
[2024-08-03 09:14:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:15 lr 0.000002	 wd 0.0500	time 0.3111 (0.3408)	loss 1.4234 (1.1627)	grad_norm 0.3536 (nan)	loss_scale 8192.0000 (9402.3812)	mem 17019MB
[2024-08-03 09:14:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:41 lr 0.000002	 wd 0.0500	time 0.2986 (0.3405)	loss 1.1751 (1.1666)	grad_norm 0.3653 (nan)	loss_scale 8192.0000 (9321.7428)	mem 17019MB
[2024-08-03 09:15:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:05:06 lr 0.000003	 wd 0.0500	time 0.3397 (0.3397)	loss 1.2122 (1.1638)	grad_norm 0.3552 (nan)	loss_scale 8192.0000 (9251.1780)	mem 17019MB
[2024-08-03 09:15:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:32 lr 0.000003	 wd 0.0500	time 0.3591 (0.3394)	loss 0.8801 (1.1632)	grad_norm 0.3945 (nan)	loss_scale 8192.0000 (9188.9101)	mem 17019MB
[2024-08-03 09:16:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:57 lr 0.000003	 wd 0.0500	time 0.2992 (0.3390)	loss 1.1150 (1.1633)	grad_norm 0.3583 (nan)	loss_scale 4096.0000 (9106.2654)	mem 17019MB
[2024-08-03 09:16:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:24 lr 0.000003	 wd 0.0500	time 0.3138 (0.3397)	loss 1.4066 (1.1628)	grad_norm 0.3556 (nan)	loss_scale 4096.0000 (8842.7059)	mem 17019MB
[2024-08-03 09:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:50 lr 0.000003	 wd 0.0500	time 0.3331 (0.3393)	loss 0.8381 (1.1618)	grad_norm 0.3653 (nan)	loss_scale 4096.0000 (8605.4893)	mem 17019MB
[2024-08-03 09:18:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:16 lr 0.000003	 wd 0.0500	time 0.3267 (0.3386)	loss 1.0643 (1.1607)	grad_norm 0.3656 (nan)	loss_scale 4096.0000 (8390.8539)	mem 17019MB
[2024-08-03 09:18:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:42 lr 0.000004	 wd 0.0500	time 0.4086 (0.3386)	loss 1.5179 (1.1603)	grad_norm 0.3795 (nan)	loss_scale 4096.0000 (8195.7219)	mem 17019MB
[2024-08-03 09:19:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:08 lr 0.000004	 wd 0.0500	time 0.3567 (0.3384)	loss 1.3200 (1.1603)	grad_norm 0.3942 (nan)	loss_scale 4096.0000 (8017.5506)	mem 17019MB
[2024-08-03 09:19:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:34 lr 0.000004	 wd 0.0500	time 0.3330 (0.3383)	loss 1.2995 (1.1603)	grad_norm 0.3704 (nan)	loss_scale 4096.0000 (7854.2207)	mem 17019MB
[2024-08-03 09:20:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2912 (0.3375)	loss 0.8737 (1.1585)	grad_norm 0.3653 (nan)	loss_scale 4096.0000 (7703.9520)	mem 17019MB
[2024-08-03 09:20:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 0 training takes 0:14:07
[2024-08-03 09:20:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_0.pth saving......
[2024-08-03 09:20:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_0.pth saved !!!
[2024-08-03 09:20:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.512 (12.512)	Loss 0.5234 (0.5234)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 09:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.078 Acc@5 97.856
[2024-08-03 09:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 09:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.08%
[2024-08-03 09:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 09:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 09:20:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:37:43 lr 0.000004	 wd 0.0500	time 10.9766 (10.9766)	loss 1.1011 (1.1011)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:21:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:17:49 lr 0.000004	 wd 0.0500	time 0.3299 (0.4452)	loss 1.1663 (1.1409)	grad_norm 0.3602 (0.3653)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:22:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:14:50 lr 0.000004	 wd 0.0500	time 0.2937 (0.3870)	loss 0.7656 (1.1578)	grad_norm 0.3643 (0.3687)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:22:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:13:28 lr 0.000004	 wd 0.0500	time 0.3127 (0.3674)	loss 0.8819 (1.1574)	grad_norm 0.3606 (0.3725)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:23:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:32 lr 0.000005	 wd 0.0500	time 0.3186 (0.3578)	loss 0.9682 (1.1721)	grad_norm 0.3795 (0.3715)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:23:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:11:45 lr 0.000005	 wd 0.0500	time 0.3013 (0.3522)	loss 1.4417 (1.1693)	grad_norm 0.3525 (0.3716)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:24:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:11:04 lr 0.000005	 wd 0.0500	time 0.3156 (0.3493)	loss 1.4749 (1.1680)	grad_norm 0.3568 (0.3730)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:10:23 lr 0.000005	 wd 0.0500	time 0.3018 (0.3462)	loss 0.8700 (1.1650)	grad_norm 0.3737 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:25:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:45 lr 0.000005	 wd 0.0500	time 0.3059 (0.3440)	loss 1.4053 (1.1641)	grad_norm 0.3448 (0.3773)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:25:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:09:10 lr 0.000005	 wd 0.0500	time 0.3232 (0.3435)	loss 1.1611 (1.1631)	grad_norm 0.3825 (0.3768)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:26:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:33 lr 0.000006	 wd 0.0500	time 0.3295 (0.3421)	loss 1.3517 (1.1618)	grad_norm 0.3736 (0.3785)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:27:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:58 lr 0.000006	 wd 0.0500	time 0.2967 (0.3415)	loss 1.4290 (1.1604)	grad_norm 0.3563 (0.3780)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:27:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:23 lr 0.000006	 wd 0.0500	time 0.3025 (0.3408)	loss 0.9633 (1.1583)	grad_norm 0.3519 (0.3777)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:28:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:48 lr 0.000006	 wd 0.0500	time 0.3831 (0.3398)	loss 1.5266 (1.1595)	grad_norm 0.3761 (0.3771)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:28:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:06:14 lr 0.000006	 wd 0.0500	time 0.3460 (0.3398)	loss 0.9353 (1.1594)	grad_norm 0.3681 (0.3764)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:29:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:40 lr 0.000006	 wd 0.0500	time 0.3300 (0.3395)	loss 1.3933 (1.1600)	grad_norm 0.3577 (0.3762)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:29:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:05:05 lr 0.000007	 wd 0.0500	time 0.3128 (0.3389)	loss 1.2791 (1.1618)	grad_norm 0.3642 (0.3758)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:30:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:31 lr 0.000007	 wd 0.0500	time 0.3073 (0.3389)	loss 0.9840 (1.1619)	grad_norm 0.3744 (0.3754)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:30:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:58 lr 0.000007	 wd 0.0500	time 0.2940 (0.3394)	loss 1.6851 (1.1613)	grad_norm 0.3671 (0.3755)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:31:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:24 lr 0.000007	 wd 0.0500	time 0.3497 (0.3389)	loss 1.4396 (1.1625)	grad_norm 0.3722 (0.3753)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:32:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:50 lr 0.000007	 wd 0.0500	time 0.3051 (0.3389)	loss 1.3931 (1.1630)	grad_norm 0.3935 (0.3751)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:32:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:16 lr 0.000007	 wd 0.0500	time 0.3049 (0.3387)	loss 1.4880 (1.1651)	grad_norm 0.3797 (0.3748)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:33:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.3067 (0.3384)	loss 1.2220 (1.1656)	grad_norm 0.3545 (0.3746)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:33:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.2986 (0.3385)	loss 0.9706 (1.1678)	grad_norm 0.3571 (0.3781)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:34:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.3297 (0.3382)	loss 1.2019 (1.1680)	grad_norm 0.3505 (0.3777)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:34:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.3156 (0.3378)	loss 0.8997 (1.1681)	grad_norm 0.3603 (0.3781)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:34:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:14:07
[2024-08-03 09:35:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.478 (12.478)	Loss 0.5176 (0.5176)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 09:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.128 Acc@5 97.858
[2024-08-03 09:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 09:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-03 09:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 09:35:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 09:35:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:46:19 lr 0.000008	 wd 0.0500	time 11.1828 (11.1828)	loss 1.2355 (1.2355)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:36:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:17:21 lr 0.000008	 wd 0.0500	time 0.3193 (0.4334)	loss 1.4138 (1.1703)	grad_norm 0.3623 (0.3733)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:36:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:50 lr 0.000008	 wd 0.0500	time 0.3461 (0.3869)	loss 1.3984 (1.1623)	grad_norm 0.3537 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:37:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:13:24 lr 0.000008	 wd 0.0500	time 0.3197 (0.3654)	loss 1.1722 (1.1532)	grad_norm 0.3690 (0.3894)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:37:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:12:26 lr 0.000009	 wd 0.0500	time 0.2865 (0.3551)	loss 1.4123 (1.1564)	grad_norm 0.3483 (0.3883)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:38:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:11:43 lr 0.000009	 wd 0.0500	time 0.3052 (0.3513)	loss 1.4228 (1.1519)	grad_norm 0.3742 (0.3839)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:38:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:11:00 lr 0.000009	 wd 0.0500	time 0.3106 (0.3474)	loss 1.2777 (1.1533)	grad_norm 0.3554 (0.3822)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:39:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:10:21 lr 0.000009	 wd 0.0500	time 0.3057 (0.3449)	loss 1.3304 (1.1582)	grad_norm 0.3663 (0.3802)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 09:40:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:45 lr 0.000009	 wd 0.0500	time 0.3224 (0.3440)	loss 1.3755 (1.1612)	grad_norm 0.3599 (nan)	loss_scale 2048.0000 (3937.4782)	mem 17019MB
[2024-08-03 09:40:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:09:08 lr 0.000009	 wd 0.0500	time 0.3138 (0.3426)	loss 0.9302 (1.1661)	grad_norm 0.3710 (nan)	loss_scale 2048.0000 (3727.7691)	mem 17019MB
[2024-08-03 09:41:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:08:33 lr 0.000010	 wd 0.0500	time 0.2942 (0.3417)	loss 0.8679 (1.1633)	grad_norm 0.3895 (nan)	loss_scale 2048.0000 (3559.9600)	mem 17019MB
[2024-08-03 09:41:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:59 lr 0.000010	 wd 0.0500	time 0.2928 (0.3418)	loss 1.3730 (1.1608)	grad_norm 0.3851 (nan)	loss_scale 2048.0000 (3422.6340)	mem 17019MB
[2024-08-03 09:42:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:07:24 lr 0.000010	 wd 0.0500	time 0.2952 (0.3410)	loss 0.9084 (1.1611)	grad_norm 0.3856 (nan)	loss_scale 2048.0000 (3308.1765)	mem 17019MB
[2024-08-03 09:42:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:48 lr 0.000010	 wd 0.0500	time 0.3442 (0.3402)	loss 1.3909 (1.1622)	grad_norm 1.7008 (nan)	loss_scale 2048.0000 (3211.3144)	mem 17019MB
[2024-08-03 09:43:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:06:14 lr 0.000010	 wd 0.0500	time 0.3097 (0.3399)	loss 1.2169 (1.1638)	grad_norm 0.3744 (nan)	loss_scale 2048.0000 (3128.2798)	mem 17019MB
[2024-08-03 09:43:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:39 lr 0.000010	 wd 0.0500	time 0.3223 (0.3392)	loss 1.3280 (1.1643)	grad_norm 0.3491 (nan)	loss_scale 2048.0000 (3056.3091)	mem 17019MB
[2024-08-03 09:44:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:05:05 lr 0.000011	 wd 0.0500	time 0.3372 (0.3387)	loss 0.9712 (1.1648)	grad_norm 0.3545 (nan)	loss_scale 2048.0000 (2993.3292)	mem 17019MB
[2024-08-03 09:45:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:31 lr 0.000011	 wd 0.0500	time 0.3091 (0.3387)	loss 1.2792 (1.1652)	grad_norm 0.3500 (nan)	loss_scale 2048.0000 (2937.7543)	mem 17019MB
[2024-08-03 09:45:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:57 lr 0.000011	 wd 0.0500	time 0.3161 (0.3381)	loss 1.0660 (1.1657)	grad_norm 0.3652 (nan)	loss_scale 2048.0000 (2888.3509)	mem 17019MB
[2024-08-03 09:46:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:23 lr 0.000011	 wd 0.0500	time 0.3398 (0.3377)	loss 1.3349 (1.1655)	grad_norm 0.3821 (nan)	loss_scale 2048.0000 (2844.1452)	mem 17019MB
[2024-08-03 09:46:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:49 lr 0.000011	 wd 0.0500	time 0.3599 (0.3378)	loss 1.1247 (1.1655)	grad_norm 0.3531 (nan)	loss_scale 2048.0000 (2804.3578)	mem 17019MB
[2024-08-03 09:47:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:15 lr 0.000011	 wd 0.0500	time 0.2987 (0.3377)	loss 0.9976 (1.1640)	grad_norm 0.3519 (nan)	loss_scale 2048.0000 (2768.3579)	mem 17019MB
[2024-08-03 09:47:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:42 lr 0.000012	 wd 0.0500	time 0.3360 (0.3377)	loss 1.3895 (1.1637)	grad_norm 0.3810 (nan)	loss_scale 2048.0000 (2735.6293)	mem 17019MB
[2024-08-03 09:48:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:08 lr 0.000012	 wd 0.0500	time 0.3121 (0.3376)	loss 1.2050 (1.1639)	grad_norm 0.4275 (nan)	loss_scale 2048.0000 (2705.7453)	mem 17019MB
[2024-08-03 09:48:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:34 lr 0.000012	 wd 0.0500	time 0.3400 (0.3372)	loss 0.7765 (1.1646)	grad_norm 0.3658 (nan)	loss_scale 2048.0000 (2678.3507)	mem 17019MB
[2024-08-03 09:49:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2911 (0.3363)	loss 1.1927 (1.1650)	grad_norm 0.3929 (nan)	loss_scale 2048.0000 (2653.1467)	mem 17019MB
[2024-08-03 09:49:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:14:03
[2024-08-03 09:49:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.624 (12.624)	Loss 0.4988 (0.4988)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 09:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.112 Acc@5 97.852
[2024-08-03 09:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 09:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-03 09:50:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:45:11 lr 0.000012	 wd 0.0500	time 11.1555 (11.1555)	loss 0.7455 (0.7455)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:50:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:17:32 lr 0.000012	 wd 0.0500	time 0.3108 (0.4384)	loss 1.2366 (1.2004)	grad_norm 0.3675 (0.4024)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:51:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:14:42 lr 0.000012	 wd 0.0500	time 0.2990 (0.3835)	loss 1.5098 (1.1747)	grad_norm 0.3775 (0.3853)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:51:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:13:20 lr 0.000012	 wd 0.0500	time 0.3051 (0.3637)	loss 1.4362 (1.1717)	grad_norm 0.3581 (0.3866)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:52:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:12:25 lr 0.000013	 wd 0.0500	time 0.3062 (0.3548)	loss 1.4009 (1.1640)	grad_norm 0.3635 (0.3826)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:52:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:43 lr 0.000013	 wd 0.0500	time 0.3150 (0.3513)	loss 0.9952 (1.1643)	grad_norm 0.3611 (0.3845)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:53:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:11:02 lr 0.000013	 wd 0.0500	time 0.3347 (0.3484)	loss 1.0625 (1.1593)	grad_norm 0.3760 (0.3816)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:10:22 lr 0.000013	 wd 0.0500	time 0.3359 (0.3456)	loss 1.5023 (1.1568)	grad_norm 0.3567 (0.3805)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:54:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:46 lr 0.000013	 wd 0.0500	time 0.3575 (0.3447)	loss 0.7726 (1.1527)	grad_norm 0.3695 (0.3855)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:55:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:09:09 lr 0.000013	 wd 0.0500	time 0.3085 (0.3431)	loss 1.5613 (1.1548)	grad_norm 0.3721 (0.3838)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:55:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:32 lr 0.000014	 wd 0.0500	time 0.3065 (0.3414)	loss 1.2863 (1.1553)	grad_norm 0.3362 (0.3825)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:56:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:58 lr 0.000014	 wd 0.0500	time 0.3775 (0.3410)	loss 0.9909 (1.1560)	grad_norm 0.4766 (0.3822)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:07:22 lr 0.000014	 wd 0.0500	time 0.3066 (0.3400)	loss 1.1483 (1.1556)	grad_norm 0.3600 (0.3808)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:57:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:47 lr 0.000014	 wd 0.0500	time 0.2982 (0.3390)	loss 1.3088 (1.1550)	grad_norm 0.3493 (0.3802)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:57:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:06:13 lr 0.000014	 wd 0.0500	time 0.2977 (0.3388)	loss 1.1199 (1.1560)	grad_norm 0.3488 (0.3792)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:38 lr 0.000014	 wd 0.0500	time 0.3094 (0.3377)	loss 1.3844 (1.1545)	grad_norm 0.3754 (0.3788)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:58:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:05:04 lr 0.000015	 wd 0.0500	time 0.3826 (0.3371)	loss 0.7730 (1.1530)	grad_norm 0.3689 (0.3782)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 09:59:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:30 lr 0.000015	 wd 0.0500	time 0.3382 (0.3372)	loss 0.9835 (1.1554)	grad_norm 0.5175 (0.3792)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 10:00:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:56 lr 0.000015	 wd 0.0500	time 0.2957 (0.3368)	loss 1.1461 (1.1565)	grad_norm 0.3783 (0.3787)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 10:00:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:22 lr 0.000015	 wd 0.0500	time 0.3484 (0.3367)	loss 1.2431 (1.1557)	grad_norm 0.3579 (0.3783)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 10:01:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:48 lr 0.000015	 wd 0.0500	time 0.3015 (0.3365)	loss 1.3045 (1.1561)	grad_norm 0.3790 (0.3776)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 10:01:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:15 lr 0.000015	 wd 0.0500	time 0.3299 (0.3362)	loss 0.9681 (1.1566)	grad_norm 0.3669 (0.3776)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 10:02:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:41 lr 0.000016	 wd 0.0500	time 0.3126 (0.3357)	loss 0.8011 (1.1558)	grad_norm 0.3549 (0.3780)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 10:02:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:07 lr 0.000016	 wd 0.0500	time 0.3510 (0.3358)	loss 0.8959 (1.1560)	grad_norm 0.3703 (0.3798)	loss_scale 4096.0000 (2104.9631)	mem 17019MB
[2024-08-03 10:03:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:34 lr 0.000016	 wd 0.0500	time 0.3063 (0.3356)	loss 1.4010 (1.1576)	grad_norm 0.3775 (0.3794)	loss_scale 4096.0000 (2187.8884)	mem 17019MB
[2024-08-03 10:03:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2910 (0.3347)	loss 1.2932 (1.1574)	grad_norm 0.4035 (0.3804)	loss_scale 4096.0000 (2264.1823)	mem 17019MB
[2024-08-03 10:03:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:14:00
[2024-08-03 10:04:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.550 (11.550)	Loss 0.5181 (0.5181)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 10:04:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.112 Acc@5 97.868
[2024-08-03 10:04:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 10:04:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-03 10:04:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:49:52 lr 0.000016	 wd 0.0500	time 11.2680 (11.2680)	loss 1.2615 (1.2615)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:05:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:17:27 lr 0.000016	 wd 0.0500	time 0.2977 (0.4360)	loss 0.9452 (1.1723)	grad_norm 0.3629 (0.3676)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:05:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:14:38 lr 0.000016	 wd 0.0500	time 0.3481 (0.3818)	loss 0.8646 (1.1598)	grad_norm 0.3703 (0.3705)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:06:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:13:27 lr 0.000016	 wd 0.0500	time 0.3020 (0.3668)	loss 0.7867 (1.1569)	grad_norm 0.3674 (0.3721)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:06:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:12:28 lr 0.000017	 wd 0.0500	time 0.3092 (0.3563)	loss 1.2909 (1.1517)	grad_norm 0.3878 (0.3717)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:07:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:11:47 lr 0.000017	 wd 0.0500	time 0.3072 (0.3535)	loss 1.2450 (1.1524)	grad_norm 0.3550 (0.3773)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:07:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:11:06 lr 0.000017	 wd 0.0500	time 0.2994 (0.3503)	loss 1.1840 (1.1571)	grad_norm 0.3827 (0.3757)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:08:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:10:26 lr 0.000017	 wd 0.0500	time 0.2988 (0.3478)	loss 0.8306 (1.1575)	grad_norm 0.3747 (0.3758)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:09:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:48 lr 0.000017	 wd 0.0500	time 0.2998 (0.3457)	loss 0.7751 (1.1601)	grad_norm 0.3802 (0.3793)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:09:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:09:12 lr 0.000017	 wd 0.0500	time 0.3168 (0.3448)	loss 0.8252 (1.1611)	grad_norm 0.3579 (0.3781)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:10:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:08:36 lr 0.000018	 wd 0.0500	time 0.3602 (0.3438)	loss 1.5486 (1.1611)	grad_norm 0.3512 (0.3786)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:08:00 lr 0.000018	 wd 0.0500	time 0.3333 (0.3426)	loss 1.4379 (1.1621)	grad_norm 0.3578 (0.3779)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:11:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:26 lr 0.000018	 wd 0.0500	time 0.3294 (0.3427)	loss 1.2766 (1.1610)	grad_norm 0.3714 (0.3770)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:11:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:50 lr 0.000018	 wd 0.0500	time 0.3250 (0.3417)	loss 1.0216 (1.1617)	grad_norm 0.3924 (0.3786)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:12:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:06:16 lr 0.000018	 wd 0.0500	time 0.3272 (0.3415)	loss 1.4014 (1.1616)	grad_norm 0.3689 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:12:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:43 lr 0.000018	 wd 0.0500	time 0.3164 (0.3428)	loss 1.2086 (1.1621)	grad_norm 0.3636 (0.3778)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:13:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:05:08 lr 0.000019	 wd 0.0500	time 0.2977 (0.3422)	loss 1.4455 (1.1607)	grad_norm 0.3936 (0.3780)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:14:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:34 lr 0.000019	 wd 0.0500	time 0.3428 (0.3418)	loss 1.1686 (1.1613)	grad_norm 0.3748 (0.3777)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:14:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:59 lr 0.000019	 wd 0.0500	time 0.3377 (0.3415)	loss 1.5105 (1.1633)	grad_norm 0.3732 (0.3773)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:15:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:25 lr 0.000019	 wd 0.0500	time 0.3416 (0.3413)	loss 1.3708 (1.1637)	grad_norm 0.3655 (0.3779)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:15:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:51 lr 0.000019	 wd 0.0500	time 0.3579 (0.3414)	loss 0.8656 (1.1615)	grad_norm 0.3746 (0.3776)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:16:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:17 lr 0.000019	 wd 0.0500	time 0.3067 (0.3412)	loss 0.9946 (1.1602)	grad_norm 0.3669 (0.3778)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:16:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:42 lr 0.000020	 wd 0.0500	time 0.3416 (0.3407)	loss 0.8776 (1.1598)	grad_norm 0.3500 (0.3775)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:17:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3478 (0.3406)	loss 0.7939 (1.1592)	grad_norm 0.3652 (0.3776)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:18:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3354 (0.3403)	loss 0.7703 (1.1591)	grad_norm 0.3372 (0.3772)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:18:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2907 (0.3392)	loss 1.2262 (1.1597)	grad_norm 0.3779 (0.3769)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:18:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:14:11
[2024-08-03 10:18:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.478 (12.478)	Loss 0.5044 (0.5044)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 10:19:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.128 Acc@5 97.890
[2024-08-03 10:19:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 10:19:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-03 10:19:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:04:58 lr 0.000020	 wd 0.0500	time 11.6299 (11.6299)	loss 1.3820 (1.3820)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:19:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:17:34 lr 0.000020	 wd 0.0500	time 0.3197 (0.4392)	loss 0.9833 (1.1884)	grad_norm 0.3841 (0.3758)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:20:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:14:38 lr 0.000020	 wd 0.0500	time 0.2987 (0.3817)	loss 1.3381 (1.1599)	grad_norm 0.3679 (0.3799)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:20:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:13:27 lr 0.000020	 wd 0.0500	time 0.3186 (0.3666)	loss 1.0097 (1.1492)	grad_norm 0.3959 (0.3805)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:12:28 lr 0.000020	 wd 0.0500	time 0.3445 (0.3563)	loss 1.2536 (1.1582)	grad_norm 0.3926 (0.3797)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:41 lr 0.000020	 wd 0.0500	time 0.2956 (0.3506)	loss 0.9953 (1.1569)	grad_norm 0.3719 (0.3793)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:22:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:11:02 lr 0.000020	 wd 0.0500	time 0.3272 (0.3483)	loss 1.0590 (1.1581)	grad_norm 0.3786 (0.3785)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:10:22 lr 0.000020	 wd 0.0500	time 0.3490 (0.3454)	loss 0.9124 (1.1567)	grad_norm 0.3542 (0.3784)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:23:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:44 lr 0.000020	 wd 0.0500	time 0.3333 (0.3434)	loss 1.5125 (1.1557)	grad_norm 0.3629 (0.3779)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:24:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:09:10 lr 0.000020	 wd 0.0500	time 0.3153 (0.3434)	loss 1.0644 (1.1546)	grad_norm 0.3612 (0.3775)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:24:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:35 lr 0.000020	 wd 0.0500	time 0.3273 (0.3430)	loss 1.2393 (1.1528)	grad_norm 0.5122 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:25:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:08:00 lr 0.000020	 wd 0.0500	time 0.3303 (0.3424)	loss 1.1646 (1.1538)	grad_norm 0.4048 (0.3781)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:25:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:07:25 lr 0.000020	 wd 0.0500	time 0.3012 (0.3422)	loss 1.4033 (1.1524)	grad_norm 0.3655 (0.3821)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 10:26:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:50 lr 0.000020	 wd 0.0500	time 0.3324 (0.3414)	loss 1.0017 (1.1531)	grad_norm 0.3623 (0.3820)	loss_scale 8192.0000 (4310.0876)	mem 17019MB
[2024-08-03 10:27:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:06:15 lr 0.000020	 wd 0.0500	time 0.3256 (0.3403)	loss 1.4123 (1.1567)	grad_norm 1.0376 (nan)	loss_scale 2048.0000 (4227.5632)	mem 17019MB
[2024-08-03 10:27:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:41 lr 0.000020	 wd 0.0500	time 0.3330 (0.3404)	loss 0.8388 (1.1578)	grad_norm 0.3994 (nan)	loss_scale 2048.0000 (4082.3558)	mem 17019MB
[2024-08-03 10:28:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:05:06 lr 0.000020	 wd 0.0500	time 0.3162 (0.3397)	loss 1.3996 (1.1604)	grad_norm 0.3780 (nan)	loss_scale 2048.0000 (3955.2879)	mem 17019MB
[2024-08-03 10:28:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:31 lr 0.000020	 wd 0.0500	time 0.3174 (0.3392)	loss 0.9471 (1.1605)	grad_norm 0.3531 (nan)	loss_scale 2048.0000 (3843.1605)	mem 17019MB
[2024-08-03 10:29:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:58 lr 0.000020	 wd 0.0500	time 0.2994 (0.3395)	loss 0.7672 (1.1584)	grad_norm 0.3633 (nan)	loss_scale 2048.0000 (3743.4847)	mem 17019MB
[2024-08-03 10:29:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:24 lr 0.000020	 wd 0.0500	time 0.3183 (0.3392)	loss 1.1266 (1.1579)	grad_norm 0.3656 (nan)	loss_scale 2048.0000 (3654.2956)	mem 17019MB
[2024-08-03 10:30:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:50 lr 0.000020	 wd 0.0500	time 0.3640 (0.3389)	loss 1.5515 (1.1595)	grad_norm 0.3579 (nan)	loss_scale 2048.0000 (3574.0210)	mem 17019MB
[2024-08-03 10:30:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:16 lr 0.000020	 wd 0.0500	time 0.3232 (0.3391)	loss 1.4669 (1.1590)	grad_norm 0.3858 (nan)	loss_scale 2048.0000 (3501.3879)	mem 17019MB
[2024-08-03 10:31:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:42 lr 0.000020	 wd 0.0500	time 0.2952 (0.3385)	loss 1.2050 (1.1602)	grad_norm 0.3612 (nan)	loss_scale 2048.0000 (3435.3548)	mem 17019MB
[2024-08-03 10:32:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3378 (0.3384)	loss 1.1171 (1.1587)	grad_norm 0.3762 (nan)	loss_scale 2048.0000 (3375.0613)	mem 17019MB
[2024-08-03 10:32:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.2984 (0.3389)	loss 0.7824 (1.1582)	grad_norm 0.4157 (nan)	loss_scale 2048.0000 (3319.7901)	mem 17019MB
[2024-08-03 10:33:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.3020 (0.3380)	loss 1.2870 (1.1594)	grad_norm 0.3662 (nan)	loss_scale 1024.0000 (3244.3727)	mem 17019MB
[2024-08-03 10:33:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:14:08
[2024-08-03 10:33:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.221 (12.221)	Loss 0.4980 (0.4980)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 10:33:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.148 Acc@5 97.878
[2024-08-03 10:33:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 10:33:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-08-03 10:33:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 10:33:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 10:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:12:41 lr 0.000020	 wd 0.0500	time 11.8150 (11.8150)	loss 1.2273 (1.2273)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:34:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:17:22 lr 0.000020	 wd 0.0500	time 0.3232 (0.4341)	loss 0.8917 (1.1813)	grad_norm 0.3815 (0.3776)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:14:41 lr 0.000020	 wd 0.0500	time 0.3424 (0.3831)	loss 0.8613 (1.1711)	grad_norm 0.3425 (0.3775)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:35:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:13:24 lr 0.000020	 wd 0.0500	time 0.3204 (0.3654)	loss 0.9436 (1.1626)	grad_norm 0.3743 (0.3772)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:36:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:33 lr 0.000020	 wd 0.0500	time 0.3270 (0.3583)	loss 0.7859 (1.1646)	grad_norm 0.3735 (0.3793)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:36:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:45 lr 0.000020	 wd 0.0500	time 0.2850 (0.3522)	loss 1.1985 (1.1650)	grad_norm 0.3615 (0.3787)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:37:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:11:04 lr 0.000020	 wd 0.0500	time 0.3272 (0.3493)	loss 0.8649 (1.1565)	grad_norm 0.3836 (0.3784)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:37:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:10:24 lr 0.000020	 wd 0.0500	time 0.3114 (0.3468)	loss 1.4709 (1.1591)	grad_norm 0.3427 (0.3773)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:38:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:45 lr 0.000020	 wd 0.0500	time 0.3313 (0.3441)	loss 0.7011 (1.1635)	grad_norm 0.3535 (0.3771)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:38:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:09:10 lr 0.000020	 wd 0.0500	time 0.3275 (0.3436)	loss 1.4847 (1.1599)	grad_norm 0.3479 (0.3774)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:39:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:34 lr 0.000020	 wd 0.0500	time 0.2966 (0.3426)	loss 1.0343 (1.1597)	grad_norm 0.3535 (0.3784)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:39:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:58 lr 0.000020	 wd 0.0500	time 0.3332 (0.3414)	loss 0.8585 (1.1563)	grad_norm 0.3678 (0.3788)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:40:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:23 lr 0.000020	 wd 0.0500	time 0.3089 (0.3408)	loss 1.2282 (1.1534)	grad_norm 0.3841 (0.3785)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:49 lr 0.000020	 wd 0.0500	time 0.3006 (0.3407)	loss 1.0923 (1.1523)	grad_norm 0.3615 (0.3785)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:41:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:06:14 lr 0.000020	 wd 0.0500	time 0.2993 (0.3399)	loss 0.9398 (1.1525)	grad_norm 0.3675 (0.3783)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:42:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:40 lr 0.000020	 wd 0.0500	time 0.3103 (0.3395)	loss 0.7256 (1.1534)	grad_norm 0.3778 (0.3777)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:42:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:05:06 lr 0.000020	 wd 0.0500	time 0.2988 (0.3394)	loss 1.4436 (1.1510)	grad_norm 0.3549 (0.3781)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:43:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:31 lr 0.000020	 wd 0.0500	time 0.3017 (0.3391)	loss 1.3699 (1.1504)	grad_norm 0.3651 (0.3784)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:43:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:57 lr 0.000020	 wd 0.0500	time 0.3454 (0.3386)	loss 1.1939 (1.1508)	grad_norm 0.3644 (0.3784)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:44:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:23 lr 0.000020	 wd 0.0500	time 0.3496 (0.3384)	loss 1.3355 (1.1520)	grad_norm 0.3623 (0.3788)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:44:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.3342 (0.3381)	loss 0.9707 (1.1528)	grad_norm 0.3696 (0.3793)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:45:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:15 lr 0.000020	 wd 0.0500	time 0.3158 (0.3381)	loss 0.9990 (1.1530)	grad_norm 0.3735 (0.3810)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:46:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:42 lr 0.000020	 wd 0.0500	time 0.3135 (0.3380)	loss 0.8106 (1.1524)	grad_norm 0.3988 (0.3811)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:46:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3286 (0.3381)	loss 1.0260 (1.1548)	grad_norm 0.3883 (0.3809)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:47:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3191 (0.3381)	loss 1.3755 (1.1541)	grad_norm 0.3746 (0.3814)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:47:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.3212 (0.3373)	loss 1.3355 (1.1537)	grad_norm 0.3568 (0.3829)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:47:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:14:06
[2024-08-03 10:47:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.390 (11.390)	Loss 0.5103 (0.5103)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 10:48:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.126 Acc@5 97.884
[2024-08-03 10:48:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 10:48:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-08-03 10:48:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:23:08 lr 0.000020	 wd 0.0500	time 12.0656 (12.0656)	loss 0.9216 (0.9216)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:49:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:17:43 lr 0.000020	 wd 0.0500	time 0.2982 (0.4428)	loss 1.1929 (1.1402)	grad_norm 0.3660 (0.3753)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:49:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.3279 (0.3860)	loss 1.1038 (1.1606)	grad_norm 0.3964 (0.3763)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:50:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:25 lr 0.000020	 wd 0.0500	time 0.3520 (0.3657)	loss 1.2832 (1.1555)	grad_norm 0.3674 (0.3751)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:28 lr 0.000020	 wd 0.0500	time 0.3144 (0.3563)	loss 0.8167 (1.1605)	grad_norm 0.3774 (0.3754)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:51:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:42 lr 0.000020	 wd 0.0500	time 0.3431 (0.3510)	loss 1.3290 (1.1573)	grad_norm 0.3572 (0.3769)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:51:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:11:02 lr 0.000020	 wd 0.0500	time 0.3040 (0.3483)	loss 1.2864 (1.1615)	grad_norm 0.3749 (0.3761)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:52:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:10:22 lr 0.000020	 wd 0.0500	time 0.3317 (0.3453)	loss 0.9621 (1.1561)	grad_norm 0.3678 (0.3765)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:52:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:45 lr 0.000020	 wd 0.0500	time 0.3083 (0.3442)	loss 0.8903 (1.1536)	grad_norm 0.3783 (0.3776)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:53:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:09:08 lr 0.000020	 wd 0.0500	time 0.3317 (0.3426)	loss 1.4201 (1.1552)	grad_norm 0.3590 (0.3779)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:53:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:32 lr 0.000020	 wd 0.0500	time 0.2916 (0.3413)	loss 0.8060 (1.1552)	grad_norm 0.3862 (0.3776)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:54:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:57 lr 0.000020	 wd 0.0500	time 0.3242 (0.3408)	loss 1.3515 (1.1545)	grad_norm 0.3808 (0.3779)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:55:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:22 lr 0.000020	 wd 0.0500	time 0.3447 (0.3396)	loss 1.3802 (1.1512)	grad_norm 0.3697 (0.3774)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:55:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:48 lr 0.000020	 wd 0.0500	time 0.3528 (0.3399)	loss 1.3651 (1.1523)	grad_norm 0.3650 (0.3788)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:56:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:06:14 lr 0.000019	 wd 0.0500	time 0.2886 (0.3401)	loss 1.2586 (1.1521)	grad_norm 0.3628 (0.3784)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 10:56:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:39 lr 0.000019	 wd 0.0500	time 0.3200 (0.3392)	loss 0.8298 (1.1527)	grad_norm 0.3685 (0.3783)	loss_scale 2048.0000 (1067.6616)	mem 17019MB
[2024-08-03 10:57:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:05:06 lr 0.000019	 wd 0.0500	time 0.3813 (0.3393)	loss 1.1024 (1.1532)	grad_norm 0.3739 (0.3788)	loss_scale 2048.0000 (1128.8944)	mem 17019MB
[2024-08-03 10:57:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:31 lr 0.000019	 wd 0.0500	time 0.3435 (0.3390)	loss 1.3231 (1.1534)	grad_norm 0.3525 (0.3788)	loss_scale 2048.0000 (1182.9277)	mem 17019MB
[2024-08-03 10:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:57 lr 0.000019	 wd 0.0500	time 0.3545 (0.3388)	loss 0.7528 (1.1557)	grad_norm 0.3673 (0.3785)	loss_scale 2048.0000 (1230.9606)	mem 17019MB
[2024-08-03 10:59:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:23 lr 0.000019	 wd 0.0500	time 0.3106 (0.3386)	loss 1.0967 (1.1561)	grad_norm 0.3884 (0.3785)	loss_scale 2048.0000 (1273.9400)	mem 17019MB
[2024-08-03 10:59:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:50 lr 0.000019	 wd 0.0500	time 0.3364 (0.3387)	loss 1.1335 (1.1567)	grad_norm 0.3742 (0.3789)	loss_scale 2048.0000 (1312.6237)	mem 17019MB
[2024-08-03 11:00:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:15 lr 0.000019	 wd 0.0500	time 0.3055 (0.3383)	loss 1.4419 (1.1593)	grad_norm 0.3768 (nan)	loss_scale 1024.0000 (1319.3565)	mem 17019MB
[2024-08-03 11:00:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:42 lr 0.000019	 wd 0.0500	time 0.2975 (0.3384)	loss 1.2175 (1.1575)	grad_norm 0.3720 (nan)	loss_scale 1024.0000 (1305.9373)	mem 17019MB
[2024-08-03 11:01:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:08 lr 0.000019	 wd 0.0500	time 0.3151 (0.3382)	loss 1.3622 (1.1566)	grad_norm 0.3815 (nan)	loss_scale 1024.0000 (1293.6845)	mem 17019MB
[2024-08-03 11:01:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3467 (0.3385)	loss 1.2397 (1.1570)	grad_norm 0.3633 (nan)	loss_scale 1024.0000 (1282.4523)	mem 17019MB
[2024-08-03 11:02:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2919 (0.3376)	loss 1.5636 (1.1571)	grad_norm 0.3493 (nan)	loss_scale 1024.0000 (1272.1184)	mem 17019MB
[2024-08-03 11:02:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:14:07
[2024-08-03 11:02:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.117 (12.117)	Loss 0.4912 (0.4912)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 11:02:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.116 Acc@5 97.888
[2024-08-03 11:02:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 11:02:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-08-03 11:03:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:32:09 lr 0.000019	 wd 0.0500	time 12.2820 (12.2820)	loss 1.2320 (1.2320)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:03:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:17:48 lr 0.000019	 wd 0.0500	time 0.3090 (0.4448)	loss 0.9731 (1.1987)	grad_norm 0.3729 (0.4005)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:04:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:14:48 lr 0.000019	 wd 0.0500	time 0.3061 (0.3858)	loss 0.8104 (1.1838)	grad_norm 0.3683 (0.3879)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:04:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:13:31 lr 0.000019	 wd 0.0500	time 0.3134 (0.3684)	loss 1.3760 (1.1754)	grad_norm 0.3809 (0.3830)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:05:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:41 lr 0.000019	 wd 0.0500	time 0.3234 (0.3622)	loss 0.8277 (1.1743)	grad_norm 0.3714 (0.3822)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:05:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:11:53 lr 0.000019	 wd 0.0500	time 0.2981 (0.3565)	loss 0.7994 (1.1637)	grad_norm 0.3722 (0.3821)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:06:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:11:08 lr 0.000019	 wd 0.0500	time 0.3051 (0.3516)	loss 0.9097 (1.1606)	grad_norm 0.3884 (0.3840)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:06:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:27 lr 0.000019	 wd 0.0500	time 0.3026 (0.3484)	loss 1.3318 (1.1613)	grad_norm 0.3846 (0.3829)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:07:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:47 lr 0.000019	 wd 0.0500	time 0.2983 (0.3454)	loss 1.4567 (1.1630)	grad_norm 0.5869 (0.3830)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:08:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:09:10 lr 0.000019	 wd 0.0500	time 0.3211 (0.3438)	loss 0.9435 (1.1582)	grad_norm 0.5245 (0.3821)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:08:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:33 lr 0.000019	 wd 0.0500	time 0.3105 (0.3421)	loss 1.3315 (1.1615)	grad_norm 0.4086 (0.3835)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:09:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:58 lr 0.000019	 wd 0.0500	time 0.3102 (0.3410)	loss 1.1863 (1.1597)	grad_norm 0.3605 (0.3825)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:09:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:22 lr 0.000019	 wd 0.0500	time 0.3389 (0.3402)	loss 1.1162 (1.1592)	grad_norm 0.3796 (0.3820)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:10:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:47 lr 0.000019	 wd 0.0500	time 0.3096 (0.3394)	loss 0.8966 (1.1586)	grad_norm 0.3879 (0.3814)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:10:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:06:13 lr 0.000019	 wd 0.0500	time 0.3341 (0.3389)	loss 1.2573 (1.1594)	grad_norm 0.3823 (0.3824)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:11:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:39 lr 0.000019	 wd 0.0500	time 0.3475 (0.3389)	loss 1.1521 (1.1597)	grad_norm 0.3937 (0.3823)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:11:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:05:06 lr 0.000019	 wd 0.0500	time 0.3296 (0.3394)	loss 1.3654 (1.1623)	grad_norm 0.4236 (0.3829)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:12:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:32 lr 0.000019	 wd 0.0500	time 0.3870 (0.3398)	loss 0.7608 (1.1647)	grad_norm 0.3565 (0.3835)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:13:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:58 lr 0.000019	 wd 0.0500	time 0.3399 (0.3402)	loss 0.7822 (1.1634)	grad_norm 0.3737 (0.3832)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:13:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:25 lr 0.000019	 wd 0.0500	time 0.3361 (0.3410)	loss 1.1648 (1.1651)	grad_norm 0.3648 (0.3844)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:14:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:51 lr 0.000019	 wd 0.0500	time 0.3332 (0.3415)	loss 0.8112 (1.1639)	grad_norm 0.3891 (0.3842)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:14:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:17 lr 0.000019	 wd 0.0500	time 0.3481 (0.3423)	loss 0.8669 (1.1643)	grad_norm 0.3916 (0.3849)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:15:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:43 lr 0.000019	 wd 0.0500	time 0.3463 (0.3422)	loss 1.1046 (1.1627)	grad_norm 0.4001 (0.3849)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:15:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:09 lr 0.000019	 wd 0.0500	time 0.2986 (0.3418)	loss 1.3257 (1.1614)	grad_norm 0.3581 (0.3855)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:16:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3199 (0.3412)	loss 1.1668 (1.1613)	grad_norm 0.3922 (0.3857)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:17:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2917 (0.3403)	loss 0.8917 (1.1611)	grad_norm 0.3564 (0.3869)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:17:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:14:14
[2024-08-03 11:17:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.560 (12.560)	Loss 0.4912 (0.4912)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 11:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.156 Acc@5 97.888
[2024-08-03 11:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 11:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.16%
[2024-08-03 11:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 11:17:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 11:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:42:30 lr 0.000019	 wd 0.0500	time 11.0912 (11.0912)	loss 1.3101 (1.3101)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:18:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:17:24 lr 0.000019	 wd 0.0500	time 0.2950 (0.4347)	loss 1.2412 (1.1380)	grad_norm 0.3720 (0.3813)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:18:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:39 lr 0.000019	 wd 0.0500	time 0.3114 (0.3821)	loss 0.8841 (1.1617)	grad_norm 0.3700 (0.3810)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:19:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:13:20 lr 0.000019	 wd 0.0500	time 0.3048 (0.3635)	loss 1.4960 (1.1704)	grad_norm 0.3933 (0.3843)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:19:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:12:26 lr 0.000019	 wd 0.0500	time 0.3378 (0.3550)	loss 0.8356 (1.1539)	grad_norm 0.3707 (0.3824)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:20:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:40 lr 0.000019	 wd 0.0500	time 0.3046 (0.3499)	loss 0.7452 (1.1491)	grad_norm 0.3840 (0.3809)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:21:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:11:00 lr 0.000019	 wd 0.0500	time 0.3240 (0.3472)	loss 0.8314 (1.1535)	grad_norm 0.3678 (0.3805)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:21:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:10:20 lr 0.000019	 wd 0.0500	time 0.3003 (0.3442)	loss 1.3629 (1.1570)	grad_norm 0.3663 (0.3799)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:22:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:43 lr 0.000019	 wd 0.0500	time 0.3463 (0.3428)	loss 1.3725 (1.1582)	grad_norm 0.3549 (0.3806)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:22:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:09:07 lr 0.000019	 wd 0.0500	time 0.2987 (0.3419)	loss 1.3335 (1.1579)	grad_norm 0.3554 (0.3801)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:23:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:08:31 lr 0.000019	 wd 0.0500	time 0.2994 (0.3407)	loss 1.1620 (1.1585)	grad_norm 0.3883 (0.3807)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:23:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:56 lr 0.000018	 wd 0.0500	time 0.2947 (0.3398)	loss 1.3173 (1.1546)	grad_norm 0.3709 (0.3804)	loss_scale 2048.0000 (1081.6639)	mem 17019MB
[2024-08-03 11:24:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:21 lr 0.000018	 wd 0.0500	time 0.2893 (0.3393)	loss 0.8379 (1.1536)	grad_norm 0.3694 (0.3832)	loss_scale 2048.0000 (1162.1249)	mem 17019MB
[2024-08-03 11:24:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:46 lr 0.000018	 wd 0.0500	time 0.3478 (0.3386)	loss 1.2617 (1.1545)	grad_norm 0.3713 (0.3834)	loss_scale 2048.0000 (1230.2168)	mem 17019MB
[2024-08-03 11:25:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:06:13 lr 0.000018	 wd 0.0500	time 0.3099 (0.3389)	loss 1.0654 (1.1537)	grad_norm 0.3715 (0.3834)	loss_scale 2048.0000 (1288.5882)	mem 17019MB
[2024-08-03 11:26:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:39 lr 0.000018	 wd 0.0500	time 0.3813 (0.3387)	loss 0.8434 (1.1569)	grad_norm 0.3864 (0.3838)	loss_scale 2048.0000 (1339.1819)	mem 17019MB
[2024-08-03 11:26:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:05:05 lr 0.000018	 wd 0.0500	time 0.3041 (0.3385)	loss 0.9026 (1.1552)	grad_norm 0.3595 (0.3848)	loss_scale 2048.0000 (1383.4553)	mem 17019MB
[2024-08-03 11:27:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:31 lr 0.000018	 wd 0.0500	time 0.3047 (0.3384)	loss 1.4184 (1.1557)	grad_norm 0.3787 (0.3854)	loss_scale 2048.0000 (1422.5232)	mem 17019MB
[2024-08-03 11:27:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:57 lr 0.000018	 wd 0.0500	time 0.3432 (0.3381)	loss 1.2421 (1.1545)	grad_norm 0.3765 (0.3853)	loss_scale 2048.0000 (1457.2526)	mem 17019MB
[2024-08-03 11:28:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:23 lr 0.000018	 wd 0.0500	time 0.3053 (0.3387)	loss 1.3849 (1.1539)	grad_norm 0.3870 (0.3850)	loss_scale 2048.0000 (1488.3282)	mem 17019MB
[2024-08-03 11:28:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:49 lr 0.000018	 wd 0.0500	time 0.3531 (0.3386)	loss 1.3750 (1.1544)	grad_norm 0.3466 (0.3851)	loss_scale 2048.0000 (1516.2979)	mem 17019MB
[2024-08-03 11:29:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:16 lr 0.000018	 wd 0.0500	time 0.2973 (0.3384)	loss 1.4001 (1.1559)	grad_norm 0.3871 (0.3853)	loss_scale 2048.0000 (1541.6050)	mem 17019MB
[2024-08-03 11:30:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:42 lr 0.000018	 wd 0.0500	time 0.3631 (0.3383)	loss 1.1485 (1.1570)	grad_norm 0.3836 (0.3859)	loss_scale 2048.0000 (1564.6124)	mem 17019MB
[2024-08-03 11:30:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:08 lr 0.000018	 wd 0.0500	time 0.3396 (0.3388)	loss 0.7839 (1.1570)	grad_norm 0.3804 (nan)	loss_scale 1024.0000 (1576.7197)	mem 17019MB
[2024-08-03 11:31:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:34 lr 0.000018	 wd 0.0500	time 0.2955 (0.3386)	loss 1.0717 (1.1573)	grad_norm 0.3909 (nan)	loss_scale 1024.0000 (1553.6993)	mem 17019MB
[2024-08-03 11:31:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2910 (0.3387)	loss 1.2549 (1.1569)	grad_norm 0.3841 (nan)	loss_scale 1024.0000 (1532.5198)	mem 17019MB
[2024-08-03 11:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 9 training takes 0:14:10
[2024-08-03 11:31:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.883 (11.883)	Loss 0.5024 (0.5024)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 11:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.160 Acc@5 97.882
[2024-08-03 11:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 11:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.16%
[2024-08-03 11:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 11:32:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 11:32:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:52:37 lr 0.000018	 wd 0.0500	time 11.3338 (11.3338)	loss 1.6190 (1.6190)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:32:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:17:45 lr 0.000018	 wd 0.0500	time 0.3384 (0.4438)	loss 0.8849 (1.1676)	grad_norm 0.4131 (0.3744)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:33:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:52 lr 0.000018	 wd 0.0500	time 0.3296 (0.3875)	loss 1.4255 (1.1627)	grad_norm 0.3656 (0.3760)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:34:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:13:36 lr 0.000018	 wd 0.0500	time 0.3385 (0.3706)	loss 1.3117 (1.1575)	grad_norm 0.3920 (0.3759)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:34:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:12:36 lr 0.000018	 wd 0.0500	time 0.3121 (0.3597)	loss 0.8749 (1.1609)	grad_norm 0.4345 (0.3765)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:35:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:49 lr 0.000018	 wd 0.0500	time 0.3096 (0.3544)	loss 0.6986 (1.1564)	grad_norm 0.3607 (0.3806)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:35:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:11:11 lr 0.000018	 wd 0.0500	time 0.3232 (0.3532)	loss 1.1560 (1.1643)	grad_norm 0.3662 (0.3804)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:36:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:10:31 lr 0.000018	 wd 0.0500	time 0.3676 (0.3503)	loss 1.2898 (1.1627)	grad_norm 0.3755 (0.3816)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:36:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:52 lr 0.000018	 wd 0.0500	time 0.3332 (0.3479)	loss 1.2683 (1.1654)	grad_norm 0.3658 (0.3876)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:37:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:09:13 lr 0.000018	 wd 0.0500	time 0.3030 (0.3457)	loss 1.4396 (1.1630)	grad_norm 0.3644 (0.3871)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:37:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:36 lr 0.000018	 wd 0.0500	time 0.3481 (0.3442)	loss 1.2766 (1.1641)	grad_norm 0.3724 (0.3861)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:38:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:08:01 lr 0.000018	 wd 0.0500	time 0.3377 (0.3432)	loss 0.9931 (1.1698)	grad_norm 0.3987 (0.3876)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:39:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:07:25 lr 0.000018	 wd 0.0500	time 0.2971 (0.3423)	loss 1.3906 (1.1673)	grad_norm 0.3731 (0.3876)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:39:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:50 lr 0.000018	 wd 0.0500	time 0.3274 (0.3414)	loss 1.3531 (1.1649)	grad_norm 0.3835 (0.3869)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:40:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:15 lr 0.000018	 wd 0.0500	time 0.3006 (0.3407)	loss 1.3480 (1.1628)	grad_norm 0.3676 (0.3871)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:40:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:41 lr 0.000018	 wd 0.0500	time 0.3272 (0.3403)	loss 1.1525 (1.1603)	grad_norm 0.3938 (0.3877)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:41:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:05:06 lr 0.000018	 wd 0.0500	time 0.3598 (0.3401)	loss 1.0973 (1.1590)	grad_norm 0.3554 (0.3979)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:41:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:32 lr 0.000018	 wd 0.0500	time 0.2991 (0.3396)	loss 1.2311 (1.1610)	grad_norm 0.3752 (0.4004)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:42:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:58 lr 0.000018	 wd 0.0500	time 0.3819 (0.3394)	loss 1.1288 (1.1601)	grad_norm 0.3962 (0.3997)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:42:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:24 lr 0.000018	 wd 0.0500	time 0.3374 (0.3392)	loss 1.0757 (1.1591)	grad_norm 0.4021 (0.3991)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:50 lr 0.000017	 wd 0.0500	time 0.3176 (0.3390)	loss 1.3241 (1.1626)	grad_norm 0.3820 (0.3981)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:44:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:16 lr 0.000017	 wd 0.0500	time 0.2917 (0.3386)	loss 1.1909 (1.1639)	grad_norm 0.3537 (0.3985)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:44:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:42 lr 0.000017	 wd 0.0500	time 0.3232 (0.3383)	loss 1.3110 (1.1650)	grad_norm 0.3672 (0.3984)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:45:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:08 lr 0.000017	 wd 0.0500	time 0.3079 (0.3381)	loss 1.2777 (1.1629)	grad_norm 0.3798 (0.3984)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:45:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:34 lr 0.000017	 wd 0.0500	time 0.3363 (0.3387)	loss 1.3502 (1.1619)	grad_norm 0.5320 (0.3980)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:46:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2961 (0.3379)	loss 0.8959 (1.1614)	grad_norm 0.3723 (0.3973)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:46:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 10 training takes 0:14:08
[2024-08-03 11:46:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_10.pth saving......
[2024-08-03 11:46:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_10.pth saved !!!
[2024-08-03 11:46:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.716 (11.716)	Loss 0.4944 (0.4944)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-03 11:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.182 Acc@5 97.900
[2024-08-03 11:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 11:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.18%
[2024-08-03 11:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 11:46:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 11:47:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:36:27 lr 0.000017	 wd 0.0500	time 10.9463 (10.9463)	loss 0.9145 (0.9145)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:47:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:17:18 lr 0.000017	 wd 0.0500	time 0.3160 (0.4322)	loss 1.3825 (1.1461)	grad_norm 0.3619 (0.3849)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:48:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:26 lr 0.000017	 wd 0.0500	time 0.3433 (0.3766)	loss 1.3154 (1.1673)	grad_norm 0.3841 (0.3980)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:48:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:13:13 lr 0.000017	 wd 0.0500	time 0.3206 (0.3605)	loss 0.8466 (1.1577)	grad_norm 0.3650 (0.3943)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:49:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:12:20 lr 0.000017	 wd 0.0500	time 0.3290 (0.3522)	loss 1.3913 (1.1559)	grad_norm 0.4070 (0.3905)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:49:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:36 lr 0.000017	 wd 0.0500	time 0.3067 (0.3479)	loss 0.7240 (1.1553)	grad_norm 0.3631 (0.3889)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:50:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:10:57 lr 0.000017	 wd 0.0500	time 0.2974 (0.3455)	loss 1.4475 (1.1560)	grad_norm 0.4036 (0.3890)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:50:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:10:19 lr 0.000017	 wd 0.0500	time 0.3026 (0.3436)	loss 1.0081 (1.1515)	grad_norm 0.4058 (0.3893)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:51:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:41 lr 0.000017	 wd 0.0500	time 0.2983 (0.3419)	loss 1.4698 (1.1496)	grad_norm 0.4356 (0.3886)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:51:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:09:06 lr 0.000017	 wd 0.0500	time 0.2994 (0.3413)	loss 0.9888 (1.1524)	grad_norm 0.3894 (0.3884)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:52:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:08:31 lr 0.000017	 wd 0.0500	time 0.3044 (0.3405)	loss 1.1518 (1.1524)	grad_norm 0.3838 (0.3876)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:53:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:55 lr 0.000017	 wd 0.0500	time 0.3004 (0.3392)	loss 0.9086 (1.1510)	grad_norm 0.3934 (0.3873)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:53:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:20 lr 0.000017	 wd 0.0500	time 0.3137 (0.3385)	loss 0.9185 (1.1479)	grad_norm 0.3812 (0.3880)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-03 11:54:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:46 lr 0.000017	 wd 0.0500	time 0.3249 (0.3380)	loss 1.3894 (1.1484)	grad_norm 0.3852 (0.3877)	loss_scale 2048.0000 (1042.8901)	mem 17019MB
[2024-08-03 11:54:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:06:11 lr 0.000017	 wd 0.0500	time 0.3447 (0.3375)	loss 0.8123 (1.1488)	grad_norm 0.3696 (0.3873)	loss_scale 2048.0000 (1114.6324)	mem 17019MB
[2024-08-03 11:55:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:37 lr 0.000017	 wd 0.0500	time 0.3031 (0.3370)	loss 1.3903 (1.1496)	grad_norm 0.3851 (0.3879)	loss_scale 2048.0000 (1176.8155)	mem 17019MB
[2024-08-03 11:55:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:05:03 lr 0.000017	 wd 0.0500	time 0.3062 (0.3367)	loss 1.2645 (1.1509)	grad_norm 0.3734 (0.3887)	loss_scale 2048.0000 (1231.2305)	mem 17019MB
[2024-08-03 11:56:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:29 lr 0.000017	 wd 0.0500	time 0.3134 (0.3362)	loss 1.0796 (1.1501)	grad_norm 0.3788 (0.3883)	loss_scale 2048.0000 (1279.2475)	mem 17019MB
[2024-08-03 11:56:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:55 lr 0.000017	 wd 0.0500	time 0.3454 (0.3361)	loss 0.8301 (1.1500)	grad_norm 0.3599 (0.3877)	loss_scale 2048.0000 (1321.9323)	mem 17019MB
[2024-08-03 11:57:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:22 lr 0.000017	 wd 0.0500	time 0.3031 (0.3359)	loss 1.1338 (1.1491)	grad_norm 0.3799 (0.3898)	loss_scale 2048.0000 (1360.1262)	mem 17019MB
[2024-08-03 11:58:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:48 lr 0.000017	 wd 0.0500	time 0.3118 (0.3360)	loss 1.0556 (1.1493)	grad_norm 0.3425 (0.3896)	loss_scale 2048.0000 (1394.5027)	mem 17019MB
[2024-08-03 11:58:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:15 lr 0.000017	 wd 0.0500	time 0.3068 (0.3361)	loss 1.3495 (1.1485)	grad_norm 0.3942 (0.3889)	loss_scale 2048.0000 (1425.6069)	mem 17019MB
[2024-08-03 11:59:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:41 lr 0.000017	 wd 0.0500	time 0.3223 (0.3363)	loss 0.9735 (1.1504)	grad_norm 0.3850 (0.3886)	loss_scale 2048.0000 (1453.8846)	mem 17019MB
[2024-08-03 11:59:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:07 lr 0.000016	 wd 0.0500	time 0.3146 (0.3364)	loss 1.1509 (1.1486)	grad_norm 0.3842 (0.3881)	loss_scale 2048.0000 (1479.7045)	mem 17019MB
[2024-08-03 12:00:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:34 lr 0.000016	 wd 0.0500	time 0.2963 (0.3363)	loss 1.3819 (1.1498)	grad_norm 0.3723 (0.3904)	loss_scale 2048.0000 (1503.3736)	mem 17019MB
[2024-08-03 12:00:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2913 (0.3356)	loss 0.8429 (1.1481)	grad_norm 0.3501 (0.3902)	loss_scale 2048.0000 (1525.1499)	mem 17019MB
[2024-08-03 12:00:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 11 training takes 0:14:02
[2024-08-03 12:01:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.107 (12.107)	Loss 0.4939 (0.4939)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 12:01:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.166 Acc@5 97.868
[2024-08-03 12:01:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 12:01:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.18%
[2024-08-03 12:01:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:00:24 lr 0.000016	 wd 0.0500	time 11.5204 (11.5204)	loss 0.9458 (0.9458)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:02:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:17:35 lr 0.000016	 wd 0.0500	time 0.3574 (0.4396)	loss 1.3071 (1.1524)	grad_norm 0.3639 (0.3963)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:02:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:14:43 lr 0.000016	 wd 0.0500	time 0.3283 (0.3837)	loss 1.2874 (1.1494)	grad_norm 0.3733 (0.3890)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:03:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:13:24 lr 0.000016	 wd 0.0500	time 0.3411 (0.3654)	loss 0.7432 (1.1613)	grad_norm 0.4093 (0.3907)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:03:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:30 lr 0.000016	 wd 0.0500	time 0.3124 (0.3571)	loss 1.2151 (1.1532)	grad_norm 0.3579 (0.3884)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:04:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:45 lr 0.000016	 wd 0.0500	time 0.2995 (0.3522)	loss 1.4189 (1.1532)	grad_norm 0.3694 (0.3866)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:04:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:11:03 lr 0.000016	 wd 0.0500	time 0.3288 (0.3489)	loss 1.4089 (1.1516)	grad_norm 0.4039 (0.3860)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:05:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:10:24 lr 0.000016	 wd 0.0500	time 0.3494 (0.3465)	loss 1.2001 (1.1498)	grad_norm 0.3769 (0.3855)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:05:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:46 lr 0.000016	 wd 0.0500	time 0.3043 (0.3444)	loss 1.3575 (1.1508)	grad_norm 0.4120 (0.3863)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:06:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:09:10 lr 0.000016	 wd 0.0500	time 0.3074 (0.3435)	loss 1.5207 (1.1483)	grad_norm 0.3683 (0.3855)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:07:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:33 lr 0.000016	 wd 0.0500	time 0.3036 (0.3421)	loss 1.3836 (1.1502)	grad_norm 0.3844 (0.3865)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:07:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:57 lr 0.000016	 wd 0.0500	time 0.3068 (0.3409)	loss 0.8000 (1.1518)	grad_norm 0.3977 (0.3868)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:08:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:23 lr 0.000016	 wd 0.0500	time 0.3216 (0.3404)	loss 1.4230 (1.1500)	grad_norm 0.3761 (0.3880)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:48 lr 0.000016	 wd 0.0500	time 0.3173 (0.3396)	loss 1.0445 (1.1508)	grad_norm 0.3583 (0.3885)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:09:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:06:13 lr 0.000016	 wd 0.0500	time 0.3102 (0.3393)	loss 1.2964 (1.1505)	grad_norm 0.3698 (0.3930)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:09:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:39 lr 0.000016	 wd 0.0500	time 0.3090 (0.3390)	loss 0.8060 (1.1478)	grad_norm 0.3997 (0.3937)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:10:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:05:05 lr 0.000016	 wd 0.0500	time 0.3418 (0.3387)	loss 1.3604 (1.1445)	grad_norm 0.3625 (0.3926)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:10:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:31 lr 0.000016	 wd 0.0500	time 0.3391 (0.3387)	loss 1.0570 (1.1466)	grad_norm 0.3814 (0.3918)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:11:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:57 lr 0.000016	 wd 0.0500	time 0.3444 (0.3384)	loss 0.8073 (1.1452)	grad_norm 0.3842 (0.3915)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:12:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:23 lr 0.000016	 wd 0.0500	time 0.3295 (0.3382)	loss 1.2981 (1.1460)	grad_norm 0.3760 (0.3915)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:12:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:49 lr 0.000016	 wd 0.0500	time 0.2936 (0.3383)	loss 1.0347 (1.1468)	grad_norm 0.4127 (0.3912)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:13:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:16 lr 0.000016	 wd 0.0500	time 0.3009 (0.3383)	loss 0.8066 (1.1461)	grad_norm 0.3391 (0.3934)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:13:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:42 lr 0.000016	 wd 0.0500	time 0.3260 (0.3380)	loss 0.8083 (1.1475)	grad_norm 0.3865 (0.3939)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:14:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:08 lr 0.000015	 wd 0.0500	time 0.3456 (0.3376)	loss 1.1851 (1.1479)	grad_norm 0.3702 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:14:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:34 lr 0.000015	 wd 0.0500	time 0.3119 (0.3374)	loss 1.3461 (1.1482)	grad_norm 0.7851 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:15:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.3283 (0.3368)	loss 0.9310 (1.1475)	grad_norm 0.3829 (0.3956)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:15:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 12 training takes 0:14:05
[2024-08-03 12:15:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.039 (12.039)	Loss 0.5068 (0.5068)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 12:15:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.164 Acc@5 97.902
[2024-08-03 12:15:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 12:15:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.18%
[2024-08-03 12:16:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:59:01 lr 0.000015	 wd 0.0500	time 11.4874 (11.4874)	loss 1.3187 (1.3187)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:16:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:17:39 lr 0.000015	 wd 0.0500	time 0.3125 (0.4411)	loss 1.3247 (1.2120)	grad_norm 0.3725 (0.3857)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:17:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:49 lr 0.000015	 wd 0.0500	time 0.3098 (0.3863)	loss 1.3773 (1.1995)	grad_norm 0.3611 (0.3903)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:13:27 lr 0.000015	 wd 0.0500	time 0.2990 (0.3669)	loss 1.3765 (1.1996)	grad_norm 0.3750 (0.3889)	loss_scale 4096.0000 (2238.5116)	mem 17019MB
[2024-08-03 12:18:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:12:30 lr 0.000015	 wd 0.0500	time 0.3490 (0.3572)	loss 1.4485 (1.1804)	grad_norm 0.3792 (0.3863)	loss_scale 4096.0000 (2701.7257)	mem 17019MB
[2024-08-03 12:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:11:43 lr 0.000015	 wd 0.0500	time 0.3472 (0.3516)	loss 1.2723 (1.1737)	grad_norm 0.3980 (0.3859)	loss_scale 4096.0000 (2980.0240)	mem 17019MB
[2024-08-03 12:19:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:11:01 lr 0.000015	 wd 0.0500	time 0.3586 (0.3477)	loss 1.1828 (1.1677)	grad_norm 0.3698 (0.3869)	loss_scale 4096.0000 (3165.7105)	mem 17019MB
[2024-08-03 12:19:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:10:21 lr 0.000015	 wd 0.0500	time 0.3021 (0.3448)	loss 1.1714 (1.1588)	grad_norm 0.3888 (0.3862)	loss_scale 4096.0000 (3298.4194)	mem 17019MB
[2024-08-03 12:20:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:43 lr 0.000015	 wd 0.0500	time 0.3153 (0.3426)	loss 1.2240 (1.1580)	grad_norm 0.3761 (0.3858)	loss_scale 4096.0000 (3397.9925)	mem 17019MB
[2024-08-03 12:21:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:09:07 lr 0.000015	 wd 0.0500	time 0.3176 (0.3415)	loss 1.3204 (1.1587)	grad_norm 0.3851 (0.3854)	loss_scale 4096.0000 (3475.4628)	mem 17019MB
[2024-08-03 12:21:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:30 lr 0.000015	 wd 0.0500	time 0.2987 (0.3400)	loss 1.3687 (1.1575)	grad_norm 0.3590 (nan)	loss_scale 2048.0000 (3467.8921)	mem 17019MB
[2024-08-03 12:22:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:55 lr 0.000015	 wd 0.0500	time 0.3440 (0.3391)	loss 1.3890 (1.1534)	grad_norm 0.4783 (nan)	loss_scale 2048.0000 (3338.9282)	mem 17019MB
[2024-08-03 12:22:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:20 lr 0.000015	 wd 0.0500	time 0.3198 (0.3381)	loss 1.1764 (1.1574)	grad_norm 0.3828 (nan)	loss_scale 2048.0000 (3231.4405)	mem 17019MB
[2024-08-03 12:23:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:45 lr 0.000015	 wd 0.0500	time 0.3095 (0.3375)	loss 0.7251 (1.1558)	grad_norm 0.3797 (nan)	loss_scale 2048.0000 (3140.4766)	mem 17019MB
[2024-08-03 12:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:06:11 lr 0.000015	 wd 0.0500	time 0.2977 (0.3372)	loss 1.3255 (1.1544)	grad_norm 0.3795 (nan)	loss_scale 2048.0000 (3062.4982)	mem 17019MB
[2024-08-03 12:24:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:37 lr 0.000015	 wd 0.0500	time 0.3092 (0.3365)	loss 1.1967 (1.1549)	grad_norm 0.6753 (nan)	loss_scale 2048.0000 (2994.9101)	mem 17019MB
[2024-08-03 12:24:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:05:03 lr 0.000015	 wd 0.0500	time 0.3306 (0.3360)	loss 0.9119 (1.1538)	grad_norm 0.3712 (nan)	loss_scale 2048.0000 (2935.7651)	mem 17019MB
[2024-08-03 12:25:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:29 lr 0.000015	 wd 0.0500	time 0.3128 (0.3358)	loss 1.0250 (1.1551)	grad_norm 0.4001 (nan)	loss_scale 2048.0000 (2883.5744)	mem 17019MB
[2024-08-03 12:26:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:55 lr 0.000015	 wd 0.0500	time 0.3427 (0.3359)	loss 1.2108 (1.1555)	grad_norm 0.3696 (nan)	loss_scale 2048.0000 (2837.1793)	mem 17019MB
[2024-08-03 12:26:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:22 lr 0.000015	 wd 0.0500	time 0.3457 (0.3360)	loss 1.0693 (1.1556)	grad_norm 0.4063 (nan)	loss_scale 2048.0000 (2795.6654)	mem 17019MB
[2024-08-03 12:27:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:48 lr 0.000015	 wd 0.0500	time 0.3118 (0.3363)	loss 1.3725 (1.1555)	grad_norm 0.3582 (nan)	loss_scale 2048.0000 (2758.3008)	mem 17019MB
[2024-08-03 12:27:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:15 lr 0.000014	 wd 0.0500	time 0.3346 (0.3362)	loss 1.3453 (1.1555)	grad_norm 0.3749 (nan)	loss_scale 2048.0000 (2724.4931)	mem 17019MB
[2024-08-03 12:28:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:41 lr 0.000014	 wd 0.0500	time 0.3078 (0.3361)	loss 1.1926 (1.1541)	grad_norm 0.3766 (nan)	loss_scale 2048.0000 (2693.7574)	mem 17019MB
[2024-08-03 12:28:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:07 lr 0.000014	 wd 0.0500	time 0.3378 (0.3360)	loss 1.3270 (1.1531)	grad_norm 0.3961 (nan)	loss_scale 2048.0000 (2665.6932)	mem 17019MB
[2024-08-03 12:29:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:34 lr 0.000014	 wd 0.0500	time 0.3581 (0.3360)	loss 0.9365 (1.1557)	grad_norm 0.3531 (nan)	loss_scale 2048.0000 (2639.9667)	mem 17019MB
[2024-08-03 12:29:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.2917 (0.3352)	loss 0.7888 (1.1545)	grad_norm 0.3761 (nan)	loss_scale 2048.0000 (2616.2975)	mem 17019MB
[2024-08-03 12:29:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 13 training takes 0:14:01
[2024-08-03 12:30:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.857 (11.857)	Loss 0.4810 (0.4810)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 12:30:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.166 Acc@5 97.920
[2024-08-03 12:30:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 12:30:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.18%
[2024-08-03 12:30:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:56:48 lr 0.000014	 wd 0.0500	time 11.4344 (11.4344)	loss 1.4128 (1.4128)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:31:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:17:34 lr 0.000014	 wd 0.0500	time 0.3035 (0.4389)	loss 1.1718 (1.1711)	grad_norm 0.3724 (0.3860)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:31:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:42 lr 0.000014	 wd 0.0500	time 0.3181 (0.3834)	loss 1.4392 (1.1690)	grad_norm 0.3612 (0.3817)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:13:18 lr 0.000014	 wd 0.0500	time 0.3020 (0.3626)	loss 0.9231 (1.1723)	grad_norm 0.4193 (0.3842)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:32:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:12:24 lr 0.000014	 wd 0.0500	time 0.3315 (0.3540)	loss 1.5810 (1.1720)	grad_norm 0.3775 (0.3831)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:33:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:39 lr 0.000014	 wd 0.0500	time 0.3432 (0.3496)	loss 0.9659 (1.1677)	grad_norm 0.4013 (0.3835)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:59 lr 0.000014	 wd 0.0500	time 0.3080 (0.3465)	loss 0.9661 (1.1668)	grad_norm 0.3945 (0.3848)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:10:20 lr 0.000014	 wd 0.0500	time 0.3179 (0.3441)	loss 1.1224 (1.1674)	grad_norm 0.3989 (0.3856)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:34:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:42 lr 0.000014	 wd 0.0500	time 0.3086 (0.3420)	loss 0.7978 (1.1617)	grad_norm 0.3564 (0.3859)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:35:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:09:06 lr 0.000014	 wd 0.0500	time 0.3527 (0.3409)	loss 0.8449 (1.1607)	grad_norm 0.3833 (0.3858)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:36:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:31 lr 0.000014	 wd 0.0500	time 0.3318 (0.3402)	loss 0.9962 (1.1580)	grad_norm 0.3965 (0.3906)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:36:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:55 lr 0.000014	 wd 0.0500	time 0.3288 (0.3393)	loss 1.3740 (1.1573)	grad_norm 0.3952 (0.3914)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:37:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:23 lr 0.000014	 wd 0.0500	time 0.3134 (0.3407)	loss 1.4609 (1.1569)	grad_norm 0.3946 (0.3922)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:37:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:48 lr 0.000014	 wd 0.0500	time 0.3208 (0.3400)	loss 1.2349 (1.1601)	grad_norm 0.3843 (0.3917)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:38:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:14 lr 0.000014	 wd 0.0500	time 0.3470 (0.3397)	loss 1.2303 (1.1597)	grad_norm 0.3920 (0.3912)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:38:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:40 lr 0.000014	 wd 0.0500	time 0.3227 (0.3395)	loss 1.3811 (1.1587)	grad_norm 0.3751 (0.3909)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:39:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:05:06 lr 0.000014	 wd 0.0500	time 0.3467 (0.3393)	loss 0.7611 (1.1577)	grad_norm 0.3838 (0.3909)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:32 lr 0.000014	 wd 0.0500	time 0.3502 (0.3392)	loss 0.9287 (1.1566)	grad_norm 0.4630 (0.3933)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:40:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:58 lr 0.000013	 wd 0.0500	time 0.3396 (0.3390)	loss 0.8917 (1.1572)	grad_norm 0.4244 (0.3934)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:41:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:23 lr 0.000013	 wd 0.0500	time 0.3465 (0.3389)	loss 1.1482 (1.1575)	grad_norm 0.3924 (0.3932)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:41:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:50 lr 0.000013	 wd 0.0500	time 0.3027 (0.3388)	loss 1.2054 (1.1569)	grad_norm 0.3793 (0.3928)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:42:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:16 lr 0.000013	 wd 0.0500	time 0.3358 (0.3386)	loss 1.3357 (1.1563)	grad_norm 0.3817 (0.3922)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:42:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:42 lr 0.000013	 wd 0.0500	time 0.2960 (0.3384)	loss 1.2981 (1.1554)	grad_norm 0.3803 (0.3918)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:43:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:08 lr 0.000013	 wd 0.0500	time 0.3341 (0.3384)	loss 0.8535 (1.1544)	grad_norm 0.3772 (0.3914)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:43:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:34 lr 0.000013	 wd 0.0500	time 0.3138 (0.3384)	loss 0.8591 (1.1536)	grad_norm 0.3979 (0.3912)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 12:44:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.3090 (0.3375)	loss 1.2357 (1.1525)	grad_norm 0.3794 (0.3908)	loss_scale 4096.0000 (2077.4794)	mem 17019MB
[2024-08-03 12:44:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 14 training takes 0:14:06
[2024-08-03 12:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.134 (12.134)	Loss 0.4724 (0.4724)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 12:45:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.208 Acc@5 97.916
[2024-08-03 12:45:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 12:45:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-03 12:45:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 12:45:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 12:45:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:30:26 lr 0.000013	 wd 0.0500	time 10.8021 (10.8021)	loss 1.2491 (1.2491)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:45:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:17:21 lr 0.000013	 wd 0.0500	time 0.2984 (0.4335)	loss 0.7676 (1.1582)	grad_norm 0.3839 (0.4710)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:46:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:46 lr 0.000013	 wd 0.0500	time 0.3076 (0.3852)	loss 1.4283 (1.1441)	grad_norm 0.3860 (0.4298)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:13:26 lr 0.000013	 wd 0.0500	time 0.3044 (0.3662)	loss 0.9238 (1.1440)	grad_norm 0.3799 (0.4170)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:47:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:12:31 lr 0.000013	 wd 0.0500	time 0.3202 (0.3574)	loss 0.7940 (1.1428)	grad_norm 0.3712 (0.4087)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:47:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:11:43 lr 0.000013	 wd 0.0500	time 0.3013 (0.3516)	loss 0.9452 (1.1540)	grad_norm 0.3918 (0.4069)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:48:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:11:01 lr 0.000013	 wd 0.0500	time 0.3387 (0.3479)	loss 1.3714 (1.1568)	grad_norm 0.3807 (0.4057)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:49:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:10:23 lr 0.000013	 wd 0.0500	time 0.3137 (0.3462)	loss 1.2388 (1.1567)	grad_norm 0.3898 (0.4038)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:49:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:47 lr 0.000013	 wd 0.0500	time 0.3045 (0.3451)	loss 1.3917 (1.1586)	grad_norm 0.3739 (0.4039)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:50:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:09:09 lr 0.000013	 wd 0.0500	time 0.4003 (0.3432)	loss 1.2471 (1.1582)	grad_norm 0.3917 (0.4023)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:50:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:08:33 lr 0.000013	 wd 0.0500	time 0.3369 (0.3421)	loss 0.8533 (1.1582)	grad_norm 0.3805 (0.4019)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:51:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:08:02 lr 0.000013	 wd 0.0500	time 0.3230 (0.3438)	loss 0.8571 (1.1590)	grad_norm 0.3841 (0.4007)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:51:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:07:26 lr 0.000013	 wd 0.0500	time 0.3463 (0.3427)	loss 0.8348 (1.1590)	grad_norm 0.3907 (0.3992)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:52:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:51 lr 0.000013	 wd 0.0500	time 0.3014 (0.3423)	loss 0.8633 (1.1601)	grad_norm 0.3836 (0.3989)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:53:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:06:16 lr 0.000012	 wd 0.0500	time 0.2993 (0.3420)	loss 0.9396 (1.1602)	grad_norm 0.3879 (0.3982)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:53:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:42 lr 0.000012	 wd 0.0500	time 0.3311 (0.3415)	loss 1.0167 (1.1564)	grad_norm 0.4354 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:54:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:05:07 lr 0.000012	 wd 0.0500	time 0.3275 (0.3410)	loss 1.0168 (1.1572)	grad_norm 0.4203 (0.3973)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:54:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:33 lr 0.000012	 wd 0.0500	time 0.3065 (0.3407)	loss 0.8199 (1.1572)	grad_norm 0.3846 (0.3965)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:55:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:59 lr 0.000012	 wd 0.0500	time 0.3192 (0.3405)	loss 1.6603 (1.1552)	grad_norm 0.4054 (0.3962)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:55:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:24 lr 0.000012	 wd 0.0500	time 0.3328 (0.3399)	loss 0.9633 (1.1535)	grad_norm 0.3973 (0.3956)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:56:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:50 lr 0.000012	 wd 0.0500	time 0.3243 (0.3397)	loss 0.8386 (1.1548)	grad_norm 0.3796 (0.3954)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:56:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:16 lr 0.000012	 wd 0.0500	time 0.2917 (0.3396)	loss 0.7833 (1.1535)	grad_norm 0.3750 (0.3951)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:57:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:42 lr 0.000012	 wd 0.0500	time 0.3308 (0.3395)	loss 1.0751 (1.1531)	grad_norm 0.3684 (0.3948)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:58:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:08 lr 0.000012	 wd 0.0500	time 0.2970 (0.3394)	loss 1.0146 (1.1534)	grad_norm 0.3965 (0.3961)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:58:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:34 lr 0.000012	 wd 0.0500	time 0.3240 (0.3394)	loss 1.4536 (1.1538)	grad_norm 0.3637 (0.3958)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:59:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2911 (0.3385)	loss 1.4966 (1.1543)	grad_norm 0.3865 (0.3954)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 12:59:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 15 training takes 0:14:09
[2024-08-03 12:59:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.715 (11.715)	Loss 0.5249 (0.5249)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 12:59:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.156 Acc@5 97.884
[2024-08-03 12:59:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 12:59:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-03 12:59:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:19:37 lr 0.000012	 wd 0.0500	time 11.9813 (11.9813)	loss 1.1975 (1.1975)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:00:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:17:42 lr 0.000012	 wd 0.0500	time 0.3006 (0.4425)	loss 1.0327 (1.1636)	grad_norm 0.3884 (0.4111)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:00:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:14:49 lr 0.000012	 wd 0.0500	time 0.3705 (0.3864)	loss 1.2481 (1.1541)	grad_norm 0.3766 (0.3987)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:01:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:13:30 lr 0.000012	 wd 0.0500	time 0.3007 (0.3680)	loss 1.0028 (1.1472)	grad_norm 0.3730 (0.3969)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:02:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:12:35 lr 0.000012	 wd 0.0500	time 0.3356 (0.3592)	loss 0.9193 (1.1360)	grad_norm 0.3775 (0.3935)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:49 lr 0.000012	 wd 0.0500	time 0.3036 (0.3543)	loss 1.5142 (1.1389)	grad_norm 0.3919 (0.3929)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:03:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:11:06 lr 0.000012	 wd 0.0500	time 0.3006 (0.3503)	loss 1.4375 (1.1397)	grad_norm 0.3534 (0.3937)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:03:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:10:26 lr 0.000012	 wd 0.0500	time 0.3029 (0.3479)	loss 1.0621 (1.1423)	grad_norm 0.3809 (0.3961)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:04:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:49 lr 0.000012	 wd 0.0500	time 0.3446 (0.3466)	loss 1.3119 (1.1438)	grad_norm 0.3879 (0.3945)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:04:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:09:12 lr 0.000012	 wd 0.0500	time 0.3106 (0.3448)	loss 1.1283 (1.1449)	grad_norm 0.3972 (0.3974)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:05:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:35 lr 0.000011	 wd 0.0500	time 0.3375 (0.3432)	loss 1.3491 (1.1448)	grad_norm 0.6378 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:05:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:59 lr 0.000011	 wd 0.0500	time 0.3056 (0.3421)	loss 0.8448 (1.1488)	grad_norm 0.3854 (0.3981)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:06:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:23 lr 0.000011	 wd 0.0500	time 0.3065 (0.3409)	loss 0.8699 (1.1466)	grad_norm 0.3973 (0.3975)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:48 lr 0.000011	 wd 0.0500	time 0.3081 (0.3400)	loss 1.2891 (1.1495)	grad_norm 0.3792 (0.3966)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:07:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:06:13 lr 0.000011	 wd 0.0500	time 0.3068 (0.3393)	loss 1.2260 (1.1495)	grad_norm 0.3890 (0.3956)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:08:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:39 lr 0.000011	 wd 0.0500	time 0.2982 (0.3389)	loss 1.2828 (1.1518)	grad_norm 0.3847 (0.3954)	loss_scale 8192.0000 (4205.1539)	mem 17019MB
[2024-08-03 13:08:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:05:05 lr 0.000011	 wd 0.0500	time 0.3325 (0.3383)	loss 1.1125 (1.1524)	grad_norm 0.3988 (0.3950)	loss_scale 8192.0000 (4454.1761)	mem 17019MB
[2024-08-03 13:09:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:31 lr 0.000011	 wd 0.0500	time 0.2987 (0.3381)	loss 1.0663 (1.1515)	grad_norm 0.4005 (0.3949)	loss_scale 8192.0000 (4673.9189)	mem 17019MB
[2024-08-03 13:09:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:57 lr 0.000011	 wd 0.0500	time 0.2959 (0.3378)	loss 0.8724 (1.1504)	grad_norm 0.3522 (0.3956)	loss_scale 8192.0000 (4869.2593)	mem 17019MB
[2024-08-03 13:10:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:23 lr 0.000011	 wd 0.0500	time 0.3120 (0.3381)	loss 1.0238 (1.1512)	grad_norm 0.3902 (0.3949)	loss_scale 8192.0000 (5044.0484)	mem 17019MB
[2024-08-03 13:10:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:49 lr 0.000011	 wd 0.0500	time 0.3173 (0.3378)	loss 1.3482 (1.1519)	grad_norm 0.3634 (0.3950)	loss_scale 8192.0000 (5201.3673)	mem 17019MB
[2024-08-03 13:11:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:15 lr 0.000011	 wd 0.0500	time 0.2996 (0.3375)	loss 1.3517 (1.1527)	grad_norm 0.3591 (0.3943)	loss_scale 8192.0000 (5343.7106)	mem 17019MB
[2024-08-03 13:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:41 lr 0.000011	 wd 0.0500	time 0.3320 (0.3377)	loss 1.0047 (1.1524)	grad_norm 0.3817 (0.3941)	loss_scale 8192.0000 (5473.1195)	mem 17019MB
[2024-08-03 13:12:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:08 lr 0.000011	 wd 0.0500	time 0.3018 (0.3376)	loss 0.8873 (1.1506)	grad_norm 0.4174 (0.3949)	loss_scale 8192.0000 (5591.2803)	mem 17019MB
[2024-08-03 13:13:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:34 lr 0.000011	 wd 0.0500	time 0.3264 (0.3373)	loss 1.2071 (1.1496)	grad_norm 0.3652 (0.3948)	loss_scale 8192.0000 (5699.5985)	mem 17019MB
[2024-08-03 13:13:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2916 (0.3362)	loss 0.8423 (1.1499)	grad_norm 0.3861 (0.3951)	loss_scale 8192.0000 (5799.2547)	mem 17019MB
[2024-08-03 13:13:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 16 training takes 0:14:03
[2024-08-03 13:13:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.054 (12.054)	Loss 0.4927 (0.4927)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 13:14:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.200 Acc@5 97.906
[2024-08-03 13:14:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 13:14:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-03 13:14:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:06:20 lr 0.000011	 wd 0.0500	time 11.6631 (11.6631)	loss 1.3883 (1.3883)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:14:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:17:35 lr 0.000011	 wd 0.0500	time 0.2974 (0.4395)	loss 1.2261 (1.1335)	grad_norm 0.3690 (0.3842)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:15:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:41 lr 0.000011	 wd 0.0500	time 0.3033 (0.3830)	loss 1.2730 (1.1336)	grad_norm 0.3761 (0.3900)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:16:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:13:29 lr 0.000011	 wd 0.0500	time 0.3158 (0.3677)	loss 1.2275 (1.1489)	grad_norm 0.3915 (0.3909)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:16:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:36 lr 0.000011	 wd 0.0500	time 0.3120 (0.3597)	loss 1.4110 (1.1470)	grad_norm 0.3879 (0.3957)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:17:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:47 lr 0.000010	 wd 0.0500	time 0.3018 (0.3533)	loss 1.3587 (1.1513)	grad_norm 0.3893 (0.3962)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:17:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:11:03 lr 0.000010	 wd 0.0500	time 0.3097 (0.3489)	loss 0.8058 (1.1523)	grad_norm 0.3849 (nan)	loss_scale 4096.0000 (8123.8469)	mem 17019MB
[2024-08-03 13:18:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:10:25 lr 0.000010	 wd 0.0500	time 0.3286 (0.3471)	loss 1.2731 (1.1500)	grad_norm 0.3941 (nan)	loss_scale 4096.0000 (7549.2611)	mem 17019MB
[2024-08-03 13:18:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:47 lr 0.000010	 wd 0.0500	time 0.3033 (0.3452)	loss 1.1750 (1.1482)	grad_norm 0.3862 (nan)	loss_scale 4096.0000 (7118.1423)	mem 17019MB
[2024-08-03 13:19:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:09:10 lr 0.000010	 wd 0.0500	time 0.3076 (0.3434)	loss 1.2834 (1.1497)	grad_norm 0.3865 (nan)	loss_scale 4096.0000 (6782.7214)	mem 17019MB
[2024-08-03 13:19:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:34 lr 0.000010	 wd 0.0500	time 0.3142 (0.3426)	loss 1.0281 (1.1491)	grad_norm 0.3944 (nan)	loss_scale 4096.0000 (6514.3177)	mem 17019MB
[2024-08-03 13:20:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:58 lr 0.000010	 wd 0.0500	time 0.3180 (0.3415)	loss 1.0187 (1.1536)	grad_norm 0.3717 (nan)	loss_scale 4096.0000 (6294.6703)	mem 17019MB
[2024-08-03 13:21:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:24 lr 0.000010	 wd 0.0500	time 0.3599 (0.3414)	loss 0.7574 (1.1535)	grad_norm 0.3761 (nan)	loss_scale 4096.0000 (6111.6003)	mem 17019MB
[2024-08-03 13:21:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:49 lr 0.000010	 wd 0.0500	time 0.3025 (0.3407)	loss 1.4132 (1.1505)	grad_norm 0.3963 (nan)	loss_scale 4096.0000 (5956.6733)	mem 17019MB
[2024-08-03 13:22:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:06:15 lr 0.000010	 wd 0.0500	time 0.3198 (0.3406)	loss 1.3115 (1.1507)	grad_norm 0.3630 (nan)	loss_scale 4096.0000 (5823.8630)	mem 17019MB
[2024-08-03 13:22:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:40 lr 0.000010	 wd 0.0500	time 0.3275 (0.3398)	loss 1.4553 (1.1512)	grad_norm 0.3897 (nan)	loss_scale 4096.0000 (5708.7488)	mem 17019MB
[2024-08-03 13:23:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:05:06 lr 0.000010	 wd 0.0500	time 0.3009 (0.3393)	loss 1.3823 (1.1511)	grad_norm 0.3694 (nan)	loss_scale 4096.0000 (5608.0150)	mem 17019MB
[2024-08-03 13:23:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:31 lr 0.000010	 wd 0.0500	time 0.3148 (0.3388)	loss 1.5204 (1.1529)	grad_norm 0.3715 (nan)	loss_scale 4096.0000 (5519.1252)	mem 17019MB
[2024-08-03 13:24:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:57 lr 0.000010	 wd 0.0500	time 0.3640 (0.3389)	loss 0.9712 (1.1523)	grad_norm 0.4126 (nan)	loss_scale 4096.0000 (5440.1066)	mem 17019MB
[2024-08-03 13:24:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:23 lr 0.000010	 wd 0.0500	time 0.3246 (0.3385)	loss 1.2335 (1.1513)	grad_norm 0.3732 (nan)	loss_scale 4096.0000 (5369.4014)	mem 17019MB
[2024-08-03 13:25:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:49 lr 0.000010	 wd 0.0500	time 0.2954 (0.3383)	loss 1.1923 (1.1516)	grad_norm 0.3833 (nan)	loss_scale 4096.0000 (5305.7631)	mem 17019MB
[2024-08-03 13:26:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:15 lr 0.000010	 wd 0.0500	time 0.3136 (0.3382)	loss 1.4613 (1.1515)	grad_norm 0.4015 (nan)	loss_scale 4096.0000 (5248.1828)	mem 17019MB
[2024-08-03 13:26:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:42 lr 0.000010	 wd 0.0500	time 0.3857 (0.3380)	loss 1.3585 (1.1513)	grad_norm 0.3771 (nan)	loss_scale 4096.0000 (5195.8346)	mem 17019MB
[2024-08-03 13:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:08 lr 0.000010	 wd 0.0500	time 0.3107 (0.3377)	loss 0.7882 (1.1508)	grad_norm 0.3879 (nan)	loss_scale 4096.0000 (5148.0365)	mem 17019MB
[2024-08-03 13:27:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:34 lr 0.000010	 wd 0.0500	time 0.3124 (0.3381)	loss 0.9280 (1.1495)	grad_norm 0.3923 (nan)	loss_scale 4096.0000 (5104.2199)	mem 17019MB
[2024-08-03 13:28:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2977 (0.3372)	loss 1.4489 (1.1492)	grad_norm 0.3692 (nan)	loss_scale 4096.0000 (5063.9072)	mem 17019MB
[2024-08-03 13:28:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 17 training takes 0:14:06
[2024-08-03 13:28:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.390 (12.390)	Loss 0.4839 (0.4839)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 13:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.270 Acc@5 97.878
[2024-08-03 13:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-08-03 13:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 13:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-03 13:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-03 13:28:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:47:30 lr 0.000009	 wd 0.0500	time 11.2114 (11.2114)	loss 1.4267 (1.4267)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:29:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:17:26 lr 0.000009	 wd 0.0500	time 0.3025 (0.4355)	loss 1.4809 (1.1832)	grad_norm 0.3844 (0.4212)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:30:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:41 lr 0.000009	 wd 0.0500	time 0.3294 (0.3829)	loss 1.0220 (1.1750)	grad_norm 0.4232 (0.4046)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:30:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:13:24 lr 0.000009	 wd 0.0500	time 0.3459 (0.3655)	loss 1.4976 (1.1736)	grad_norm 0.3788 (0.4017)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:31:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:12:28 lr 0.000009	 wd 0.0500	time 0.3466 (0.3562)	loss 0.7462 (1.1724)	grad_norm 0.4270 (0.4010)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:31:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:11:42 lr 0.000009	 wd 0.0500	time 0.3050 (0.3508)	loss 1.2812 (1.1640)	grad_norm 0.3681 (0.4018)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:11:01 lr 0.000009	 wd 0.0500	time 0.3261 (0.3477)	loss 1.4816 (1.1567)	grad_norm 0.3885 (0.4021)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:32:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:10:21 lr 0.000009	 wd 0.0500	time 0.3531 (0.3446)	loss 1.4363 (1.1616)	grad_norm 0.3808 (0.4005)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:33:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:43 lr 0.000009	 wd 0.0500	time 0.3014 (0.3428)	loss 1.3166 (1.1645)	grad_norm 0.3747 (0.3990)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:09:07 lr 0.000009	 wd 0.0500	time 0.3417 (0.3419)	loss 0.9128 (1.1643)	grad_norm 0.3924 (0.3996)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:31 lr 0.000009	 wd 0.0500	time 0.3556 (0.3408)	loss 1.4180 (1.1601)	grad_norm 0.3833 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:56 lr 0.000009	 wd 0.0500	time 0.2991 (0.3399)	loss 0.9866 (1.1620)	grad_norm 0.3855 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:35:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:21 lr 0.000009	 wd 0.0500	time 0.3125 (0.3393)	loss 1.0411 (1.1552)	grad_norm 0.3809 (0.3973)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:36:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:48 lr 0.000009	 wd 0.0500	time 0.3483 (0.3395)	loss 1.4592 (1.1554)	grad_norm 0.3936 (0.3967)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:36:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:06:13 lr 0.000009	 wd 0.0500	time 0.3359 (0.3392)	loss 1.1445 (1.1557)	grad_norm 0.3865 (0.3975)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:37:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:39 lr 0.000009	 wd 0.0500	time 0.3761 (0.3386)	loss 1.3037 (1.1553)	grad_norm 0.3642 (0.3973)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:37:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:05:05 lr 0.000009	 wd 0.0500	time 0.3127 (0.3384)	loss 1.4468 (1.1573)	grad_norm 0.4038 (0.3967)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:38:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:31 lr 0.000009	 wd 0.0500	time 0.3042 (0.3385)	loss 1.3228 (1.1592)	grad_norm 0.3853 (0.3978)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:38:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:57 lr 0.000009	 wd 0.0500	time 0.3243 (0.3382)	loss 0.7194 (1.1595)	grad_norm 0.3822 (0.3982)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:39:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:23 lr 0.000009	 wd 0.0500	time 0.3260 (0.3386)	loss 1.4183 (1.1592)	grad_norm 0.3767 (0.3986)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.3761 (0.3385)	loss 1.0569 (1.1588)	grad_norm 0.3788 (0.3980)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:40:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:16 lr 0.000008	 wd 0.0500	time 0.3513 (0.3384)	loss 1.3192 (1.1577)	grad_norm 0.3816 (0.3980)	loss_scale 8192.0000 (4119.3946)	mem 17019MB
[2024-08-03 13:41:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.3497 (0.3381)	loss 1.0156 (1.1579)	grad_norm 0.3647 (0.3977)	loss_scale 8192.0000 (4304.4289)	mem 17019MB
[2024-08-03 13:41:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.3387 (0.3378)	loss 1.3056 (1.1586)	grad_norm 0.3703 (0.3987)	loss_scale 8192.0000 (4473.3803)	mem 17019MB
[2024-08-03 13:42:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.2932 (0.3375)	loss 1.4187 (1.1591)	grad_norm 0.3944 (0.3983)	loss_scale 8192.0000 (4628.2582)	mem 17019MB
[2024-08-03 13:42:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2913 (0.3366)	loss 1.2512 (1.1594)	grad_norm 0.3758 (0.3978)	loss_scale 8192.0000 (4770.7509)	mem 17019MB
[2024-08-03 13:42:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 18 training takes 0:14:04
[2024-08-03 13:43:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.001 (12.001)	Loss 0.4949 (0.4949)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 13:43:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.200 Acc@5 97.882
[2024-08-03 13:43:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 13:43:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 13:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:21:18 lr 0.000008	 wd 0.0500	time 12.0218 (12.0218)	loss 0.7927 (0.7927)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:44:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:57 lr 0.000008	 wd 0.0500	time 0.3181 (0.4486)	loss 1.5856 (1.2058)	grad_norm 0.3672 (0.3981)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:49 lr 0.000008	 wd 0.0500	time 0.3156 (0.3863)	loss 1.1731 (1.1755)	grad_norm 0.3941 (0.3929)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:45:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:13:26 lr 0.000008	 wd 0.0500	time 0.3479 (0.3661)	loss 1.3953 (1.1621)	grad_norm 0.3920 (0.3925)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:45:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:12:29 lr 0.000008	 wd 0.0500	time 0.3586 (0.3566)	loss 0.9581 (1.1556)	grad_norm 0.3797 (0.3934)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:46:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:57 lr 0.000008	 wd 0.0500	time 0.3224 (0.3583)	loss 1.4402 (1.1599)	grad_norm 0.3901 (0.3943)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-03 13:46:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:11:12 lr 0.000008	 wd 0.0500	time 0.3341 (0.3535)	loss 0.8420 (1.1494)	grad_norm 0.4026 (nan)	loss_scale 4096.0000 (7769.4509)	mem 17019MB
[2024-08-03 13:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:10:30 lr 0.000008	 wd 0.0500	time 0.3291 (0.3501)	loss 1.2403 (1.1474)	grad_norm 0.3687 (nan)	loss_scale 4096.0000 (7245.4208)	mem 17019MB
[2024-08-03 13:47:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:51 lr 0.000008	 wd 0.0500	time 0.3050 (0.3473)	loss 1.5169 (1.1491)	grad_norm 0.3884 (nan)	loss_scale 4096.0000 (6852.2347)	mem 17019MB
[2024-08-03 13:48:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:09:13 lr 0.000008	 wd 0.0500	time 0.3071 (0.3453)	loss 1.2692 (1.1496)	grad_norm 0.4179 (nan)	loss_scale 4096.0000 (6546.3263)	mem 17019MB
[2024-08-03 13:49:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:36 lr 0.000008	 wd 0.0500	time 0.3410 (0.3440)	loss 1.4456 (1.1512)	grad_norm 0.3835 (nan)	loss_scale 4096.0000 (6301.5385)	mem 17019MB
[2024-08-03 13:49:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:08:00 lr 0.000008	 wd 0.0500	time 0.3257 (0.3427)	loss 1.1708 (1.1501)	grad_norm 0.3831 (nan)	loss_scale 4096.0000 (6101.2171)	mem 17019MB
[2024-08-03 13:50:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:25 lr 0.000008	 wd 0.0500	time 0.3219 (0.3418)	loss 1.3141 (1.1498)	grad_norm 0.3881 (nan)	loss_scale 4096.0000 (5934.2548)	mem 17019MB
[2024-08-03 13:50:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:49 lr 0.000008	 wd 0.0500	time 0.2887 (0.3410)	loss 0.7749 (1.1487)	grad_norm 0.3942 (nan)	loss_scale 4096.0000 (5792.9593)	mem 17019MB
[2024-08-03 13:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:06:15 lr 0.000008	 wd 0.0500	time 0.3003 (0.3404)	loss 0.7964 (1.1479)	grad_norm 0.3886 (nan)	loss_scale 4096.0000 (5671.8344)	mem 17019MB
[2024-08-03 13:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:40 lr 0.000008	 wd 0.0500	time 0.3500 (0.3396)	loss 1.3570 (1.1491)	grad_norm 0.3669 (nan)	loss_scale 4096.0000 (5566.8488)	mem 17019MB
[2024-08-03 13:52:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:05:05 lr 0.000007	 wd 0.0500	time 0.3020 (0.3391)	loss 1.4048 (1.1509)	grad_norm 0.3778 (nan)	loss_scale 4096.0000 (5474.9781)	mem 17019MB
[2024-08-03 13:52:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:31 lr 0.000007	 wd 0.0500	time 0.3351 (0.3389)	loss 0.8902 (1.1513)	grad_norm 0.3910 (nan)	loss_scale 4096.0000 (5393.9095)	mem 17019MB
[2024-08-03 13:53:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:57 lr 0.000007	 wd 0.0500	time 0.3076 (0.3386)	loss 1.3197 (1.1537)	grad_norm 0.3822 (nan)	loss_scale 4096.0000 (5321.8434)	mem 17019MB
[2024-08-03 13:54:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:23 lr 0.000007	 wd 0.0500	time 0.3429 (0.3385)	loss 1.0931 (1.1527)	grad_norm 0.3922 (nan)	loss_scale 4096.0000 (5257.3593)	mem 17019MB
[2024-08-03 13:54:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:50 lr 0.000007	 wd 0.0500	time 0.3133 (0.3388)	loss 1.4645 (1.1539)	grad_norm 0.3917 (nan)	loss_scale 4096.0000 (5199.3203)	mem 17019MB
[2024-08-03 13:55:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:16 lr 0.000007	 wd 0.0500	time 0.3223 (0.3384)	loss 1.5697 (1.1552)	grad_norm 0.3788 (nan)	loss_scale 4096.0000 (5146.8063)	mem 17019MB
[2024-08-03 13:55:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:42 lr 0.000007	 wd 0.0500	time 0.3350 (0.3380)	loss 1.1319 (1.1544)	grad_norm 0.4052 (nan)	loss_scale 4096.0000 (5099.0641)	mem 17019MB
[2024-08-03 13:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:08 lr 0.000007	 wd 0.0500	time 0.2910 (0.3377)	loss 1.0017 (1.1546)	grad_norm 0.3734 (nan)	loss_scale 4096.0000 (5055.4715)	mem 17019MB
[2024-08-03 13:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:34 lr 0.000007	 wd 0.0500	time 0.3294 (0.3376)	loss 1.0913 (1.1536)	grad_norm 0.3904 (nan)	loss_scale 4096.0000 (5015.5102)	mem 17019MB
[2024-08-03 13:57:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2916 (0.3370)	loss 0.8035 (1.1539)	grad_norm 0.3800 (nan)	loss_scale 4096.0000 (4978.7445)	mem 17019MB
[2024-08-03 13:57:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 19 training takes 0:14:05
[2024-08-03 13:57:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.403 (12.403)	Loss 0.4800 (0.4800)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 13:57:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.210 Acc@5 97.898
[2024-08-03 13:57:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 13:57:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 13:58:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:12:52 lr 0.000007	 wd 0.0500	time 11.8195 (11.8195)	loss 1.5143 (1.5143)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:58:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:17:28 lr 0.000007	 wd 0.0500	time 0.2920 (0.4363)	loss 1.0628 (1.1634)	grad_norm 0.3955 (0.3994)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:59:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:36 lr 0.000007	 wd 0.0500	time 0.3316 (0.3810)	loss 1.3099 (1.1473)	grad_norm 0.4359 (0.3951)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 13:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:13:24 lr 0.000007	 wd 0.0500	time 0.2980 (0.3655)	loss 0.6957 (1.1593)	grad_norm 0.4206 (0.3948)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:00:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:12:29 lr 0.000007	 wd 0.0500	time 0.3033 (0.3564)	loss 1.2977 (1.1587)	grad_norm 0.3989 (0.3931)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:00:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:11:42 lr 0.000007	 wd 0.0500	time 0.3208 (0.3507)	loss 1.2721 (1.1566)	grad_norm 0.3812 (0.3953)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:01:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:11:02 lr 0.000007	 wd 0.0500	time 0.3054 (0.3485)	loss 1.2418 (1.1615)	grad_norm 0.4292 (0.3951)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:01:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:10:23 lr 0.000007	 wd 0.0500	time 0.3309 (0.3460)	loss 1.2573 (1.1599)	grad_norm 0.3771 (0.3952)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:02:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:45 lr 0.000007	 wd 0.0500	time 0.2995 (0.3439)	loss 1.1563 (1.1602)	grad_norm 0.3762 (0.3945)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:03:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:09:09 lr 0.000007	 wd 0.0500	time 0.3383 (0.3429)	loss 1.2498 (1.1594)	grad_norm 0.3947 (0.3937)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:03:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:08:34 lr 0.000007	 wd 0.0500	time 0.3002 (0.3425)	loss 1.3759 (1.1614)	grad_norm 0.3729 (0.3930)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:04:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:59 lr 0.000007	 wd 0.0500	time 0.2982 (0.3418)	loss 1.0616 (1.1592)	grad_norm 0.4430 (0.3926)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:04:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:24 lr 0.000006	 wd 0.0500	time 0.3735 (0.3411)	loss 1.2180 (1.1553)	grad_norm 0.3669 (0.3924)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:49 lr 0.000006	 wd 0.0500	time 0.3188 (0.3410)	loss 1.3406 (1.1549)	grad_norm 0.3846 (0.3953)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:05:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:06:15 lr 0.000006	 wd 0.0500	time 0.3031 (0.3403)	loss 1.0636 (1.1563)	grad_norm 0.4000 (0.4094)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:06:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:40 lr 0.000006	 wd 0.0500	time 0.3498 (0.3398)	loss 1.3142 (1.1590)	grad_norm 0.4371 (0.4083)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:06:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:05:06 lr 0.000006	 wd 0.0500	time 0.3540 (0.3401)	loss 1.2807 (1.1610)	grad_norm 0.4067 (0.4070)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:32 lr 0.000006	 wd 0.0500	time 0.3013 (0.3398)	loss 1.6063 (1.1623)	grad_norm 0.3743 (0.4065)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:08:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:58 lr 0.000006	 wd 0.0500	time 0.3531 (0.3395)	loss 0.7584 (1.1600)	grad_norm 0.3821 (0.4054)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:08:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:24 lr 0.000006	 wd 0.0500	time 0.3207 (0.3399)	loss 0.8279 (1.1612)	grad_norm 0.3792 (0.4048)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:09:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:50 lr 0.000006	 wd 0.0500	time 0.3236 (0.3399)	loss 1.1028 (1.1603)	grad_norm 0.4059 (0.4043)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:09:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:16 lr 0.000006	 wd 0.0500	time 0.3322 (0.3395)	loss 1.2246 (1.1603)	grad_norm 0.3590 (0.4034)	loss_scale 8192.0000 (4220.7711)	mem 17019MB
[2024-08-03 14:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:42 lr 0.000006	 wd 0.0500	time 0.3313 (0.3392)	loss 1.3838 (1.1615)	grad_norm 0.3960 (0.4032)	loss_scale 8192.0000 (4401.1995)	mem 17019MB
[2024-08-03 14:10:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:08 lr 0.000006	 wd 0.0500	time 0.3557 (0.3393)	loss 0.8651 (1.1626)	grad_norm 0.3801 (nan)	loss_scale 4096.0000 (4416.4172)	mem 17019MB
[2024-08-03 14:11:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:34 lr 0.000006	 wd 0.0500	time 0.3439 (0.3392)	loss 0.7824 (1.1620)	grad_norm 0.3996 (nan)	loss_scale 4096.0000 (4403.0721)	mem 17019MB
[2024-08-03 14:11:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2985 (0.3383)	loss 1.0440 (1.1638)	grad_norm 0.3806 (nan)	loss_scale 4096.0000 (4390.7941)	mem 17019MB
[2024-08-03 14:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 20 training takes 0:14:08
[2024-08-03 14:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_20.pth saving......
[2024-08-03 14:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_20.pth saved !!!
[2024-08-03 14:12:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.920 (11.920)	Loss 0.4895 (0.4895)	Acc@1 92.188 (92.188)	Acc@5 98.047 (98.047)	Mem 17019MB
[2024-08-03 14:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.184 Acc@5 97.922
[2024-08-03 14:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 14:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 14:12:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:18:05 lr 0.000006	 wd 0.0500	time 11.9447 (11.9447)	loss 0.9943 (0.9943)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:13:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:17:34 lr 0.000006	 wd 0.0500	time 0.3374 (0.4392)	loss 1.3518 (1.1835)	grad_norm 0.4010 (0.3940)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:13:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:14:40 lr 0.000006	 wd 0.0500	time 0.3196 (0.3825)	loss 1.3114 (1.1780)	grad_norm 0.3899 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:14:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:13:22 lr 0.000006	 wd 0.0500	time 0.3459 (0.3646)	loss 1.1782 (1.1680)	grad_norm 0.4081 (0.4008)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:14:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:12:26 lr 0.000006	 wd 0.0500	time 0.3273 (0.3552)	loss 1.3555 (1.1601)	grad_norm 0.3902 (0.3993)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:15:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:40 lr 0.000006	 wd 0.0500	time 0.3012 (0.3498)	loss 0.8998 (1.1551)	grad_norm 0.3924 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:15:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:59 lr 0.000006	 wd 0.0500	time 0.3435 (0.3465)	loss 1.2935 (1.1504)	grad_norm 0.3727 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:16:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:10:19 lr 0.000006	 wd 0.0500	time 0.2967 (0.3438)	loss 1.2299 (1.1480)	grad_norm 0.3811 (0.3992)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:17:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:42 lr 0.000006	 wd 0.0500	time 0.3307 (0.3424)	loss 1.2036 (1.1466)	grad_norm 0.3936 (0.3977)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:17:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:09:06 lr 0.000005	 wd 0.0500	time 0.3401 (0.3411)	loss 1.4316 (1.1496)	grad_norm 0.3963 (0.3987)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:18:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:30 lr 0.000005	 wd 0.0500	time 0.3176 (0.3400)	loss 1.4873 (1.1529)	grad_norm 0.3775 (0.4006)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:18:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:55 lr 0.000005	 wd 0.0500	time 0.2958 (0.3392)	loss 0.7985 (1.1560)	grad_norm 0.3734 (nan)	loss_scale 2048.0000 (3913.7075)	mem 17019MB
[2024-08-03 14:19:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:20 lr 0.000005	 wd 0.0500	time 0.3344 (0.3387)	loss 1.3828 (1.1529)	grad_norm 0.3914 (nan)	loss_scale 2048.0000 (3758.3614)	mem 17019MB
[2024-08-03 14:19:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:46 lr 0.000005	 wd 0.0500	time 0.2956 (0.3380)	loss 1.1887 (1.1534)	grad_norm 0.3750 (nan)	loss_scale 2048.0000 (3626.8962)	mem 17019MB
[2024-08-03 14:20:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:06:12 lr 0.000005	 wd 0.0500	time 0.3626 (0.3377)	loss 0.8274 (1.1507)	grad_norm 0.4218 (nan)	loss_scale 2048.0000 (3514.1984)	mem 17019MB
[2024-08-03 14:20:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:37 lr 0.000005	 wd 0.0500	time 0.3297 (0.3372)	loss 0.8428 (1.1524)	grad_norm 0.3819 (nan)	loss_scale 2048.0000 (3416.5170)	mem 17019MB
[2024-08-03 14:21:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:05:05 lr 0.000005	 wd 0.0500	time 0.3840 (0.3387)	loss 1.0450 (1.1490)	grad_norm 0.4136 (nan)	loss_scale 2048.0000 (3331.0381)	mem 17019MB
[2024-08-03 14:22:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:31 lr 0.000005	 wd 0.0500	time 0.3615 (0.3390)	loss 1.3421 (1.1498)	grad_norm 0.3665 (nan)	loss_scale 2048.0000 (3255.6096)	mem 17019MB
[2024-08-03 14:22:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:57 lr 0.000005	 wd 0.0500	time 0.3374 (0.3390)	loss 0.7653 (1.1503)	grad_norm 0.3946 (nan)	loss_scale 2048.0000 (3188.5575)	mem 17019MB
[2024-08-03 14:23:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:24 lr 0.000005	 wd 0.0500	time 0.3065 (0.3392)	loss 0.8417 (1.1510)	grad_norm 0.7949 (nan)	loss_scale 2048.0000 (3128.5597)	mem 17019MB
[2024-08-03 14:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:50 lr 0.000005	 wd 0.0500	time 0.3070 (0.3390)	loss 1.0611 (1.1501)	grad_norm 0.4142 (nan)	loss_scale 2048.0000 (3074.5587)	mem 17019MB
[2024-08-03 14:24:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:16 lr 0.000005	 wd 0.0500	time 0.3142 (0.3392)	loss 1.1248 (1.1498)	grad_norm 0.3814 (nan)	loss_scale 2048.0000 (3025.6982)	mem 17019MB
[2024-08-03 14:24:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:42 lr 0.000005	 wd 0.0500	time 0.3369 (0.3391)	loss 1.4412 (1.1484)	grad_norm 0.3935 (nan)	loss_scale 2048.0000 (2981.2776)	mem 17019MB
[2024-08-03 14:25:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:08 lr 0.000005	 wd 0.0500	time 0.3276 (0.3388)	loss 1.5271 (1.1501)	grad_norm 0.4418 (nan)	loss_scale 2048.0000 (2940.7179)	mem 17019MB
[2024-08-03 14:26:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:34 lr 0.000005	 wd 0.0500	time 0.3237 (0.3387)	loss 1.2230 (1.1519)	grad_norm 0.3951 (nan)	loss_scale 2048.0000 (2903.5369)	mem 17019MB
[2024-08-03 14:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.3135 (0.3380)	loss 1.4737 (1.1524)	grad_norm 0.3857 (nan)	loss_scale 2048.0000 (2869.3291)	mem 17019MB
[2024-08-03 14:26:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 21 training takes 0:14:08
[2024-08-03 14:26:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.057 (12.057)	Loss 0.5059 (0.5059)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 14:27:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.200 Acc@5 97.878
[2024-08-03 14:27:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 14:27:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 14:27:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:11:22 lr 0.000005	 wd 0.0500	time 11.7837 (11.7837)	loss 0.9897 (0.9897)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:27:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:17:39 lr 0.000005	 wd 0.0500	time 0.3028 (0.4410)	loss 1.4699 (1.1698)	grad_norm 0.4141 (0.4245)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:28:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:49 lr 0.000005	 wd 0.0500	time 0.3418 (0.3864)	loss 0.7396 (1.1621)	grad_norm 0.3716 (0.4100)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:28:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:13:30 lr 0.000005	 wd 0.0500	time 0.2976 (0.3682)	loss 1.3640 (1.1673)	grad_norm 0.3846 (0.4041)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:29:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:12:32 lr 0.000005	 wd 0.0500	time 0.3214 (0.3580)	loss 0.7501 (1.1731)	grad_norm 0.3755 (0.4029)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:30:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:45 lr 0.000005	 wd 0.0500	time 0.3362 (0.3526)	loss 1.1865 (1.1706)	grad_norm 0.3707 (0.3996)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:30:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:11:04 lr 0.000005	 wd 0.0500	time 0.3795 (0.3493)	loss 0.8665 (1.1722)	grad_norm 0.3696 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:31:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:10:24 lr 0.000005	 wd 0.0500	time 0.3710 (0.3468)	loss 1.3413 (1.1668)	grad_norm 0.3931 (0.3970)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:31:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:46 lr 0.000004	 wd 0.0500	time 0.3011 (0.3448)	loss 0.9286 (1.1656)	grad_norm 0.3800 (0.3989)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:32:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:09:10 lr 0.000004	 wd 0.0500	time 0.3213 (0.3434)	loss 1.0882 (1.1643)	grad_norm 0.4030 (0.3987)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:32:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:35 lr 0.000004	 wd 0.0500	time 0.3280 (0.3434)	loss 1.2658 (1.1647)	grad_norm 0.3752 (0.3985)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:33:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:08:00 lr 0.000004	 wd 0.0500	time 0.3203 (0.3427)	loss 0.8023 (1.1618)	grad_norm 0.4006 (0.3985)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:33:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:25 lr 0.000004	 wd 0.0500	time 0.3296 (0.3421)	loss 1.3699 (1.1628)	grad_norm 0.3779 (0.3985)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:34:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:50 lr 0.000004	 wd 0.0500	time 0.3550 (0.3416)	loss 0.8538 (1.1565)	grad_norm 0.3705 (0.3976)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:35:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:06:15 lr 0.000004	 wd 0.0500	time 0.3349 (0.3412)	loss 1.0036 (1.1512)	grad_norm 0.3845 (0.3974)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:35:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:41 lr 0.000004	 wd 0.0500	time 0.3207 (0.3406)	loss 1.1916 (1.1508)	grad_norm 0.3695 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:36:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:05:06 lr 0.000004	 wd 0.0500	time 0.2988 (0.3403)	loss 1.5096 (1.1509)	grad_norm 0.3784 (0.3977)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:36:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:32 lr 0.000004	 wd 0.0500	time 0.3036 (0.3402)	loss 0.8942 (1.1491)	grad_norm 0.3985 (0.3976)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:37:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:58 lr 0.000004	 wd 0.0500	time 0.3400 (0.3400)	loss 1.3963 (1.1477)	grad_norm 0.3655 (0.3978)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:37:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:24 lr 0.000004	 wd 0.0500	time 0.2907 (0.3401)	loss 0.9314 (1.1485)	grad_norm 0.3757 (0.3976)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:38:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:50 lr 0.000004	 wd 0.0500	time 0.3262 (0.3400)	loss 0.9195 (1.1496)	grad_norm 0.3678 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:39:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:16 lr 0.000004	 wd 0.0500	time 0.3025 (0.3400)	loss 1.2983 (1.1511)	grad_norm 0.3714 (0.3973)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:39:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:42 lr 0.000004	 wd 0.0500	time 0.3423 (0.3398)	loss 1.0767 (1.1510)	grad_norm 0.3970 (0.3980)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:40:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:08 lr 0.000004	 wd 0.0500	time 0.3196 (0.3396)	loss 1.1408 (1.1510)	grad_norm 0.3740 (0.4007)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:40:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:34 lr 0.000004	 wd 0.0500	time 0.3096 (0.3392)	loss 1.3344 (1.1520)	grad_norm 2.2073 (0.4020)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:41:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2913 (0.3382)	loss 1.1012 (1.1516)	grad_norm 0.3690 (0.4020)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:41:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 22 training takes 0:14:09
[2024-08-03 14:41:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.473 (11.473)	Loss 0.4924 (0.4924)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 14:41:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.204 Acc@5 97.898
[2024-08-03 14:41:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 14:41:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 14:41:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:53:42 lr 0.000004	 wd 0.0500	time 11.3599 (11.3599)	loss 0.7639 (0.7639)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:42:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:18:38 lr 0.000004	 wd 0.0500	time 0.3159 (0.4659)	loss 1.4675 (1.1410)	grad_norm 0.3973 (0.4061)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:43:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:15:11 lr 0.000004	 wd 0.0500	time 0.3166 (0.3959)	loss 0.8486 (1.1456)	grad_norm 0.4084 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:43:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:13:42 lr 0.000004	 wd 0.0500	time 0.2994 (0.3737)	loss 1.4163 (1.1366)	grad_norm 0.4016 (0.3968)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:44:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:12:43 lr 0.000004	 wd 0.0500	time 0.3230 (0.3632)	loss 0.8933 (1.1422)	grad_norm 0.3846 (0.3960)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:55 lr 0.000004	 wd 0.0500	time 0.3436 (0.3573)	loss 0.8107 (1.1453)	grad_norm 0.3636 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:45:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:11:10 lr 0.000004	 wd 0.0500	time 0.3212 (0.3526)	loss 1.4251 (1.1446)	grad_norm 0.5347 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:45:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:10:30 lr 0.000004	 wd 0.0500	time 0.3431 (0.3499)	loss 1.1377 (1.1480)	grad_norm 0.4112 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:46:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:52 lr 0.000003	 wd 0.0500	time 0.3019 (0.3479)	loss 1.4320 (1.1496)	grad_norm 0.3753 (0.3977)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 14:46:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:09:14 lr 0.000003	 wd 0.0500	time 0.3001 (0.3460)	loss 1.1410 (1.1503)	grad_norm 0.3798 (nan)	loss_scale 2048.0000 (4086.9079)	mem 17019MB
[2024-08-03 14:47:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:37 lr 0.000003	 wd 0.0500	time 0.2945 (0.3448)	loss 1.1261 (1.1460)	grad_norm 0.3535 (nan)	loss_scale 2048.0000 (3883.2208)	mem 17019MB
[2024-08-03 14:48:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:08:03 lr 0.000003	 wd 0.0500	time 0.3157 (0.3448)	loss 1.0389 (1.1488)	grad_norm 0.3967 (nan)	loss_scale 2048.0000 (3716.5341)	mem 17019MB
[2024-08-03 14:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:27 lr 0.000003	 wd 0.0500	time 0.3166 (0.3437)	loss 1.3949 (1.1518)	grad_norm 0.3922 (nan)	loss_scale 2048.0000 (3577.6053)	mem 17019MB
[2024-08-03 14:49:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:52 lr 0.000003	 wd 0.0500	time 0.3356 (0.3431)	loss 1.2408 (1.1525)	grad_norm 0.3681 (nan)	loss_scale 2048.0000 (3460.0338)	mem 17019MB
[2024-08-03 14:49:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:06:17 lr 0.000003	 wd 0.0500	time 0.2977 (0.3427)	loss 1.2692 (1.1526)	grad_norm 0.4120 (nan)	loss_scale 2048.0000 (3359.2463)	mem 17019MB
[2024-08-03 14:50:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:43 lr 0.000003	 wd 0.0500	time 0.3088 (0.3424)	loss 1.2815 (1.1511)	grad_norm 0.3876 (nan)	loss_scale 2048.0000 (3271.8881)	mem 17019MB
[2024-08-03 14:50:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:05:08 lr 0.000003	 wd 0.0500	time 0.3192 (0.3419)	loss 1.1303 (1.1544)	grad_norm 0.3703 (nan)	loss_scale 2048.0000 (3195.4428)	mem 17019MB
[2024-08-03 14:51:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:34 lr 0.000003	 wd 0.0500	time 0.3138 (0.3417)	loss 0.7154 (1.1518)	grad_norm 0.3938 (nan)	loss_scale 2048.0000 (3127.9859)	mem 17019MB
[2024-08-03 14:52:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:04:00 lr 0.000003	 wd 0.0500	time 0.3352 (0.3428)	loss 1.3024 (1.1510)	grad_norm 0.4100 (nan)	loss_scale 2048.0000 (3068.0200)	mem 17019MB
[2024-08-03 14:52:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:26 lr 0.000003	 wd 0.0500	time 0.2903 (0.3424)	loss 1.2874 (1.1504)	grad_norm 0.3708 (nan)	loss_scale 2048.0000 (3014.3630)	mem 17019MB
[2024-08-03 14:53:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:51 lr 0.000003	 wd 0.0500	time 0.2977 (0.3419)	loss 0.9265 (1.1500)	grad_norm 0.4309 (nan)	loss_scale 2048.0000 (2966.0690)	mem 17019MB
[2024-08-03 14:53:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:17 lr 0.000003	 wd 0.0500	time 0.3097 (0.3413)	loss 1.3906 (1.1503)	grad_norm 0.4300 (nan)	loss_scale 2048.0000 (2922.3722)	mem 17019MB
[2024-08-03 14:54:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:42 lr 0.000003	 wd 0.0500	time 0.3537 (0.3410)	loss 1.3105 (1.1488)	grad_norm 0.4375 (nan)	loss_scale 2048.0000 (2882.6461)	mem 17019MB
[2024-08-03 14:54:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:08 lr 0.000003	 wd 0.0500	time 0.3227 (0.3407)	loss 1.3588 (1.1489)	grad_norm 0.3776 (nan)	loss_scale 2048.0000 (2846.3729)	mem 17019MB
[2024-08-03 14:55:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:34 lr 0.000003	 wd 0.0500	time 0.3359 (0.3414)	loss 0.8080 (1.1488)	grad_norm 0.3840 (nan)	loss_scale 2048.0000 (2813.1212)	mem 17019MB
[2024-08-03 14:55:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2914 (0.3413)	loss 0.8610 (1.1480)	grad_norm 0.3924 (nan)	loss_scale 2048.0000 (2782.5286)	mem 17019MB
[2024-08-03 14:56:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 23 training takes 0:14:17
[2024-08-03 14:56:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.630 (11.630)	Loss 0.5093 (0.5093)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 14:56:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.214 Acc@5 97.878
[2024-08-03 14:56:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 14:56:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 14:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:16:52 lr 0.000003	 wd 0.0500	time 11.9153 (11.9153)	loss 1.2217 (1.2217)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:57:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:17:52 lr 0.000003	 wd 0.0500	time 0.3032 (0.4464)	loss 0.9451 (1.1550)	grad_norm 0.3669 (0.4018)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:57:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:14:57 lr 0.000003	 wd 0.0500	time 0.3025 (0.3899)	loss 1.2299 (1.1447)	grad_norm 0.3674 (0.3942)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:58:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:13:34 lr 0.000003	 wd 0.0500	time 0.3192 (0.3698)	loss 1.3634 (1.1588)	grad_norm 0.3792 (0.3944)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:58:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:12:37 lr 0.000003	 wd 0.0500	time 0.2942 (0.3604)	loss 0.7176 (1.1584)	grad_norm 0.4035 (0.3954)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 14:59:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:11:50 lr 0.000003	 wd 0.0500	time 0.3581 (0.3547)	loss 1.4218 (1.1587)	grad_norm 0.4080 (0.4006)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:00:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:11:07 lr 0.000003	 wd 0.0500	time 0.3134 (0.3512)	loss 0.6821 (1.1537)	grad_norm 0.3844 (0.3996)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:00:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:10:28 lr 0.000003	 wd 0.0500	time 0.2940 (0.3488)	loss 1.1912 (1.1524)	grad_norm 0.3725 (0.3983)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:01:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:49 lr 0.000003	 wd 0.0500	time 0.3183 (0.3463)	loss 1.1995 (1.1545)	grad_norm 0.3880 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:01:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:09:14 lr 0.000003	 wd 0.0500	time 0.3422 (0.3462)	loss 0.9430 (1.1583)	grad_norm 0.3700 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:02:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:37 lr 0.000003	 wd 0.0500	time 0.3258 (0.3448)	loss 0.8025 (1.1553)	grad_norm 0.3891 (0.3989)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:02:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:08:01 lr 0.000003	 wd 0.0500	time 0.3143 (0.3436)	loss 0.8635 (1.1547)	grad_norm 0.3961 (0.3987)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:03:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:26 lr 0.000002	 wd 0.0500	time 0.2879 (0.3427)	loss 0.8911 (1.1527)	grad_norm 0.3876 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:03:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:51 lr 0.000002	 wd 0.0500	time 0.3210 (0.3422)	loss 0.7657 (1.1498)	grad_norm 0.4007 (0.3976)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:04:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:06:16 lr 0.000002	 wd 0.0500	time 0.2984 (0.3417)	loss 1.4407 (1.1508)	grad_norm 0.3944 (0.3980)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:05:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:41 lr 0.000002	 wd 0.0500	time 0.3427 (0.3409)	loss 1.2395 (1.1469)	grad_norm 0.3988 (0.3983)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:05:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:05:06 lr 0.000002	 wd 0.0500	time 0.3163 (0.3403)	loss 1.2854 (1.1459)	grad_norm 0.3889 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:06:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:32 lr 0.000002	 wd 0.0500	time 0.3322 (0.3400)	loss 1.4993 (1.1474)	grad_norm 0.3985 (0.3979)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:06:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:58 lr 0.000002	 wd 0.0500	time 0.3527 (0.3397)	loss 1.0817 (1.1473)	grad_norm 0.4559 (0.3976)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:07:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:24 lr 0.000002	 wd 0.0500	time 0.3031 (0.3399)	loss 0.9126 (1.1486)	grad_norm 0.3750 (0.3991)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:07:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:50 lr 0.000002	 wd 0.0500	time 0.3059 (0.3396)	loss 1.2750 (1.1485)	grad_norm 0.4164 (0.3989)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:08:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:16 lr 0.000002	 wd 0.0500	time 0.3153 (0.3396)	loss 0.9927 (1.1487)	grad_norm 0.3931 (0.3989)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:09:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:42 lr 0.000002	 wd 0.0500	time 0.3276 (0.3393)	loss 1.0081 (1.1503)	grad_norm 0.4100 (0.3984)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:09:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:08 lr 0.000002	 wd 0.0500	time 0.2992 (0.3391)	loss 1.3776 (1.1527)	grad_norm 0.4349 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:10:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:34 lr 0.000002	 wd 0.0500	time 0.3409 (0.3390)	loss 1.2170 (1.1506)	grad_norm 0.3739 (0.3979)	loss_scale 4096.0000 (2053.1179)	mem 17019MB
[2024-08-03 15:10:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2924 (0.3382)	loss 0.9307 (1.1511)	grad_norm 0.3652 (0.3978)	loss_scale 4096.0000 (2134.8005)	mem 17019MB
[2024-08-03 15:10:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 24 training takes 0:14:10
[2024-08-03 15:10:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 10.518 (10.518)	Loss 0.4907 (0.4907)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 15:11:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.252 Acc@5 97.896
[2024-08-03 15:11:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-08-03 15:11:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 15:11:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:30:54 lr 0.000002	 wd 0.0500	time 12.2519 (12.2519)	loss 1.2599 (1.2599)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:12:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:17:43 lr 0.000002	 wd 0.0500	time 0.2990 (0.4429)	loss 1.1906 (1.2086)	grad_norm 0.3852 (0.3910)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:12:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:14:47 lr 0.000002	 wd 0.0500	time 0.3266 (0.3857)	loss 1.4294 (1.1781)	grad_norm 0.3835 (0.3910)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:13:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:13:30 lr 0.000002	 wd 0.0500	time 0.3296 (0.3682)	loss 1.1294 (1.1618)	grad_norm 0.3810 (0.3902)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:13:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:12:33 lr 0.000002	 wd 0.0500	time 0.3163 (0.3583)	loss 1.2915 (1.1626)	grad_norm 0.3739 (0.3930)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:14:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:11:46 lr 0.000002	 wd 0.0500	time 0.3026 (0.3528)	loss 1.4065 (1.1609)	grad_norm 0.3894 (0.3932)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:14:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:11:05 lr 0.000002	 wd 0.0500	time 0.3217 (0.3501)	loss 0.8225 (1.1510)	grad_norm 0.3846 (0.3925)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:15:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:10:27 lr 0.000002	 wd 0.0500	time 0.3424 (0.3481)	loss 0.8028 (1.1526)	grad_norm 0.3986 (0.3928)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:15:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:49 lr 0.000002	 wd 0.0500	time 0.3281 (0.3461)	loss 1.3507 (1.1517)	grad_norm 0.4269 (0.3932)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:16:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:09:11 lr 0.000002	 wd 0.0500	time 0.3192 (0.3444)	loss 1.4272 (1.1515)	grad_norm 0.3905 (0.4000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:16:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:35 lr 0.000002	 wd 0.0500	time 0.3351 (0.3432)	loss 1.5075 (1.1528)	grad_norm 0.3994 (0.4009)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:17:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:08:00 lr 0.000002	 wd 0.0500	time 0.2970 (0.3425)	loss 1.5420 (1.1536)	grad_norm 0.4098 (0.4000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:18:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:24 lr 0.000002	 wd 0.0500	time 0.3087 (0.3416)	loss 0.9706 (1.1502)	grad_norm 0.3734 (0.3995)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:50 lr 0.000002	 wd 0.0500	time 0.3063 (0.3412)	loss 0.8197 (1.1490)	grad_norm 0.4073 (0.3996)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:19:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:06:16 lr 0.000002	 wd 0.0500	time 0.3224 (0.3416)	loss 0.7959 (1.1486)	grad_norm 0.4084 (0.4021)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:19:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:42 lr 0.000002	 wd 0.0500	time 0.3444 (0.3413)	loss 0.8579 (1.1488)	grad_norm 0.3964 (0.4031)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:20:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:05:07 lr 0.000002	 wd 0.0500	time 0.3030 (0.3407)	loss 0.8066 (1.1479)	grad_norm 0.3784 (0.4039)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:20:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:32 lr 0.000002	 wd 0.0500	time 0.2912 (0.3403)	loss 0.7579 (1.1457)	grad_norm 0.3859 (0.4032)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:58 lr 0.000002	 wd 0.0500	time 0.3125 (0.3400)	loss 0.8755 (1.1477)	grad_norm 0.3837 (0.4025)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:24 lr 0.000002	 wd 0.0500	time 0.3109 (0.3395)	loss 1.6435 (1.1500)	grad_norm 0.3802 (0.4024)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:22:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:50 lr 0.000002	 wd 0.0500	time 0.3239 (0.3392)	loss 0.9876 (1.1505)	grad_norm 0.5004 (0.4020)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:23:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:16 lr 0.000002	 wd 0.0500	time 0.3196 (0.3389)	loss 1.2682 (1.1517)	grad_norm 0.3732 (0.4026)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:23:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:42 lr 0.000001	 wd 0.0500	time 0.3008 (0.3387)	loss 1.1095 (1.1520)	grad_norm 0.3847 (0.4024)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:24:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3355 (0.3384)	loss 1.3342 (1.1514)	grad_norm 0.3791 (0.4028)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:24:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3143 (0.3382)	loss 1.3417 (1.1505)	grad_norm 0.3496 (0.4023)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:25:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2906 (0.3373)	loss 0.7461 (1.1494)	grad_norm 0.4219 (0.4032)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:25:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 25 training takes 0:14:08
[2024-08-03 15:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.253 (13.253)	Loss 0.5151 (0.5151)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 15:25:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.212 Acc@5 97.882
[2024-08-03 15:25:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 15:25:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 15:26:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:13:01 lr 0.000001	 wd 0.0500	time 11.8232 (11.8232)	loss 1.1585 (1.1585)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:26:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:17:36 lr 0.000001	 wd 0.0500	time 0.3289 (0.4398)	loss 1.2988 (1.1624)	grad_norm 0.3696 (0.3963)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:27:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:14:40 lr 0.000001	 wd 0.0500	time 0.3165 (0.3826)	loss 1.0775 (1.1615)	grad_norm 0.4563 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:27:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:13:21 lr 0.000001	 wd 0.0500	time 0.3342 (0.3639)	loss 0.9113 (1.1437)	grad_norm 0.3771 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:28:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:12:29 lr 0.000001	 wd 0.0500	time 0.3366 (0.3564)	loss 1.3384 (1.1422)	grad_norm 0.4002 (0.3960)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:28:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:42 lr 0.000001	 wd 0.0500	time 0.3438 (0.3510)	loss 1.3113 (1.1510)	grad_norm 0.3794 (nan)	loss_scale 2048.0000 (4022.4192)	mem 17019MB
[2024-08-03 15:29:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:11:02 lr 0.000001	 wd 0.0500	time 0.3292 (0.3483)	loss 0.7810 (1.1550)	grad_norm 0.3915 (nan)	loss_scale 2048.0000 (3693.8968)	mem 17019MB
[2024-08-03 15:29:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:10:23 lr 0.000001	 wd 0.0500	time 0.3359 (0.3460)	loss 1.3820 (1.1508)	grad_norm 0.3618 (nan)	loss_scale 2048.0000 (3459.1041)	mem 17019MB
[2024-08-03 15:30:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:45 lr 0.000001	 wd 0.0500	time 0.3360 (0.3441)	loss 1.2312 (1.1548)	grad_norm 0.3953 (nan)	loss_scale 2048.0000 (3282.9363)	mem 17019MB
[2024-08-03 15:31:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:09:08 lr 0.000001	 wd 0.0500	time 0.2993 (0.3427)	loss 1.2709 (1.1540)	grad_norm 0.3760 (nan)	loss_scale 2048.0000 (3145.8735)	mem 17019MB
[2024-08-03 15:31:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:33 lr 0.000001	 wd 0.0500	time 0.2934 (0.3418)	loss 1.1151 (1.1529)	grad_norm 0.3877 (nan)	loss_scale 2048.0000 (3036.1958)	mem 17019MB
[2024-08-03 15:32:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:58 lr 0.000001	 wd 0.0500	time 0.3363 (0.3410)	loss 0.9231 (1.1528)	grad_norm 0.3763 (nan)	loss_scale 2048.0000 (2946.4414)	mem 17019MB
[2024-08-03 15:32:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:22 lr 0.000001	 wd 0.0500	time 0.3053 (0.3401)	loss 1.0712 (1.1544)	grad_norm 0.3873 (nan)	loss_scale 2048.0000 (2871.6336)	mem 17019MB
[2024-08-03 15:33:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:47 lr 0.000001	 wd 0.0500	time 0.3019 (0.3394)	loss 1.2681 (1.1559)	grad_norm 0.4002 (nan)	loss_scale 2048.0000 (2808.3259)	mem 17019MB
[2024-08-03 15:33:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:06:13 lr 0.000001	 wd 0.0500	time 0.3058 (0.3392)	loss 1.3740 (1.1568)	grad_norm 0.3838 (nan)	loss_scale 2048.0000 (2754.0557)	mem 17019MB
[2024-08-03 15:34:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:39 lr 0.000001	 wd 0.0500	time 0.3035 (0.3387)	loss 1.4005 (1.1584)	grad_norm 0.3776 (nan)	loss_scale 2048.0000 (2707.0167)	mem 17019MB
[2024-08-03 15:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:05:05 lr 0.000001	 wd 0.0500	time 0.2916 (0.3385)	loss 1.2053 (1.1558)	grad_norm 0.3881 (nan)	loss_scale 2048.0000 (2665.8538)	mem 17019MB
[2024-08-03 15:35:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:31 lr 0.000001	 wd 0.0500	time 0.3383 (0.3383)	loss 1.2916 (1.1540)	grad_norm 0.3777 (nan)	loss_scale 2048.0000 (2629.5309)	mem 17019MB
[2024-08-03 15:36:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:57 lr 0.000001	 wd 0.0500	time 0.3575 (0.3381)	loss 0.8766 (1.1540)	grad_norm 0.4000 (nan)	loss_scale 2048.0000 (2597.2415)	mem 17019MB
[2024-08-03 15:36:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:23 lr 0.000001	 wd 0.0500	time 0.3034 (0.3379)	loss 1.2877 (1.1550)	grad_norm 0.3980 (nan)	loss_scale 2048.0000 (2568.3493)	mem 17019MB
[2024-08-03 15:37:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.3037 (0.3377)	loss 1.1811 (1.1535)	grad_norm 0.3872 (nan)	loss_scale 2048.0000 (2542.3448)	mem 17019MB
[2024-08-03 15:37:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:15 lr 0.000001	 wd 0.0500	time 0.2986 (0.3373)	loss 1.3633 (1.1524)	grad_norm 0.4052 (nan)	loss_scale 2048.0000 (2518.8158)	mem 17019MB
[2024-08-03 15:38:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:41 lr 0.000001	 wd 0.0500	time 0.3120 (0.3372)	loss 1.2738 (1.1508)	grad_norm 0.3832 (nan)	loss_scale 2048.0000 (2497.4248)	mem 17019MB
[2024-08-03 15:38:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3155 (0.3369)	loss 1.2835 (1.1510)	grad_norm 0.3637 (nan)	loss_scale 2048.0000 (2477.8931)	mem 17019MB
[2024-08-03 15:39:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3369 (0.3369)	loss 0.7092 (1.1515)	grad_norm 0.3848 (nan)	loss_scale 2048.0000 (2459.9883)	mem 17019MB
[2024-08-03 15:39:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2913 (0.3361)	loss 0.9341 (1.1520)	grad_norm 0.3965 (nan)	loss_scale 2048.0000 (2443.5154)	mem 17019MB
[2024-08-03 15:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 26 training takes 0:14:06
[2024-08-03 15:40:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.190 (12.190)	Loss 0.4973 (0.4973)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 15:40:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.238 Acc@5 97.898
[2024-08-03 15:40:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 15:40:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 15:40:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:31:32 lr 0.000001	 wd 0.0500	time 10.8281 (10.8281)	loss 1.3911 (1.3911)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:41:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:17:26 lr 0.000001	 wd 0.0500	time 0.3196 (0.4357)	loss 1.2674 (1.1429)	grad_norm 0.3592 (0.4143)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:41:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:14:30 lr 0.000001	 wd 0.0500	time 0.3344 (0.3782)	loss 1.3760 (1.1549)	grad_norm 0.3840 (0.4045)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:42:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:13:14 lr 0.000001	 wd 0.0500	time 0.3069 (0.3607)	loss 1.3351 (1.1543)	grad_norm 0.3854 (0.4128)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:12:20 lr 0.000001	 wd 0.0500	time 0.3733 (0.3523)	loss 0.9933 (1.1507)	grad_norm 0.6346 (0.4165)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:37 lr 0.000001	 wd 0.0500	time 0.3213 (0.3486)	loss 0.6692 (1.1519)	grad_norm 0.4052 (0.4136)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:44:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:10:57 lr 0.000001	 wd 0.0500	time 0.2964 (0.3455)	loss 1.3097 (1.1518)	grad_norm 0.4017 (0.4093)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:10:18 lr 0.000001	 wd 0.0500	time 0.3252 (0.3432)	loss 0.9773 (1.1535)	grad_norm 0.4339 (0.4067)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:45:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:41 lr 0.000001	 wd 0.0500	time 0.3052 (0.3418)	loss 0.9238 (1.1551)	grad_norm 0.3942 (0.4069)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:45:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:09:05 lr 0.000001	 wd 0.0500	time 0.2998 (0.3407)	loss 1.2241 (1.1531)	grad_norm 0.3693 (0.4056)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:46:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:30 lr 0.000001	 wd 0.0500	time 0.3264 (0.3402)	loss 1.1093 (1.1498)	grad_norm 0.3492 (0.4056)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:46:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:56 lr 0.000001	 wd 0.0500	time 0.3560 (0.3397)	loss 1.1239 (1.1519)	grad_norm 0.3786 (0.4046)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:21 lr 0.000001	 wd 0.0500	time 0.3218 (0.3392)	loss 0.7158 (1.1526)	grad_norm 0.3778 (0.4043)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:47:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:47 lr 0.000001	 wd 0.0500	time 0.3516 (0.3391)	loss 0.8073 (1.1495)	grad_norm 0.3997 (0.4035)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:48:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:06:13 lr 0.000001	 wd 0.0500	time 0.2970 (0.3388)	loss 0.8243 (1.1508)	grad_norm 0.3697 (0.4030)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:49:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:39 lr 0.000001	 wd 0.0500	time 0.2998 (0.3385)	loss 1.4294 (1.1518)	grad_norm 0.3890 (0.4028)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:49:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:05:05 lr 0.000001	 wd 0.0500	time 0.3051 (0.3385)	loss 1.3102 (1.1535)	grad_norm 0.3822 (0.4025)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:50:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:31 lr 0.000001	 wd 0.0500	time 0.3336 (0.3383)	loss 1.3386 (1.1552)	grad_norm 0.3906 (0.4021)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:50:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:57 lr 0.000001	 wd 0.0500	time 0.3170 (0.3384)	loss 1.3951 (1.1548)	grad_norm 0.3816 (0.4015)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:51:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:23 lr 0.000001	 wd 0.0500	time 0.3524 (0.3382)	loss 1.3646 (1.1556)	grad_norm 0.3907 (0.4015)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 15:51:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.3042 (0.3381)	loss 1.3978 (1.1563)	grad_norm 0.3725 (0.4013)	loss_scale 4096.0000 (2068.4698)	mem 17019MB
[2024-08-03 15:52:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:15 lr 0.000001	 wd 0.0500	time 0.3284 (0.3379)	loss 1.4855 (1.1554)	grad_norm 0.3996 (0.4009)	loss_scale 4096.0000 (2164.9729)	mem 17019MB
[2024-08-03 15:52:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:41 lr 0.000001	 wd 0.0500	time 0.3233 (0.3376)	loss 1.1788 (1.1567)	grad_norm 0.4021 (0.4009)	loss_scale 4096.0000 (2252.7070)	mem 17019MB
[2024-08-03 15:53:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3788 (0.3378)	loss 1.2836 (1.1564)	grad_norm 0.3752 (0.4006)	loss_scale 4096.0000 (2332.8153)	mem 17019MB
[2024-08-03 15:54:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3160 (0.3380)	loss 1.3411 (1.1580)	grad_norm 0.4344 (0.4004)	loss_scale 4096.0000 (2406.2507)	mem 17019MB
[2024-08-03 15:54:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.3138 (0.3373)	loss 1.2544 (1.1571)	grad_norm 0.4038 (0.3999)	loss_scale 4096.0000 (2473.8137)	mem 17019MB
[2024-08-03 15:54:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 27 training takes 0:14:08
[2024-08-03 15:54:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.121 (12.121)	Loss 0.4849 (0.4849)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 15:55:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.238 Acc@5 97.896
[2024-08-03 15:55:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 15:55:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 15:55:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:23:12 lr 0.000001	 wd 0.0500	time 10.6287 (10.6287)	loss 0.9571 (0.9571)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:56:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:17:58 lr 0.000000	 wd 0.0500	time 0.3197 (0.4490)	loss 0.9019 (1.1928)	grad_norm 0.3916 (0.3908)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:15:10 lr 0.000000	 wd 0.0500	time 0.3929 (0.3954)	loss 1.5428 (1.1792)	grad_norm 0.3810 (0.3921)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:57:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:13:50 lr 0.000000	 wd 0.0500	time 0.3021 (0.3771)	loss 1.4313 (1.1729)	grad_norm 0.4033 (0.4035)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-03 15:57:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:12:49 lr 0.000000	 wd 0.0500	time 0.3042 (0.3661)	loss 0.7262 (1.1644)	grad_norm 0.3665 (nan)	loss_scale 2048.0000 (4055.1421)	mem 17019MB
[2024-08-03 15:58:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:59 lr 0.000000	 wd 0.0500	time 0.3066 (0.3593)	loss 0.7880 (1.1694)	grad_norm 0.3787 (nan)	loss_scale 2048.0000 (3654.5150)	mem 17019MB
[2024-08-03 15:58:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:11:16 lr 0.000000	 wd 0.0500	time 0.3145 (0.3555)	loss 0.8728 (1.1620)	grad_norm 0.4121 (nan)	loss_scale 2048.0000 (3387.2080)	mem 17019MB
[2024-08-03 15:59:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:10:33 lr 0.000000	 wd 0.0500	time 0.3020 (0.3514)	loss 1.2678 (1.1588)	grad_norm 0.3956 (nan)	loss_scale 2048.0000 (3196.1655)	mem 17019MB
[2024-08-03 15:59:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:53 lr 0.000000	 wd 0.0500	time 0.3187 (0.3486)	loss 0.8351 (1.1593)	grad_norm 0.3830 (nan)	loss_scale 2048.0000 (3052.8240)	mem 17019MB
[2024-08-03 16:00:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:09:15 lr 0.000000	 wd 0.0500	time 0.3446 (0.3466)	loss 0.8523 (1.1559)	grad_norm 0.3955 (nan)	loss_scale 2048.0000 (2941.3008)	mem 17019MB
[2024-08-03 16:01:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:38 lr 0.000000	 wd 0.0500	time 0.3018 (0.3451)	loss 1.0054 (1.1575)	grad_norm 0.3820 (nan)	loss_scale 2048.0000 (2852.0599)	mem 17019MB
[2024-08-03 16:01:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:08:02 lr 0.000000	 wd 0.0500	time 0.3091 (0.3439)	loss 1.2525 (1.1576)	grad_norm 0.3901 (nan)	loss_scale 2048.0000 (2779.0300)	mem 17019MB
[2024-08-03 16:02:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:26 lr 0.000000	 wd 0.0500	time 0.3056 (0.3431)	loss 1.4185 (1.1576)	grad_norm 0.4164 (nan)	loss_scale 2048.0000 (2718.1615)	mem 17019MB
[2024-08-03 16:02:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:51 lr 0.000000	 wd 0.0500	time 0.2957 (0.3421)	loss 1.4951 (1.1588)	grad_norm 0.3898 (nan)	loss_scale 2048.0000 (2666.6503)	mem 17019MB
[2024-08-03 16:03:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:06:16 lr 0.000000	 wd 0.0500	time 0.3379 (0.3414)	loss 0.9457 (1.1582)	grad_norm 0.3889 (nan)	loss_scale 2048.0000 (2622.4925)	mem 17019MB
[2024-08-03 16:03:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:41 lr 0.000000	 wd 0.0500	time 0.5678 (0.3409)	loss 1.1798 (1.1567)	grad_norm 0.3816 (nan)	loss_scale 2048.0000 (2584.2185)	mem 17019MB
[2024-08-03 16:04:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:05:07 lr 0.000000	 wd 0.0500	time 0.3109 (0.3404)	loss 1.3875 (1.1594)	grad_norm 0.4176 (nan)	loss_scale 2048.0000 (2550.7258)	mem 17019MB
[2024-08-03 16:04:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:32 lr 0.000000	 wd 0.0500	time 0.3501 (0.3399)	loss 1.2302 (1.1589)	grad_norm 0.4176 (nan)	loss_scale 2048.0000 (2521.1711)	mem 17019MB
[2024-08-03 16:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:58 lr 0.000000	 wd 0.0500	time 0.3228 (0.3399)	loss 0.7851 (1.1584)	grad_norm 0.3833 (nan)	loss_scale 2048.0000 (2494.8984)	mem 17019MB
[2024-08-03 16:06:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:24 lr 0.000000	 wd 0.0500	time 0.3451 (0.3395)	loss 1.0945 (1.1561)	grad_norm 0.3834 (nan)	loss_scale 2048.0000 (2471.3898)	mem 17019MB
[2024-08-03 16:06:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:50 lr 0.000000	 wd 0.0500	time 0.3082 (0.3392)	loss 1.1002 (1.1561)	grad_norm 0.4584 (nan)	loss_scale 2048.0000 (2450.2309)	mem 17019MB
[2024-08-03 16:07:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:16 lr 0.000000	 wd 0.0500	time 0.3095 (0.3390)	loss 0.9705 (1.1561)	grad_norm 0.5924 (nan)	loss_scale 2048.0000 (2431.0861)	mem 17019MB
[2024-08-03 16:07:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:42 lr 0.000000	 wd 0.0500	time 0.3113 (0.3388)	loss 1.2599 (1.1551)	grad_norm 0.4005 (nan)	loss_scale 2048.0000 (2413.6811)	mem 17019MB
[2024-08-03 16:08:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:08 lr 0.000000	 wd 0.0500	time 0.3766 (0.3387)	loss 1.2333 (1.1550)	grad_norm 0.4006 (nan)	loss_scale 2048.0000 (2397.7888)	mem 17019MB
[2024-08-03 16:08:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3475 (0.3388)	loss 1.4966 (1.1537)	grad_norm 0.3647 (nan)	loss_scale 2048.0000 (2383.2203)	mem 17019MB
[2024-08-03 16:09:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2905 (0.3380)	loss 1.0745 (1.1530)	grad_norm 0.3658 (nan)	loss_scale 2048.0000 (2369.8169)	mem 17019MB
[2024-08-03 16:09:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 28 training takes 0:14:10
[2024-08-03 16:09:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.255 (12.255)	Loss 0.4927 (0.4927)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-03 16:09:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.210 Acc@5 97.876
[2024-08-03 16:09:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 16:09:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 16:10:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:19:40 lr 0.000000	 wd 0.0500	time 11.9828 (11.9828)	loss 1.2378 (1.2378)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:17:44 lr 0.000000	 wd 0.0500	time 0.3320 (0.4430)	loss 1.4017 (1.1642)	grad_norm 0.3929 (0.3928)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:11:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:14:58 lr 0.000000	 wd 0.0500	time 0.3108 (0.3905)	loss 0.8110 (1.1626)	grad_norm 0.4088 (0.3959)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:11:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:13:37 lr 0.000000	 wd 0.0500	time 0.3233 (0.3713)	loss 0.8628 (1.1595)	grad_norm 0.4018 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:12:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:12:39 lr 0.000000	 wd 0.0500	time 0.3695 (0.3614)	loss 1.1158 (1.1531)	grad_norm 0.3875 (0.3950)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:12:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:11:49 lr 0.000000	 wd 0.0500	time 0.3016 (0.3546)	loss 1.2941 (1.1486)	grad_norm 0.3950 (0.3942)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:13:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:11:07 lr 0.000000	 wd 0.0500	time 0.3127 (0.3509)	loss 0.7166 (1.1517)	grad_norm 0.3999 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:14:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:10:26 lr 0.000000	 wd 0.0500	time 0.3516 (0.3477)	loss 1.1201 (1.1524)	grad_norm 0.3847 (0.3948)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:14:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:47 lr 0.000000	 wd 0.0500	time 0.3258 (0.3451)	loss 0.7608 (1.1532)	grad_norm 0.4182 (0.3946)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-03 16:15:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:09:10 lr 0.000000	 wd 0.0500	time 0.3110 (0.3439)	loss 1.4912 (1.1545)	grad_norm 0.3981 (nan)	loss_scale 1024.0000 (2034.3618)	mem 17019MB
[2024-08-03 16:15:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:35 lr 0.000000	 wd 0.0500	time 0.3460 (0.3429)	loss 1.4402 (1.1554)	grad_norm 0.3883 (nan)	loss_scale 1024.0000 (1933.4266)	mem 17019MB
[2024-08-03 16:16:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:59 lr 0.000000	 wd 0.0500	time 0.3137 (0.3417)	loss 1.2330 (1.1552)	grad_norm 0.3921 (nan)	loss_scale 1024.0000 (1850.8265)	mem 17019MB
[2024-08-03 16:16:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:24 lr 0.000000	 wd 0.0500	time 0.3132 (0.3416)	loss 1.0191 (1.1530)	grad_norm 0.3725 (nan)	loss_scale 1024.0000 (1781.9817)	mem 17019MB
[2024-08-03 16:17:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:49 lr 0.000000	 wd 0.0500	time 0.2916 (0.3408)	loss 1.1230 (1.1564)	grad_norm 0.4133 (nan)	loss_scale 512.0000 (1711.1268)	mem 17019MB
[2024-08-03 16:17:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:06:14 lr 0.000000	 wd 0.0500	time 0.3007 (0.3402)	loss 1.1194 (1.1514)	grad_norm 0.4307 (nan)	loss_scale 512.0000 (1625.5360)	mem 17019MB
[2024-08-03 16:18:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:40 lr 0.000000	 wd 0.0500	time 0.3086 (0.3397)	loss 1.0163 (1.1517)	grad_norm 0.3818 (nan)	loss_scale 512.0000 (1551.3498)	mem 17019MB
[2024-08-03 16:19:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:05:06 lr 0.000000	 wd 0.0500	time 0.3019 (0.3393)	loss 0.8135 (1.1538)	grad_norm 0.3746 (nan)	loss_scale 512.0000 (1486.4310)	mem 17019MB
[2024-08-03 16:19:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:31 lr 0.000000	 wd 0.0500	time 0.3553 (0.3391)	loss 1.3478 (1.1564)	grad_norm 0.3903 (nan)	loss_scale 512.0000 (1429.1452)	mem 17019MB
[2024-08-03 16:20:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:57 lr 0.000000	 wd 0.0500	time 0.3467 (0.3389)	loss 1.1627 (1.1548)	grad_norm 0.3733 (nan)	loss_scale 512.0000 (1378.2210)	mem 17019MB
[2024-08-03 16:20:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:23 lr 0.000000	 wd 0.0500	time 0.3021 (0.3386)	loss 0.7498 (1.1551)	grad_norm 0.3975 (nan)	loss_scale 512.0000 (1332.6544)	mem 17019MB
[2024-08-03 16:21:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:49 lr 0.000000	 wd 0.0500	time 0.3129 (0.3385)	loss 0.9104 (1.1569)	grad_norm 0.3897 (nan)	loss_scale 512.0000 (1291.6422)	mem 17019MB
[2024-08-03 16:21:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:16 lr 0.000000	 wd 0.0500	time 0.3718 (0.3383)	loss 0.7511 (1.1578)	grad_norm 0.3557 (nan)	loss_scale 512.0000 (1254.5340)	mem 17019MB
[2024-08-03 16:22:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:42 lr 0.000000	 wd 0.0500	time 0.3075 (0.3383)	loss 0.8300 (1.1583)	grad_norm 0.3954 (nan)	loss_scale 512.0000 (1220.7978)	mem 17019MB
[2024-08-03 16:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:08 lr 0.000000	 wd 0.0500	time 0.3000 (0.3383)	loss 0.8532 (1.1554)	grad_norm 0.3950 (nan)	loss_scale 512.0000 (1189.9939)	mem 17019MB
[2024-08-03 16:23:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3481 (0.3379)	loss 1.2510 (1.1555)	grad_norm 0.3953 (nan)	loss_scale 512.0000 (1161.7559)	mem 17019MB
[2024-08-03 16:24:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2907 (0.3370)	loss 1.3100 (1.1548)	grad_norm 0.3881 (nan)	loss_scale 512.0000 (1135.7761)	mem 17019MB
[2024-08-03 16:24:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 249): INFO EPOCH 29 training takes 0:14:08
[2024-08-03 16:24:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_29.pth saving......
[2024-08-03 16:24:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft/diffusion_ft_adapter_smt_l_step_cross2/ckpt_epoch_29.pth saved !!!
[2024-08-03 16:24:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.800 (11.800)	Loss 0.4832 (0.4832)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-03 16:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 296): INFO  * Acc@1 86.234 Acc@5 97.900
[2024-08-03 16:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 16:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-03 16:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross2-full-ft] (main.py 189): INFO Training time 7:18:26
