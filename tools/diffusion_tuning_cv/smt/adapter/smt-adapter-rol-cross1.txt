[2024-07-30 09:26:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/config.json
[2024-07-30 09:26:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_sequence_cross1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-30 09:26:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_sequence_cross1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_sequence_cross1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-30 09:27:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft
[2024-07-30 09:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-30 09:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 113): INFO number of params: 4162792
[2024-07-30 09:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1, ignoring auto resume
[2024-07-30 09:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-30 09:27:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-30 09:27:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth'
[2024-07-30 09:27:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 14.338 (14.338)	Loss 0.5083 (0.5083)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2339MB
[2024-07-30 09:27:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.906 Acc@5 97.812
[2024-07-30 09:27:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 09:27:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 168): INFO Start training
[2024-07-30 09:27:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:18:13 lr 0.000000	 wd 0.0500	time 11.9480 (11.9480)	loss 1.5300 (1.5300)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17013MB
[2024-07-30 09:28:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:40 lr 0.000000	 wd 0.0500	time 0.3041 (0.4413)	loss 1.2844 (1.2079)	grad_norm 0.3556 (nan)	loss_scale 16384.0000 (27901.4653)	mem 17013MB
[2024-07-30 09:28:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:42 lr 0.000000	 wd 0.0500	time 0.3110 (0.3832)	loss 1.0245 (1.1925)	grad_norm 0.3470 (nan)	loss_scale 4096.0000 (17280.6368)	mem 17013MB
[2024-07-30 09:29:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:30 lr 0.000000	 wd 0.0500	time 0.3393 (0.3680)	loss 0.9031 (1.1559)	grad_norm 0.4003 (nan)	loss_scale 4096.0000 (12900.3588)	mem 17013MB
[2024-07-30 09:30:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:12:31 lr 0.000001	 wd 0.0500	time 0.3001 (0.3574)	loss 0.9747 (1.1634)	grad_norm 0.3597 (nan)	loss_scale 4096.0000 (10704.7581)	mem 17013MB
[2024-07-30 09:30:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:49 lr 0.000001	 wd 0.0500	time 0.3095 (0.3544)	loss 1.1488 (1.1682)	grad_norm 0.3283 (nan)	loss_scale 4096.0000 (9385.6447)	mem 17013MB
[2024-07-30 09:31:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:11:08 lr 0.000001	 wd 0.0500	time 0.3092 (0.3515)	loss 1.3133 (1.1680)	grad_norm 0.3546 (nan)	loss_scale 4096.0000 (8505.5042)	mem 17013MB
[2024-07-30 09:31:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:10:27 lr 0.000001	 wd 0.0500	time 0.2974 (0.3482)	loss 1.2335 (1.1700)	grad_norm 0.3657 (nan)	loss_scale 4096.0000 (7876.4736)	mem 17013MB
[2024-07-30 09:32:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:48 lr 0.000001	 wd 0.0500	time 0.2919 (0.3455)	loss 1.0591 (1.1692)	grad_norm 0.3387 (nan)	loss_scale 4096.0000 (7404.5044)	mem 17013MB
[2024-07-30 09:32:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:09:11 lr 0.000001	 wd 0.0500	time 0.3081 (0.3442)	loss 1.3676 (1.1689)	grad_norm 0.3554 (nan)	loss_scale 4096.0000 (7037.3008)	mem 17013MB
[2024-07-30 09:33:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:34 lr 0.000002	 wd 0.0500	time 0.3318 (0.3426)	loss 1.4367 (1.1666)	grad_norm 0.3572 (nan)	loss_scale 4096.0000 (6743.4645)	mem 17013MB
[2024-07-30 09:33:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:58 lr 0.000002	 wd 0.0500	time 0.3176 (0.3416)	loss 1.2342 (1.1677)	grad_norm 0.3719 (nan)	loss_scale 4096.0000 (6503.0045)	mem 17013MB
[2024-07-30 09:34:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:23 lr 0.000002	 wd 0.0500	time 0.3044 (0.3407)	loss 0.9221 (1.1674)	grad_norm 0.3429 (nan)	loss_scale 4096.0000 (6302.5878)	mem 17013MB
[2024-07-30 09:35:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:48 lr 0.000002	 wd 0.0500	time 0.3499 (0.3399)	loss 1.2316 (1.1691)	grad_norm 0.3488 (nan)	loss_scale 4096.0000 (6132.9808)	mem 17013MB
[2024-07-30 09:35:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:14 lr 0.000002	 wd 0.0500	time 0.3075 (0.3396)	loss 1.4352 (1.1709)	grad_norm 0.3517 (nan)	loss_scale 4096.0000 (5987.5860)	mem 17013MB
[2024-07-30 09:36:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:39 lr 0.000002	 wd 0.0500	time 0.2969 (0.3390)	loss 1.1847 (1.1749)	grad_norm 0.3546 (nan)	loss_scale 4096.0000 (5861.5643)	mem 17013MB
[2024-07-30 09:36:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:05:05 lr 0.000003	 wd 0.0500	time 0.3070 (0.3385)	loss 1.2171 (1.1721)	grad_norm 0.3459 (nan)	loss_scale 4096.0000 (5751.2854)	mem 17013MB
[2024-07-30 09:37:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:30 lr 0.000003	 wd 0.0500	time 0.2952 (0.3378)	loss 0.8871 (1.1715)	grad_norm 0.3804 (nan)	loss_scale 4096.0000 (5653.9730)	mem 17013MB
[2024-07-30 09:37:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:57 lr 0.000003	 wd 0.0500	time 0.3391 (0.3376)	loss 1.1219 (1.1716)	grad_norm 0.3876 (nan)	loss_scale 4096.0000 (5567.4670)	mem 17013MB
[2024-07-30 09:38:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:23 lr 0.000003	 wd 0.0500	time 0.2955 (0.3375)	loss 1.4160 (1.1711)	grad_norm 0.3520 (nan)	loss_scale 4096.0000 (5490.0621)	mem 17013MB
[2024-07-30 09:38:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:49 lr 0.000003	 wd 0.0500	time 0.3120 (0.3373)	loss 0.8450 (1.1701)	grad_norm 0.3546 (nan)	loss_scale 4096.0000 (5420.3938)	mem 17013MB
[2024-07-30 09:39:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:15 lr 0.000003	 wd 0.0500	time 0.3440 (0.3371)	loss 1.0775 (1.1690)	grad_norm 0.3627 (nan)	loss_scale 4096.0000 (5357.3574)	mem 17013MB
[2024-07-30 09:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:41 lr 0.000004	 wd 0.0500	time 0.3568 (0.3371)	loss 1.5233 (1.1685)	grad_norm 0.3652 (nan)	loss_scale 4096.0000 (5300.0491)	mem 17013MB
[2024-07-30 09:40:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:08 lr 0.000004	 wd 0.0500	time 0.3151 (0.3371)	loss 1.3315 (1.1687)	grad_norm 0.3769 (nan)	loss_scale 4096.0000 (5247.7219)	mem 17013MB
[2024-07-30 09:41:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:34 lr 0.000004	 wd 0.0500	time 0.3310 (0.3370)	loss 1.3100 (1.1686)	grad_norm 0.3462 (nan)	loss_scale 2048.0000 (5179.2820)	mem 17013MB
[2024-07-30 09:41:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2910 (0.3361)	loss 0.8818 (1.1668)	grad_norm 0.3688 (nan)	loss_scale 2048.0000 (5054.0808)	mem 17013MB
[2024-07-30 09:41:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 0 training takes 0:14:03
[2024-07-30 09:41:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_0.pth saving......
[2024-07-30 09:41:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_0.pth saved !!!
[2024-07-30 09:41:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.090 (11.090)	Loss 0.5273 (0.5273)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 09:42:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.904 Acc@5 97.818
[2024-07-30 09:42:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 09:42:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.90%
[2024-07-30 09:42:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 09:42:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 09:42:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:13:12 lr 0.000004	 wd 0.0500	time 10.3885 (10.3885)	loss 1.1107 (1.1107)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:42:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:17:36 lr 0.000004	 wd 0.0500	time 0.2960 (0.4400)	loss 1.1742 (1.1493)	grad_norm 0.3501 (0.3606)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:15:04 lr 0.000004	 wd 0.0500	time 0.2955 (0.3930)	loss 0.7688 (1.1662)	grad_norm 0.3568 (0.3625)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:44:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:13:40 lr 0.000004	 wd 0.0500	time 0.3002 (0.3725)	loss 0.8933 (1.1657)	grad_norm 0.3529 (0.3638)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:39 lr 0.000005	 wd 0.0500	time 0.3563 (0.3614)	loss 0.9744 (1.1805)	grad_norm 0.8906 (0.3747)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:45:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:11:51 lr 0.000005	 wd 0.0500	time 0.3330 (0.3552)	loss 1.4551 (1.1778)	grad_norm 0.3543 (0.3740)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:45:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:11:06 lr 0.000005	 wd 0.0500	time 0.3258 (0.3506)	loss 1.4876 (1.1765)	grad_norm 0.3494 (0.3716)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:46:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:10:26 lr 0.000005	 wd 0.0500	time 0.3103 (0.3474)	loss 0.8723 (1.1735)	grad_norm 0.3404 (0.3723)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:46:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:46 lr 0.000005	 wd 0.0500	time 0.3252 (0.3449)	loss 1.4240 (1.1726)	grad_norm 0.3342 (0.3720)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:47:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:09:10 lr 0.000005	 wd 0.0500	time 0.3433 (0.3434)	loss 1.1672 (1.1717)	grad_norm 0.3731 (0.3710)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:47:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:34 lr 0.000006	 wd 0.0500	time 0.3307 (0.3423)	loss 1.3611 (1.1704)	grad_norm 0.3775 (0.3705)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:48:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:59 lr 0.000006	 wd 0.0500	time 0.3178 (0.3420)	loss 1.4347 (1.1690)	grad_norm 0.3485 (0.3709)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:49:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:24 lr 0.000006	 wd 0.0500	time 0.3392 (0.3410)	loss 0.9725 (1.1668)	grad_norm 0.3432 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:49:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:49 lr 0.000006	 wd 0.0500	time 0.2977 (0.3403)	loss 1.5248 (1.1679)	grad_norm 0.3645 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:50:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:06:14 lr 0.000006	 wd 0.0500	time 0.3207 (0.3397)	loss 0.9389 (1.1678)	grad_norm 0.3589 (0.3689)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:50:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:39 lr 0.000006	 wd 0.0500	time 0.3035 (0.3391)	loss 1.4097 (1.1685)	grad_norm 0.3452 (0.3684)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:51:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:05:05 lr 0.000007	 wd 0.0500	time 0.3349 (0.3387)	loss 1.2917 (1.1702)	grad_norm 0.3665 (0.3707)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:32 lr 0.000007	 wd 0.0500	time 0.3112 (0.3392)	loss 0.9956 (1.1704)	grad_norm 0.3628 (0.3704)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:52:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:58 lr 0.000007	 wd 0.0500	time 0.3092 (0.3391)	loss 1.6954 (1.1697)	grad_norm 0.3634 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:52:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:24 lr 0.000007	 wd 0.0500	time 0.3635 (0.3391)	loss 1.4444 (1.1710)	grad_norm 0.3616 (0.3706)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:53:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:50 lr 0.000007	 wd 0.0500	time 0.2961 (0.3392)	loss 1.3991 (1.1715)	grad_norm 0.3741 (0.3705)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:54:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:16 lr 0.000007	 wd 0.0500	time 0.3282 (0.3389)	loss 1.4990 (1.1736)	grad_norm 0.3747 (0.3702)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:54:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.3205 (0.3387)	loss 1.2317 (1.1742)	grad_norm 0.3454 (0.3716)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:55:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.3182 (0.3383)	loss 0.9873 (1.1764)	grad_norm 0.3586 (0.3713)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.3549 (0.3380)	loss 1.2113 (1.1766)	grad_norm 0.3446 (0.3709)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2964 (0.3373)	loss 0.8967 (1.1767)	grad_norm 0.3575 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:56:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:14:06
[2024-07-30 09:56:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.191 (12.191)	Loss 0.5215 (0.5215)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 09:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.928 Acc@5 97.794
[2024-07-30 09:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 09:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.93%
[2024-07-30 09:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 09:56:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 09:56:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:20:59 lr 0.000008	 wd 0.0500	time 10.5754 (10.5754)	loss 1.2460 (1.2460)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:57:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:17:11 lr 0.000008	 wd 0.0500	time 0.2980 (0.4294)	loss 1.4255 (1.1792)	grad_norm 0.3523 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:58:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:24 lr 0.000008	 wd 0.0500	time 0.3009 (0.3757)	loss 1.4136 (1.1712)	grad_norm 0.3459 (0.3681)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:58:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:13:09 lr 0.000008	 wd 0.0500	time 0.3033 (0.3587)	loss 1.1778 (1.1619)	grad_norm 0.3575 (0.3701)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:59:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:12:15 lr 0.000009	 wd 0.0500	time 0.3113 (0.3500)	loss 1.4231 (1.1651)	grad_norm 0.3386 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 09:59:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:11:31 lr 0.000009	 wd 0.0500	time 0.3414 (0.3456)	loss 1.4286 (1.1604)	grad_norm 0.3651 (0.3666)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:00:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:10:50 lr 0.000009	 wd 0.0500	time 0.3118 (0.3420)	loss 1.2861 (1.1618)	grad_norm 0.3451 (0.3663)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:00:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:10:11 lr 0.000009	 wd 0.0500	time 0.3168 (0.3394)	loss 1.3349 (1.1668)	grad_norm 0.3596 (0.3644)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:01:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:35 lr 0.000009	 wd 0.0500	time 0.3159 (0.3381)	loss 1.3897 (1.1697)	grad_norm 0.3447 (0.3646)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:01:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:08:59 lr 0.000009	 wd 0.0500	time 0.3411 (0.3369)	loss 0.9392 (1.1747)	grad_norm 0.3629 (0.3649)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:02:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:08:24 lr 0.000010	 wd 0.0500	time 0.3020 (0.3358)	loss 0.8775 (1.1719)	grad_norm 0.3642 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:02:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:51 lr 0.000010	 wd 0.0500	time 0.3356 (0.3367)	loss 1.3763 (1.1693)	grad_norm 0.3650 (0.3647)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:03:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:07:19 lr 0.000010	 wd 0.0500	time 0.3421 (0.3372)	loss 0.9104 (1.1697)	grad_norm 0.3821 (0.3644)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:04:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:44 lr 0.000010	 wd 0.0500	time 0.3017 (0.3365)	loss 1.4040 (1.1707)	grad_norm 0.3636 (0.3637)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:04:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:06:10 lr 0.000010	 wd 0.0500	time 0.3007 (0.3360)	loss 1.2226 (1.1723)	grad_norm 0.3505 (0.3642)	loss_scale 4096.0000 (2088.9308)	mem 17013MB
[2024-07-30 10:05:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:36 lr 0.000010	 wd 0.0500	time 0.3002 (0.3362)	loss 1.3399 (1.1729)	grad_norm 0.3416 (0.3657)	loss_scale 4096.0000 (2222.6462)	mem 17013MB
[2024-07-30 10:05:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:05:03 lr 0.000011	 wd 0.0500	time 0.3996 (0.3365)	loss 0.9816 (1.1734)	grad_norm 0.3455 (0.3651)	loss_scale 4096.0000 (2339.6577)	mem 17013MB
[2024-07-30 10:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:29 lr 0.000011	 wd 0.0500	time 0.3050 (0.3361)	loss 1.2878 (1.1738)	grad_norm 0.3428 (0.3648)	loss_scale 4096.0000 (2442.9112)	mem 17013MB
[2024-07-30 10:06:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:55 lr 0.000011	 wd 0.0500	time 0.3376 (0.3356)	loss 1.0761 (1.1743)	grad_norm 0.3769 (0.3658)	loss_scale 4096.0000 (2534.6985)	mem 17013MB
[2024-07-30 10:07:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:21 lr 0.000011	 wd 0.0500	time 0.3267 (0.3354)	loss 1.3444 (1.1741)	grad_norm 0.3779 (0.3655)	loss_scale 4096.0000 (2616.8290)	mem 17013MB
[2024-07-30 10:07:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:48 lr 0.000011	 wd 0.0500	time 0.3352 (0.3357)	loss 1.1291 (1.1741)	grad_norm 0.3489 (0.3652)	loss_scale 4096.0000 (2690.7506)	mem 17013MB
[2024-07-30 10:08:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:14 lr 0.000011	 wd 0.0500	time 0.3097 (0.3355)	loss 1.0054 (1.1726)	grad_norm 0.3435 (0.3650)	loss_scale 4096.0000 (2757.6354)	mem 17013MB
[2024-07-30 10:09:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:41 lr 0.000012	 wd 0.0500	time 0.3154 (0.3352)	loss 1.3970 (1.1724)	grad_norm 0.3641 (0.3647)	loss_scale 4096.0000 (2818.4425)	mem 17013MB
[2024-07-30 10:09:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:07 lr 0.000012	 wd 0.0500	time 0.3920 (0.3353)	loss 1.2109 (1.1725)	grad_norm 0.3524 (0.3654)	loss_scale 4096.0000 (2873.9644)	mem 17013MB
[2024-07-30 10:10:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:34 lr 0.000012	 wd 0.0500	time 0.3060 (0.3355)	loss 0.7817 (1.1732)	grad_norm 0.3582 (0.3658)	loss_scale 4096.0000 (2924.8613)	mem 17013MB
[2024-07-30 10:10:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2919 (0.3345)	loss 1.2066 (1.1736)	grad_norm 0.3399 (0.3664)	loss_scale 4096.0000 (2971.6881)	mem 17013MB
[2024-07-30 10:10:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:13:59
[2024-07-30 10:10:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.653 (11.653)	Loss 0.5029 (0.5029)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 10:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.950 Acc@5 97.798
[2024-07-30 10:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 10:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.95%
[2024-07-30 10:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 10:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 10:11:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:13:44 lr 0.000012	 wd 0.0500	time 11.8402 (11.8402)	loss 0.7494 (0.7494)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:11:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:17:29 lr 0.000012	 wd 0.0500	time 0.3372 (0.4371)	loss 1.2452 (1.2089)	grad_norm 0.3606 (0.3693)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:12:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:14:46 lr 0.000012	 wd 0.0500	time 0.3353 (0.3853)	loss 1.5224 (1.1831)	grad_norm 0.4311 (0.3681)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:13:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:13:28 lr 0.000012	 wd 0.0500	time 0.3063 (0.3670)	loss 1.4434 (1.1799)	grad_norm 0.3491 (0.3664)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:13:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:12:32 lr 0.000013	 wd 0.0500	time 0.3145 (0.3580)	loss 1.4145 (1.1724)	grad_norm 0.3556 (0.3651)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:14:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:46 lr 0.000013	 wd 0.0500	time 0.2959 (0.3531)	loss 0.9980 (1.1727)	grad_norm 0.3571 (0.3665)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:11:01 lr 0.000013	 wd 0.0500	time 0.2868 (0.3480)	loss 1.0629 (1.1675)	grad_norm 0.3705 (0.3663)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:15:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:10:21 lr 0.000013	 wd 0.0500	time 0.3345 (0.3449)	loss 1.5141 (1.1651)	grad_norm 0.3530 (0.3654)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:15:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:44 lr 0.000013	 wd 0.0500	time 0.2988 (0.3435)	loss 0.7797 (1.1609)	grad_norm 0.3641 (0.3652)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:16:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:09:06 lr 0.000013	 wd 0.0500	time 0.3078 (0.3414)	loss 1.5752 (1.1631)	grad_norm 0.3693 (0.3647)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:16:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:32 lr 0.000014	 wd 0.0500	time 0.3077 (0.3413)	loss 1.2932 (1.1637)	grad_norm 0.3271 (0.3648)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:56 lr 0.000014	 wd 0.0500	time 0.3374 (0.3398)	loss 0.9925 (1.1644)	grad_norm 0.3832 (0.3655)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:18:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:07:21 lr 0.000014	 wd 0.0500	time 0.3086 (0.3387)	loss 1.1508 (1.1641)	grad_norm 0.3520 (0.3662)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:18:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:46 lr 0.000014	 wd 0.0500	time 0.3078 (0.3384)	loss 1.3132 (1.1635)	grad_norm 0.3394 (0.3663)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:06:37 lr 0.000014	 wd 0.0500	time 0.3192 (0.3610)	loss 1.1256 (1.1644)	grad_norm 0.3417 (0.3659)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:20:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:59 lr 0.000014	 wd 0.0500	time 0.2994 (0.3591)	loss 1.4010 (1.1629)	grad_norm 0.3630 (0.3657)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:05:22 lr 0.000015	 wd 0.0500	time 0.3167 (0.3576)	loss 0.7865 (1.1614)	grad_norm 0.3520 (0.3656)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:21:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:45 lr 0.000015	 wd 0.0500	time 0.3195 (0.3563)	loss 0.9917 (1.1639)	grad_norm 0.3373 (0.3658)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:21:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:04:09 lr 0.000015	 wd 0.0500	time 0.2950 (0.3549)	loss 1.1569 (1.1650)	grad_norm 0.3624 (0.3661)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:22:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:33 lr 0.000015	 wd 0.0500	time 0.3484 (0.3538)	loss 1.2557 (1.1641)	grad_norm 0.3503 (0.3661)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:23:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:57 lr 0.000015	 wd 0.0500	time 0.3118 (0.3532)	loss 1.3143 (1.1646)	grad_norm 0.3467 (0.3661)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:23:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:21 lr 0.000015	 wd 0.0500	time 0.3444 (0.3524)	loss 0.9771 (1.1651)	grad_norm 0.3591 (0.3660)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:24:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:46 lr 0.000016	 wd 0.0500	time 0.3007 (0.3516)	loss 0.8081 (1.1642)	grad_norm 0.3484 (0.3657)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:24:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:10 lr 0.000016	 wd 0.0500	time 0.3061 (0.3506)	loss 0.9086 (1.1645)	grad_norm 0.3665 (0.3657)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:25:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:35 lr 0.000016	 wd 0.0500	time 0.3070 (0.3499)	loss 1.4121 (1.1661)	grad_norm 0.3828 (0.3655)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:25:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2922 (0.3486)	loss 1.2962 (1.1659)	grad_norm 0.3625 (0.3656)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:25:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:14:34
[2024-07-30 10:26:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.532 (12.532)	Loss 0.5225 (0.5225)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 10:26:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.938 Acc@5 97.816
[2024-07-30 10:26:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 10:26:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.95%
[2024-07-30 10:26:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:32:17 lr 0.000016	 wd 0.0500	time 10.8464 (10.8464)	loss 1.2662 (1.2662)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:27:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:17:29 lr 0.000016	 wd 0.0500	time 0.3293 (0.4368)	loss 0.9539 (1.1804)	grad_norm 0.3583 (0.3586)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:27:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:14:34 lr 0.000016	 wd 0.0500	time 0.2974 (0.3800)	loss 0.8682 (1.1684)	grad_norm 0.3585 (0.3637)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:28:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:13:21 lr 0.000016	 wd 0.0500	time 0.3375 (0.3638)	loss 0.7959 (1.1651)	grad_norm 0.3644 (0.3633)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 10:28:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:12:24 lr 0.000017	 wd 0.0500	time 0.2988 (0.3542)	loss 1.2946 (1.1602)	grad_norm 0.4922 (0.3641)	loss_scale 8192.0000 (4422.8628)	mem 17013MB
[2024-07-30 10:29:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:11:40 lr 0.000017	 wd 0.0500	time 0.3295 (0.3500)	loss 1.2530 (1.1608)	grad_norm 0.3466 (0.3698)	loss_scale 8192.0000 (5175.1856)	mem 17013MB
[2024-07-30 10:29:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:59 lr 0.000017	 wd 0.0500	time 0.3374 (0.3466)	loss 1.1947 (1.1656)	grad_norm 0.3667 (0.3684)	loss_scale 8192.0000 (5677.1514)	mem 17013MB
[2024-07-30 10:30:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:10:20 lr 0.000017	 wd 0.0500	time 0.3206 (0.3441)	loss 0.8383 (1.1660)	grad_norm 0.3556 (0.3674)	loss_scale 8192.0000 (6035.9030)	mem 17013MB
[2024-07-30 10:30:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:42 lr 0.000017	 wd 0.0500	time 0.3349 (0.3424)	loss 0.7804 (1.1685)	grad_norm 0.3801 (nan)	loss_scale 2048.0000 (5911.3308)	mem 17013MB
[2024-07-30 10:31:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:09:06 lr 0.000017	 wd 0.0500	time 0.3285 (0.3414)	loss 0.8313 (1.1696)	grad_norm 0.3529 (nan)	loss_scale 2048.0000 (5482.5483)	mem 17013MB
[2024-07-30 10:31:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:08:31 lr 0.000018	 wd 0.0500	time 0.3502 (0.3403)	loss 1.5692 (1.1696)	grad_norm 0.3380 (nan)	loss_scale 2048.0000 (5139.4366)	mem 17013MB
[2024-07-30 10:32:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:55 lr 0.000018	 wd 0.0500	time 0.2942 (0.3395)	loss 1.4445 (1.1706)	grad_norm 0.3623 (nan)	loss_scale 2048.0000 (4858.6521)	mem 17013MB
[2024-07-30 10:33:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:20 lr 0.000018	 wd 0.0500	time 0.3735 (0.3386)	loss 1.2874 (1.1695)	grad_norm 0.3646 (nan)	loss_scale 2048.0000 (4624.6261)	mem 17013MB
[2024-07-30 10:33:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:46 lr 0.000018	 wd 0.0500	time 0.3076 (0.3381)	loss 1.0294 (1.1702)	grad_norm 0.3716 (nan)	loss_scale 2048.0000 (4426.5765)	mem 17013MB
[2024-07-30 10:34:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:06:11 lr 0.000018	 wd 0.0500	time 0.3108 (0.3375)	loss 1.4132 (1.1702)	grad_norm 0.3639 (nan)	loss_scale 2048.0000 (4256.7994)	mem 17013MB
[2024-07-30 10:34:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:37 lr 0.000018	 wd 0.0500	time 0.2977 (0.3369)	loss 1.2113 (1.1706)	grad_norm 0.3500 (nan)	loss_scale 2048.0000 (4109.6442)	mem 17013MB
[2024-07-30 10:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:05:03 lr 0.000019	 wd 0.0500	time 0.3371 (0.3365)	loss 1.4584 (1.1692)	grad_norm 0.3625 (nan)	loss_scale 2048.0000 (3980.8720)	mem 17013MB
[2024-07-30 10:35:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:29 lr 0.000019	 wd 0.0500	time 0.3434 (0.3364)	loss 1.1780 (1.1698)	grad_norm 0.3600 (nan)	loss_scale 2048.0000 (3867.2404)	mem 17013MB
[2024-07-30 10:36:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:56 lr 0.000019	 wd 0.0500	time 0.2948 (0.3365)	loss 1.5244 (1.1718)	grad_norm 0.3631 (nan)	loss_scale 2048.0000 (3766.2277)	mem 17013MB
[2024-07-30 10:36:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:22 lr 0.000019	 wd 0.0500	time 0.3166 (0.3364)	loss 1.3939 (1.1722)	grad_norm 0.3579 (nan)	loss_scale 2048.0000 (3675.8422)	mem 17013MB
[2024-07-30 10:37:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:48 lr 0.000019	 wd 0.0500	time 0.2975 (0.3361)	loss 0.8682 (1.1700)	grad_norm 0.3708 (nan)	loss_scale 2048.0000 (3594.4908)	mem 17013MB
[2024-07-30 10:38:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:15 lr 0.000019	 wd 0.0500	time 0.3178 (0.3360)	loss 1.0032 (1.1688)	grad_norm 0.3453 (nan)	loss_scale 2048.0000 (3520.8834)	mem 17013MB
[2024-07-30 10:38:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:41 lr 0.000020	 wd 0.0500	time 0.3251 (0.3359)	loss 0.8766 (1.1684)	grad_norm 0.3559 (nan)	loss_scale 2048.0000 (3453.9646)	mem 17013MB
[2024-07-30 10:39:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:07 lr 0.000020	 wd 0.0500	time 0.3013 (0.3358)	loss 0.8009 (1.1677)	grad_norm 0.3585 (nan)	loss_scale 2048.0000 (3392.8622)	mem 17013MB
[2024-07-30 10:39:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3155 (0.3358)	loss 0.7836 (1.1676)	grad_norm 0.3218 (nan)	loss_scale 2048.0000 (3336.8496)	mem 17013MB
[2024-07-30 10:40:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2912 (0.3351)	loss 1.2374 (1.1682)	grad_norm 0.3704 (nan)	loss_scale 2048.0000 (3285.3163)	mem 17013MB
[2024-07-30 10:40:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:14:00
[2024-07-30 10:40:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.946 (11.946)	Loss 0.5103 (0.5103)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 10:40:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.992 Acc@5 97.846
[2024-07-30 10:40:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 10:40:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.99%
[2024-07-30 10:40:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 10:40:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 10:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:25:27 lr 0.000020	 wd 0.0500	time 10.6823 (10.6823)	loss 1.3890 (1.3890)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:41:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:17:23 lr 0.000020	 wd 0.0500	time 0.2876 (0.4345)	loss 0.9887 (1.1971)	grad_norm 0.3720 (0.3800)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:42:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:14:33 lr 0.000020	 wd 0.0500	time 0.3079 (0.3793)	loss 1.3549 (1.1681)	grad_norm 0.3618 (0.3746)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:42:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:13:19 lr 0.000020	 wd 0.0500	time 0.3046 (0.3630)	loss 1.0200 (1.1574)	grad_norm 0.3505 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:43:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:12:24 lr 0.000020	 wd 0.0500	time 0.3559 (0.3543)	loss 1.2635 (1.1664)	grad_norm 0.3763 (0.3720)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:43:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:40 lr 0.000020	 wd 0.0500	time 0.2951 (0.3501)	loss 1.0063 (1.1651)	grad_norm 0.3636 (0.3738)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:44:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:58 lr 0.000020	 wd 0.0500	time 0.3398 (0.3463)	loss 1.0588 (1.1665)	grad_norm 0.3663 (0.3729)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:44:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:10:21 lr 0.000020	 wd 0.0500	time 0.3134 (0.3452)	loss 0.9172 (1.1650)	grad_norm 0.3420 (0.3728)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:43 lr 0.000020	 wd 0.0500	time 0.3131 (0.3430)	loss 1.5214 (1.1640)	grad_norm 0.3757 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:45:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:09:07 lr 0.000020	 wd 0.0500	time 0.3427 (0.3419)	loss 1.0707 (1.1628)	grad_norm 0.3660 (0.3703)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:46:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:31 lr 0.000020	 wd 0.0500	time 0.3144 (0.3407)	loss 1.2465 (1.1610)	grad_norm 0.3412 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 10:47:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:58 lr 0.000020	 wd 0.0500	time 0.3530 (0.3416)	loss 1.1715 (1.1621)	grad_norm 0.3792 (nan)	loss_scale 1024.0000 (1988.4759)	mem 17013MB
[2024-07-30 10:47:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:07:23 lr 0.000020	 wd 0.0500	time 0.2987 (0.3408)	loss 1.4204 (1.1606)	grad_norm 0.3529 (nan)	loss_scale 1024.0000 (1908.1699)	mem 17013MB
[2024-07-30 10:48:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:48 lr 0.000020	 wd 0.0500	time 0.3203 (0.3401)	loss 0.9998 (1.1613)	grad_norm 0.3532 (nan)	loss_scale 1024.0000 (1840.2091)	mem 17013MB
[2024-07-30 10:48:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:06:14 lr 0.000020	 wd 0.0500	time 0.3503 (0.3398)	loss 1.4193 (1.1649)	grad_norm 0.3773 (nan)	loss_scale 1024.0000 (1781.9500)	mem 17013MB
[2024-07-30 10:49:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:40 lr 0.000020	 wd 0.0500	time 0.3241 (0.3393)	loss 0.8442 (1.1660)	grad_norm 0.3723 (nan)	loss_scale 1024.0000 (1731.4537)	mem 17013MB
[2024-07-30 10:49:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:05:06 lr 0.000020	 wd 0.0500	time 0.3647 (0.3393)	loss 1.4131 (1.1686)	grad_norm 0.3643 (nan)	loss_scale 1024.0000 (1687.2655)	mem 17013MB
[2024-07-30 10:50:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:31 lr 0.000020	 wd 0.0500	time 0.3181 (0.3391)	loss 0.9569 (1.1688)	grad_norm 0.3642 (nan)	loss_scale 1024.0000 (1648.2728)	mem 17013MB
[2024-07-30 10:50:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:57 lr 0.000020	 wd 0.0500	time 0.3293 (0.3386)	loss 0.7723 (1.1667)	grad_norm 0.3638 (nan)	loss_scale 1024.0000 (1613.6102)	mem 17013MB
[2024-07-30 10:51:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:23 lr 0.000020	 wd 0.0500	time 0.3086 (0.3385)	loss 1.1294 (1.1662)	grad_norm 0.3609 (nan)	loss_scale 1024.0000 (1582.5944)	mem 17013MB
[2024-07-30 10:52:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.3425 (0.3385)	loss 1.5621 (1.1677)	grad_norm 0.3557 (nan)	loss_scale 1024.0000 (1554.6787)	mem 17013MB
[2024-07-30 10:52:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:15 lr 0.000020	 wd 0.0500	time 0.3226 (0.3381)	loss 1.4777 (1.1672)	grad_norm 0.3767 (nan)	loss_scale 1024.0000 (1529.4203)	mem 17013MB
[2024-07-30 10:53:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:41 lr 0.000020	 wd 0.0500	time 0.3018 (0.3377)	loss 1.2097 (1.1684)	grad_norm 0.3544 (nan)	loss_scale 1024.0000 (1506.4571)	mem 17013MB
[2024-07-30 10:53:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3007 (0.3386)	loss 1.1231 (1.1669)	grad_norm 0.3648 (nan)	loss_scale 1024.0000 (1485.4898)	mem 17013MB
[2024-07-30 10:54:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3265 (0.3386)	loss 0.7865 (1.1664)	grad_norm 0.3778 (nan)	loss_scale 1024.0000 (1466.2691)	mem 17013MB
[2024-07-30 10:54:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.3166 (0.3378)	loss 1.2953 (1.1676)	grad_norm 0.3542 (nan)	loss_scale 1024.0000 (1448.5854)	mem 17013MB
[2024-07-30 10:54:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:14:07
[2024-07-30 10:55:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.340 (11.340)	Loss 0.5034 (0.5034)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 10:55:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.968 Acc@5 97.838
[2024-07-30 10:55:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 10:55:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.99%
[2024-07-30 10:55:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:22:30 lr 0.000020	 wd 0.0500	time 10.6118 (10.6118)	loss 1.2264 (1.2264)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:56:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:17:24 lr 0.000020	 wd 0.0500	time 0.3254 (0.4350)	loss 0.8944 (1.1897)	grad_norm 0.3677 (0.3588)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:56:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:14:35 lr 0.000020	 wd 0.0500	time 0.2992 (0.3804)	loss 0.8679 (1.1792)	grad_norm 0.3424 (0.3631)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:13:16 lr 0.000020	 wd 0.0500	time 0.3097 (0.3615)	loss 0.9576 (1.1709)	grad_norm 0.3660 (0.3636)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:57:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:25 lr 0.000020	 wd 0.0500	time 0.3541 (0.3545)	loss 0.7895 (1.1731)	grad_norm 0.3610 (0.3641)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:58:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:39 lr 0.000020	 wd 0.0500	time 0.2963 (0.3495)	loss 1.2082 (1.1734)	grad_norm 0.3545 (0.3639)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:58:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:11:01 lr 0.000020	 wd 0.0500	time 0.3103 (0.3479)	loss 0.8745 (1.1649)	grad_norm 0.3676 (0.3658)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:59:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:10:24 lr 0.000020	 wd 0.0500	time 0.2911 (0.3466)	loss 1.4730 (1.1675)	grad_norm 0.3296 (0.3698)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 10:59:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:45 lr 0.000020	 wd 0.0500	time 0.2975 (0.3441)	loss 0.7052 (1.1717)	grad_norm 0.3471 (0.3691)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:00:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:09:10 lr 0.000020	 wd 0.0500	time 0.3552 (0.3435)	loss 1.4921 (1.1681)	grad_norm 0.3397 (0.3720)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:01:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:33 lr 0.000020	 wd 0.0500	time 0.3292 (0.3421)	loss 1.0406 (1.1678)	grad_norm 0.3452 (0.3724)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:01:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:58 lr 0.000020	 wd 0.0500	time 0.3199 (0.3412)	loss 0.8595 (1.1645)	grad_norm 0.3626 (0.3721)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:02:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:22 lr 0.000020	 wd 0.0500	time 0.3098 (0.3401)	loss 1.2398 (1.1615)	grad_norm 0.3707 (0.3716)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:02:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:48 lr 0.000020	 wd 0.0500	time 0.3307 (0.3401)	loss 1.0978 (1.1603)	grad_norm 0.3492 (0.3713)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:03:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:06:14 lr 0.000020	 wd 0.0500	time 0.3156 (0.3402)	loss 0.9521 (1.1605)	grad_norm 0.3575 (0.3708)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:03:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:40 lr 0.000020	 wd 0.0500	time 0.3118 (0.3398)	loss 0.7321 (1.1615)	grad_norm 0.3777 (0.3708)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:04:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:05:06 lr 0.000020	 wd 0.0500	time 0.2973 (0.3393)	loss 1.4519 (1.1590)	grad_norm 0.3400 (0.3702)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:04:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:31 lr 0.000020	 wd 0.0500	time 0.3347 (0.3388)	loss 1.3754 (1.1583)	grad_norm 0.3510 (0.3702)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:05:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:57 lr 0.000020	 wd 0.0500	time 0.3393 (0.3386)	loss 1.2027 (1.1588)	grad_norm 0.3500 (0.3706)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:06:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:23 lr 0.000020	 wd 0.0500	time 0.3042 (0.3384)	loss 1.3492 (1.1600)	grad_norm 0.3553 (0.3702)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:06:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.3801 (0.3386)	loss 0.9750 (1.1608)	grad_norm 0.3762 (0.3708)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:07:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:16 lr 0.000020	 wd 0.0500	time 0.3558 (0.3389)	loss 1.0049 (1.1609)	grad_norm 0.3630 (0.3710)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:07:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:42 lr 0.000020	 wd 0.0500	time 0.3344 (0.3392)	loss 0.8113 (1.1603)	grad_norm 0.3889 (0.3721)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:08:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3316 (0.3389)	loss 1.0295 (1.1628)	grad_norm 0.3752 (0.3730)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:08:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3000 (0.3386)	loss 1.3812 (1.1620)	grad_norm 0.3819 (0.3733)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:09:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2882 (0.3376)	loss 1.3418 (1.1617)	grad_norm 0.3483 (0.3733)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:09:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:14:07
[2024-07-30 11:09:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.065 (12.065)	Loss 0.5166 (0.5166)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 11:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.978 Acc@5 97.810
[2024-07-30 11:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 11:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.99%
[2024-07-30 11:10:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 6:46:44 lr 0.000020	 wd 0.0500	time 9.7541 (9.7541)	loss 0.9293 (0.9293)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:17:29 lr 0.000020	 wd 0.0500	time 0.3024 (0.4369)	loss 1.2024 (1.1477)	grad_norm 0.3572 (0.3874)	loss_scale 2048.0000 (1713.4257)	mem 17013MB
[2024-07-30 11:11:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:38 lr 0.000020	 wd 0.0500	time 0.2982 (0.3816)	loss 1.1126 (1.1682)	grad_norm 0.3647 (0.3780)	loss_scale 2048.0000 (1879.8806)	mem 17013MB
[2024-07-30 11:11:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:19 lr 0.000020	 wd 0.0500	time 0.3412 (0.3629)	loss 1.2898 (1.1631)	grad_norm 0.3556 (0.3758)	loss_scale 2048.0000 (1935.7342)	mem 17013MB
[2024-07-30 11:12:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:23 lr 0.000020	 wd 0.0500	time 0.2988 (0.3535)	loss 0.8261 (1.1683)	grad_norm 0.4840 (0.3742)	loss_scale 2048.0000 (1963.7307)	mem 17013MB
[2024-07-30 11:12:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:36 lr 0.000020	 wd 0.0500	time 0.3615 (0.3480)	loss 1.3371 (1.1650)	grad_norm 0.3480 (0.3726)	loss_scale 2048.0000 (1980.5509)	mem 17013MB
[2024-07-30 11:13:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:56 lr 0.000020	 wd 0.0500	time 0.3388 (0.3450)	loss 1.2947 (1.1693)	grad_norm 0.3936 (nan)	loss_scale 1024.0000 (1923.6206)	mem 17013MB
[2024-07-30 11:13:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:10:18 lr 0.000020	 wd 0.0500	time 0.3341 (0.3434)	loss 0.9704 (1.1639)	grad_norm 0.3493 (nan)	loss_scale 1024.0000 (1795.2867)	mem 17013MB
[2024-07-30 11:14:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:41 lr 0.000020	 wd 0.0500	time 0.3079 (0.3415)	loss 0.8928 (1.1613)	grad_norm 0.3639 (nan)	loss_scale 1024.0000 (1698.9963)	mem 17013MB
[2024-07-30 11:15:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:09:04 lr 0.000020	 wd 0.0500	time 0.3361 (0.3400)	loss 1.4293 (1.1631)	grad_norm 0.3506 (nan)	loss_scale 1024.0000 (1624.0799)	mem 17013MB
[2024-07-30 11:15:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:29 lr 0.000020	 wd 0.0500	time 0.3172 (0.3391)	loss 0.8055 (1.1630)	grad_norm 0.3722 (nan)	loss_scale 1024.0000 (1564.1319)	mem 17013MB
[2024-07-30 11:16:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:54 lr 0.000020	 wd 0.0500	time 0.3035 (0.3386)	loss 1.3600 (1.1621)	grad_norm 0.3543 (nan)	loss_scale 1024.0000 (1515.0736)	mem 17013MB
[2024-07-30 11:16:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:19 lr 0.000020	 wd 0.0500	time 0.3073 (0.3377)	loss 1.3949 (1.1589)	grad_norm 0.3624 (nan)	loss_scale 1024.0000 (1474.1848)	mem 17013MB
[2024-07-30 11:17:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:45 lr 0.000020	 wd 0.0500	time 0.3257 (0.3377)	loss 1.3671 (1.1599)	grad_norm 0.3468 (nan)	loss_scale 1024.0000 (1439.5819)	mem 17013MB
[2024-07-30 11:17:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:06:13 lr 0.000019	 wd 0.0500	time 0.2982 (0.3385)	loss 1.2712 (1.1597)	grad_norm 0.3563 (nan)	loss_scale 1024.0000 (1409.9186)	mem 17013MB
[2024-07-30 11:18:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:39 lr 0.000019	 wd 0.0500	time 0.3538 (0.3391)	loss 0.8318 (1.1602)	grad_norm 0.3594 (nan)	loss_scale 1024.0000 (1384.2079)	mem 17013MB
[2024-07-30 11:19:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:05:06 lr 0.000019	 wd 0.0500	time 0.3357 (0.3403)	loss 1.1115 (1.1608)	grad_norm 0.3711 (nan)	loss_scale 1024.0000 (1361.7089)	mem 17013MB
[2024-07-30 11:19:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:33 lr 0.000019	 wd 0.0500	time 0.3131 (0.3408)	loss 1.3369 (1.1610)	grad_norm 0.3419 (nan)	loss_scale 1024.0000 (1341.8554)	mem 17013MB
[2024-07-30 11:20:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:59 lr 0.000019	 wd 0.0500	time 0.3642 (0.3417)	loss 0.7567 (1.1634)	grad_norm 0.3539 (nan)	loss_scale 1024.0000 (1324.2066)	mem 17013MB
[2024-07-30 11:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:25 lr 0.000019	 wd 0.0500	time 0.3425 (0.3413)	loss 1.1015 (1.1637)	grad_norm 0.3582 (nan)	loss_scale 1024.0000 (1308.4145)	mem 17013MB
[2024-07-30 11:21:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:51 lr 0.000019	 wd 0.0500	time 0.3336 (0.3411)	loss 1.1389 (1.1643)	grad_norm 0.3611 (nan)	loss_scale 1024.0000 (1294.2009)	mem 17013MB
[2024-07-30 11:21:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:16 lr 0.000019	 wd 0.0500	time 0.3155 (0.3405)	loss 1.4515 (1.1669)	grad_norm 0.3731 (nan)	loss_scale 1024.0000 (1281.3403)	mem 17013MB
[2024-07-30 11:22:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:42 lr 0.000019	 wd 0.0500	time 0.3003 (0.3404)	loss 1.2262 (1.1651)	grad_norm 0.3585 (nan)	loss_scale 1024.0000 (1269.6483)	mem 17013MB
[2024-07-30 11:23:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:08 lr 0.000019	 wd 0.0500	time 0.2985 (0.3400)	loss 1.3775 (1.1642)	grad_norm 0.3721 (nan)	loss_scale 1024.0000 (1258.9726)	mem 17013MB
[2024-07-30 11:23:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3263 (0.3398)	loss 1.2514 (1.1646)	grad_norm 0.3518 (nan)	loss_scale 1024.0000 (1249.1862)	mem 17013MB
[2024-07-30 11:24:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2919 (0.3388)	loss 1.5737 (1.1647)	grad_norm 0.3376 (nan)	loss_scale 1024.0000 (1240.1823)	mem 17013MB
[2024-07-30 11:24:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:14:10
[2024-07-30 11:24:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.922 (11.922)	Loss 0.4963 (0.4963)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 11:24:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.966 Acc@5 97.866
[2024-07-30 11:24:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 11:24:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 85.99%
[2024-07-30 11:24:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:33:24 lr 0.000019	 wd 0.0500	time 10.8730 (10.8730)	loss 1.2444 (1.2444)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:25:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:17:18 lr 0.000019	 wd 0.0500	time 0.3111 (0.4324)	loss 0.9817 (1.2067)	grad_norm 0.3430 (0.3878)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:25:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:14:27 lr 0.000019	 wd 0.0500	time 0.2989 (0.3768)	loss 0.8175 (1.1918)	grad_norm 0.3549 (0.3812)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:26:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:13:16 lr 0.000019	 wd 0.0500	time 0.3276 (0.3619)	loss 1.3893 (1.1832)	grad_norm 0.3730 (0.3759)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:26:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:29 lr 0.000019	 wd 0.0500	time 0.3688 (0.3564)	loss 0.8317 (1.1821)	grad_norm 0.3627 (0.3730)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:27:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:11:43 lr 0.000019	 wd 0.0500	time 0.3420 (0.3512)	loss 0.8058 (1.1712)	grad_norm 0.3675 (0.3748)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:28:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:10:59 lr 0.000019	 wd 0.0500	time 0.2864 (0.3468)	loss 0.9147 (1.1682)	grad_norm 0.3799 (0.3735)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:28:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:23 lr 0.000019	 wd 0.0500	time 0.3444 (0.3458)	loss 1.3407 (1.1688)	grad_norm 0.3805 (0.3724)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:29:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:44 lr 0.000019	 wd 0.0500	time 0.3042 (0.3433)	loss 1.4657 (1.1705)	grad_norm 0.6040 (0.3751)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:29:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:09:06 lr 0.000019	 wd 0.0500	time 0.3001 (0.3411)	loss 0.9472 (1.1656)	grad_norm 0.3575 (0.3739)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:30:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:32 lr 0.000019	 wd 0.0500	time 0.3089 (0.3412)	loss 1.3373 (1.1689)	grad_norm 0.3637 (0.3764)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:30:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:56 lr 0.000019	 wd 0.0500	time 0.3188 (0.3402)	loss 1.1971 (1.1671)	grad_norm 0.3496 (0.3756)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:31:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:21 lr 0.000019	 wd 0.0500	time 0.3171 (0.3388)	loss 1.1234 (1.1666)	grad_norm 0.3759 (0.3750)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:31:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:46 lr 0.000019	 wd 0.0500	time 0.3413 (0.3384)	loss 0.9038 (1.1659)	grad_norm 0.3700 (0.3746)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:32:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:06:12 lr 0.000019	 wd 0.0500	time 0.3077 (0.3376)	loss 1.2646 (1.1668)	grad_norm 0.3680 (0.3743)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:33:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:37 lr 0.000019	 wd 0.0500	time 0.3480 (0.3370)	loss 1.1560 (1.1670)	grad_norm 0.3708 (0.3740)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:33:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:05:03 lr 0.000019	 wd 0.0500	time 0.3270 (0.3366)	loss 1.3719 (1.1696)	grad_norm 0.3693 (0.3739)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:34:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:30 lr 0.000019	 wd 0.0500	time 0.3051 (0.3368)	loss 0.7642 (1.1721)	grad_norm 0.3448 (0.3734)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:34:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:56 lr 0.000019	 wd 0.0500	time 0.3196 (0.3365)	loss 0.7897 (1.1707)	grad_norm 0.3677 (0.3730)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:35:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:22 lr 0.000019	 wd 0.0500	time 0.3272 (0.3361)	loss 1.1708 (1.1724)	grad_norm 0.4468 (0.3727)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:35:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:48 lr 0.000019	 wd 0.0500	time 0.3064 (0.3359)	loss 0.8163 (1.1712)	grad_norm 0.3598 (0.3729)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:36:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:15 lr 0.000019	 wd 0.0500	time 0.3123 (0.3361)	loss 0.8740 (1.1717)	grad_norm 0.3762 (0.3726)	loss_scale 2048.0000 (1044.4703)	mem 17013MB
[2024-07-30 11:36:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:41 lr 0.000019	 wd 0.0500	time 0.2966 (0.3361)	loss 1.1102 (1.1700)	grad_norm 0.3805 (0.3723)	loss_scale 2048.0000 (1090.0645)	mem 17013MB
[2024-07-30 11:37:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:07 lr 0.000019	 wd 0.0500	time 0.3046 (0.3359)	loss 1.3357 (1.1687)	grad_norm 0.3469 (0.3725)	loss_scale 2048.0000 (1131.6958)	mem 17013MB
[2024-07-30 11:38:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3094 (0.3354)	loss 1.1745 (1.1685)	grad_norm 0.7333 (0.3727)	loss_scale 2048.0000 (1169.8592)	mem 17013MB
[2024-07-30 11:38:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.3108 (0.3346)	loss 0.8992 (1.1684)	grad_norm 0.3463 (0.3723)	loss_scale 2048.0000 (1204.9708)	mem 17013MB
[2024-07-30 11:38:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:13:59
[2024-07-30 11:38:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.379 (12.379)	Loss 0.4973 (0.4973)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 11:39:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.034 Acc@5 97.852
[2024-07-30 11:39:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 11:39:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.03%
[2024-07-30 11:39:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 11:39:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 11:39:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:26:08 lr 0.000019	 wd 0.0500	time 10.6990 (10.6990)	loss 1.3234 (1.3234)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:39:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:17:21 lr 0.000019	 wd 0.0500	time 0.2958 (0.4338)	loss 1.2556 (1.1456)	grad_norm 0.3634 (0.3690)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:40:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:44 lr 0.000019	 wd 0.0500	time 0.3237 (0.3844)	loss 0.8921 (1.1692)	grad_norm 0.3582 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:40:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:13:18 lr 0.000019	 wd 0.0500	time 0.3042 (0.3626)	loss 1.4999 (1.1779)	grad_norm 0.4361 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:41:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:12:21 lr 0.000019	 wd 0.0500	time 0.3112 (0.3527)	loss 0.8404 (1.1612)	grad_norm 0.3631 (0.3774)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:42:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:40 lr 0.000019	 wd 0.0500	time 0.3330 (0.3500)	loss 0.7497 (1.1563)	grad_norm 0.3749 (0.3809)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:42:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:59 lr 0.000019	 wd 0.0500	time 0.3210 (0.3468)	loss 0.8354 (1.1607)	grad_norm 0.3637 (0.3781)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:43:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:10:18 lr 0.000019	 wd 0.0500	time 0.2953 (0.3434)	loss 1.3703 (1.1642)	grad_norm 0.3538 (0.3803)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:40 lr 0.000019	 wd 0.0500	time 0.3008 (0.3413)	loss 1.3720 (1.1654)	grad_norm 0.3436 (0.3790)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 11:44:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:09:03 lr 0.000019	 wd 0.0500	time 0.3065 (0.3396)	loss 1.3466 (1.1651)	grad_norm 0.3467 (nan)	loss_scale 1024.0000 (2000.2664)	mem 17013MB
[2024-07-30 11:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:08:28 lr 0.000019	 wd 0.0500	time 0.3182 (0.3386)	loss 1.1761 (1.1658)	grad_norm 0.3518 (nan)	loss_scale 1024.0000 (1902.7373)	mem 17013MB
[2024-07-30 11:45:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:54 lr 0.000018	 wd 0.0500	time 0.3637 (0.3387)	loss 1.3262 (1.1618)	grad_norm 0.3613 (nan)	loss_scale 1024.0000 (1822.9246)	mem 17013MB
[2024-07-30 11:45:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:20 lr 0.000018	 wd 0.0500	time 0.3050 (0.3380)	loss 0.8396 (1.1607)	grad_norm 0.3605 (nan)	loss_scale 1024.0000 (1756.4030)	mem 17013MB
[2024-07-30 11:46:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:45 lr 0.000018	 wd 0.0500	time 0.3350 (0.3374)	loss 1.2715 (1.1616)	grad_norm 0.3683 (nan)	loss_scale 1024.0000 (1700.1076)	mem 17013MB
[2024-07-30 11:46:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:06:11 lr 0.000018	 wd 0.0500	time 0.2966 (0.3368)	loss 1.0695 (1.1608)	grad_norm 0.3550 (nan)	loss_scale 1024.0000 (1651.8487)	mem 17013MB
[2024-07-30 11:47:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:37 lr 0.000018	 wd 0.0500	time 0.2973 (0.3365)	loss 0.8538 (1.1640)	grad_norm 0.3644 (nan)	loss_scale 1024.0000 (1610.0200)	mem 17013MB
[2024-07-30 11:48:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:05:03 lr 0.000018	 wd 0.0500	time 0.3274 (0.3361)	loss 0.9102 (1.1623)	grad_norm 0.3840 (nan)	loss_scale 1024.0000 (1573.4166)	mem 17013MB
[2024-07-30 11:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:30 lr 0.000018	 wd 0.0500	time 0.3349 (0.3368)	loss 1.4298 (1.1628)	grad_norm 0.3653 (nan)	loss_scale 1024.0000 (1541.1170)	mem 17013MB
[2024-07-30 11:49:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:57 lr 0.000018	 wd 0.0500	time 0.3452 (0.3377)	loss 1.2462 (1.1616)	grad_norm 0.3631 (nan)	loss_scale 1024.0000 (1512.4042)	mem 17013MB
[2024-07-30 11:49:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:23 lr 0.000018	 wd 0.0500	time 0.3526 (0.3373)	loss 1.4012 (1.1610)	grad_norm 0.5968 (nan)	loss_scale 1024.0000 (1486.7123)	mem 17013MB
[2024-07-30 11:50:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:49 lr 0.000018	 wd 0.0500	time 0.3227 (0.3369)	loss 1.3901 (1.1616)	grad_norm 0.3376 (nan)	loss_scale 1024.0000 (1463.5882)	mem 17013MB
[2024-07-30 11:50:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:15 lr 0.000018	 wd 0.0500	time 0.3193 (0.3365)	loss 1.4054 (1.1631)	grad_norm 0.3721 (nan)	loss_scale 1024.0000 (1442.6654)	mem 17013MB
[2024-07-30 11:51:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:41 lr 0.000018	 wd 0.0500	time 0.3092 (0.3361)	loss 1.1493 (1.1642)	grad_norm 0.3680 (nan)	loss_scale 1024.0000 (1423.6438)	mem 17013MB
[2024-07-30 11:51:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:07 lr 0.000018	 wd 0.0500	time 0.3483 (0.3358)	loss 0.7879 (1.1642)	grad_norm 0.3606 (nan)	loss_scale 1024.0000 (1406.2755)	mem 17013MB
[2024-07-30 11:52:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:34 lr 0.000018	 wd 0.0500	time 0.3903 (0.3364)	loss 1.0797 (1.1645)	grad_norm 0.3868 (nan)	loss_scale 1024.0000 (1390.3540)	mem 17013MB
[2024-07-30 11:53:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2913 (0.3358)	loss 1.2576 (1.1640)	grad_norm 0.3722 (nan)	loss_scale 1024.0000 (1375.7057)	mem 17013MB
[2024-07-30 11:53:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 9 training takes 0:14:02
[2024-07-30 11:53:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.110 (12.110)	Loss 0.5083 (0.5083)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 11:53:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 85.992 Acc@5 97.868
[2024-07-30 11:53:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 11:53:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.03%
[2024-07-30 11:53:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:49:18 lr 0.000018	 wd 0.0500	time 12.6934 (12.6934)	loss 1.6220 (1.6220)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:54:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:17:53 lr 0.000018	 wd 0.0500	time 0.2981 (0.4468)	loss 0.8899 (1.1750)	grad_norm 0.3935 (0.4168)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:54:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:48 lr 0.000018	 wd 0.0500	time 0.3387 (0.3862)	loss 1.4287 (1.1701)	grad_norm 0.3446 (0.4043)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:55:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:13:25 lr 0.000018	 wd 0.0500	time 0.2966 (0.3657)	loss 1.3210 (1.1649)	grad_norm 0.3839 (0.3933)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:55:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:12:31 lr 0.000018	 wd 0.0500	time 0.3470 (0.3576)	loss 0.8869 (1.1682)	grad_norm 0.3660 (0.3878)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:56:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:48 lr 0.000018	 wd 0.0500	time 0.3147 (0.3539)	loss 0.7038 (1.1637)	grad_norm 0.3497 (0.3840)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:57:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:11:05 lr 0.000018	 wd 0.0500	time 0.3344 (0.3501)	loss 1.1567 (1.1717)	grad_norm 0.3533 (0.3812)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:57:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:10:24 lr 0.000018	 wd 0.0500	time 0.3013 (0.3468)	loss 1.3011 (1.1699)	grad_norm 0.3290 (0.3791)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:58:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:45 lr 0.000018	 wd 0.0500	time 0.3595 (0.3439)	loss 1.2726 (1.1725)	grad_norm 0.5012 (0.3826)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:58:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:09:07 lr 0.000018	 wd 0.0500	time 0.3080 (0.3418)	loss 1.4450 (1.1701)	grad_norm 0.3567 (0.3815)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:59:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:31 lr 0.000018	 wd 0.0500	time 0.3411 (0.3402)	loss 1.2759 (1.1712)	grad_norm 0.3574 (0.3811)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 11:59:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:56 lr 0.000018	 wd 0.0500	time 0.3099 (0.3401)	loss 0.9992 (1.1769)	grad_norm 0.3785 (0.3803)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:00:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:07:23 lr 0.000018	 wd 0.0500	time 0.3497 (0.3406)	loss 1.3948 (1.1745)	grad_norm 0.3733 (0.3803)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:00:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:48 lr 0.000018	 wd 0.0500	time 0.3323 (0.3401)	loss 1.3642 (1.1720)	grad_norm 0.3524 (0.3804)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:01:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:13 lr 0.000018	 wd 0.0500	time 0.3012 (0.3393)	loss 1.3470 (1.1699)	grad_norm 0.3571 (0.3796)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:02:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:39 lr 0.000018	 wd 0.0500	time 0.2987 (0.3386)	loss 1.1617 (1.1674)	grad_norm 0.3773 (0.3791)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:02:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:05:04 lr 0.000018	 wd 0.0500	time 0.3358 (0.3379)	loss 1.1014 (1.1661)	grad_norm 0.3471 (0.3790)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:03:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:31 lr 0.000018	 wd 0.0500	time 0.3678 (0.3381)	loss 1.2341 (1.1681)	grad_norm 0.3614 (0.3790)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:03:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:57 lr 0.000018	 wd 0.0500	time 0.3404 (0.3384)	loss 1.1355 (1.1671)	grad_norm 0.3922 (0.3783)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:04:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:23 lr 0.000018	 wd 0.0500	time 0.3216 (0.3382)	loss 1.0860 (1.1661)	grad_norm 0.3638 (0.3780)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:04:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:49 lr 0.000017	 wd 0.0500	time 0.3365 (0.3380)	loss 1.3319 (1.1696)	grad_norm 0.3685 (0.3776)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:05:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:15 lr 0.000017	 wd 0.0500	time 0.2970 (0.3376)	loss 1.2046 (1.1709)	grad_norm 0.3468 (0.3791)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:05:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:41 lr 0.000017	 wd 0.0500	time 0.3613 (0.3373)	loss 1.3196 (1.1720)	grad_norm 0.3550 (0.3792)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:06:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:08 lr 0.000017	 wd 0.0500	time 0.3088 (0.3370)	loss 1.2859 (1.1698)	grad_norm 0.3673 (0.3787)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 12:07:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:34 lr 0.000017	 wd 0.0500	time 0.3088 (0.3368)	loss 1.3598 (1.1688)	grad_norm 0.3554 (0.3787)	loss_scale 2048.0000 (1042.7655)	mem 17013MB
[2024-07-30 12:07:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2919 (0.3358)	loss 0.8993 (1.1684)	grad_norm 0.3584 (0.3784)	loss_scale 2048.0000 (1082.9588)	mem 17013MB
[2024-07-30 12:07:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 10 training takes 0:14:02
[2024-07-30 12:07:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_10.pth saving......
[2024-07-30 12:07:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_10.pth saved !!!
[2024-07-30 12:07:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.592 (11.592)	Loss 0.5010 (0.5010)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 17013MB
[2024-07-30 12:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.054 Acc@5 97.862
[2024-07-30 12:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 12:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.05%
[2024-07-30 12:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 12:08:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 12:08:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:28:43 lr 0.000017	 wd 0.0500	time 10.7609 (10.7609)	loss 0.9178 (0.9178)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:08:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:17:29 lr 0.000017	 wd 0.0500	time 0.2851 (0.4370)	loss 1.3875 (1.1528)	grad_norm 0.3777 (0.3827)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:09:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:43 lr 0.000017	 wd 0.0500	time 0.3025 (0.3840)	loss 1.3229 (1.1738)	grad_norm 0.3749 (0.3889)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:13:25 lr 0.000017	 wd 0.0500	time 0.3118 (0.3659)	loss 0.8592 (1.1643)	grad_norm 0.3509 (0.3806)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:10:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:12:30 lr 0.000017	 wd 0.0500	time 0.3046 (0.3570)	loss 1.3984 (1.1625)	grad_norm 0.3691 (0.3768)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:11:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:44 lr 0.000017	 wd 0.0500	time 0.2908 (0.3517)	loss 0.7256 (1.1618)	grad_norm 0.3505 (0.3758)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:11:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:11:01 lr 0.000017	 wd 0.0500	time 0.2981 (0.3480)	loss 1.4590 (1.1625)	grad_norm 0.3627 (0.3749)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:12:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:10:22 lr 0.000017	 wd 0.0500	time 0.3054 (0.3456)	loss 1.0110 (1.1580)	grad_norm 0.3904 (0.3776)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:12:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:45 lr 0.000017	 wd 0.0500	time 0.3021 (0.3437)	loss 1.4744 (1.1562)	grad_norm 0.3696 (0.3773)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:13:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:09:07 lr 0.000017	 wd 0.0500	time 0.2942 (0.3421)	loss 1.0051 (1.1590)	grad_norm 0.3774 (0.3762)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:13:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:08:32 lr 0.000017	 wd 0.0500	time 0.3026 (0.3409)	loss 1.1537 (1.1591)	grad_norm 0.3672 (0.3757)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:14:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:56 lr 0.000017	 wd 0.0500	time 0.2897 (0.3401)	loss 0.9097 (1.1576)	grad_norm 1.1613 (0.3768)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:14:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:21 lr 0.000017	 wd 0.0500	time 0.2978 (0.3390)	loss 0.9209 (1.1545)	grad_norm 0.3604 (0.3762)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:15:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:46 lr 0.000017	 wd 0.0500	time 0.3004 (0.3386)	loss 1.3997 (1.1550)	grad_norm 0.4121 (0.3783)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:16:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:06:12 lr 0.000017	 wd 0.0500	time 0.3258 (0.3383)	loss 0.8163 (1.1554)	grad_norm 0.3593 (0.3781)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:16:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:38 lr 0.000017	 wd 0.0500	time 0.3069 (0.3378)	loss 1.3924 (1.1562)	grad_norm 0.3741 (0.3795)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:17:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:05:04 lr 0.000017	 wd 0.0500	time 0.3354 (0.3374)	loss 1.2686 (1.1574)	grad_norm 0.3618 (0.3789)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:17:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:30 lr 0.000017	 wd 0.0500	time 0.2999 (0.3372)	loss 1.0809 (1.1565)	grad_norm 0.3575 (0.3784)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:18:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:56 lr 0.000017	 wd 0.0500	time 0.3550 (0.3369)	loss 0.8396 (1.1564)	grad_norm 0.3473 (0.3781)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:18:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:22 lr 0.000017	 wd 0.0500	time 0.3121 (0.3367)	loss 1.1365 (1.1556)	grad_norm 0.3704 (0.3786)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:19:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:48 lr 0.000017	 wd 0.0500	time 0.3162 (0.3365)	loss 1.0543 (1.1558)	grad_norm 0.3330 (0.3781)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:19:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:15 lr 0.000017	 wd 0.0500	time 0.3226 (0.3363)	loss 1.3549 (1.1550)	grad_norm 0.3829 (0.3777)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:20:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:41 lr 0.000017	 wd 0.0500	time 0.2944 (0.3361)	loss 0.9710 (1.1569)	grad_norm 0.3977 (0.3781)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:21:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:07 lr 0.000016	 wd 0.0500	time 0.3355 (0.3360)	loss 1.1595 (1.1550)	grad_norm 0.3699 (0.3776)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:21:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:34 lr 0.000016	 wd 0.0500	time 0.3217 (0.3357)	loss 1.3808 (1.1562)	grad_norm 0.3571 (0.3774)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:22:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2917 (0.3350)	loss 0.8528 (1.1545)	grad_norm 0.3445 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:22:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 11 training takes 0:14:00
[2024-07-30 12:22:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.267 (11.267)	Loss 0.5015 (0.5015)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 12:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.062 Acc@5 97.864
[2024-07-30 12:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 12:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.06%
[2024-07-30 12:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 12:22:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 12:22:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:15:37 lr 0.000016	 wd 0.0500	time 11.8857 (11.8857)	loss 0.9520 (0.9520)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:23:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:17:28 lr 0.000016	 wd 0.0500	time 0.3112 (0.4366)	loss 1.3035 (1.1583)	grad_norm 0.3565 (0.3680)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:23:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:14:40 lr 0.000016	 wd 0.0500	time 0.3709 (0.3827)	loss 1.2979 (1.1555)	grad_norm 0.3581 (0.3709)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:24:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:13:33 lr 0.000016	 wd 0.0500	time 0.2973 (0.3695)	loss 0.7478 (1.1674)	grad_norm 0.3831 (0.3717)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:25:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:36 lr 0.000016	 wd 0.0500	time 0.2930 (0.3598)	loss 1.2237 (1.1593)	grad_norm 0.3532 (0.3737)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:25:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:49 lr 0.000016	 wd 0.0500	time 0.3228 (0.3542)	loss 1.4304 (1.1593)	grad_norm 0.3599 (0.3731)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:26:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:11:06 lr 0.000016	 wd 0.0500	time 0.2973 (0.3503)	loss 1.4170 (1.1578)	grad_norm 0.3850 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:26:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:10:25 lr 0.000016	 wd 0.0500	time 0.3075 (0.3470)	loss 1.2099 (1.1560)	grad_norm 0.4243 (0.3756)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:27:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:46 lr 0.000016	 wd 0.0500	time 0.3436 (0.3445)	loss 1.3650 (1.1571)	grad_norm 0.3714 (0.3746)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:27:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:09:09 lr 0.000016	 wd 0.0500	time 0.3117 (0.3428)	loss 1.5290 (1.1546)	grad_norm 0.3778 (0.3750)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:28:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:32 lr 0.000016	 wd 0.0500	time 0.2964 (0.3414)	loss 1.3922 (1.1566)	grad_norm 0.3711 (0.3742)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:28:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:57 lr 0.000016	 wd 0.0500	time 0.3138 (0.3405)	loss 0.8065 (1.1581)	grad_norm 0.3632 (0.3752)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:29:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:22 lr 0.000016	 wd 0.0500	time 0.2919 (0.3396)	loss 1.4340 (1.1563)	grad_norm 0.3740 (0.3746)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:29:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:47 lr 0.000016	 wd 0.0500	time 0.3144 (0.3389)	loss 1.0489 (1.1571)	grad_norm 0.3521 (0.3757)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:30:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:06:12 lr 0.000016	 wd 0.0500	time 0.3426 (0.3384)	loss 1.3092 (1.1568)	grad_norm 0.3577 (0.3768)	loss_scale 4096.0000 (2118.1670)	mem 17013MB
[2024-07-30 12:31:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:38 lr 0.000016	 wd 0.0500	time 0.2924 (0.3379)	loss 0.8118 (1.1541)	grad_norm 0.3868 (0.3778)	loss_scale 4096.0000 (2249.9347)	mem 17013MB
[2024-07-30 12:31:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:05:05 lr 0.000016	 wd 0.0500	time 0.3399 (0.3383)	loss 1.3636 (1.1508)	grad_norm 0.3466 (0.3774)	loss_scale 4096.0000 (2365.2417)	mem 17013MB
[2024-07-30 12:32:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:30 lr 0.000016	 wd 0.0500	time 0.3123 (0.3378)	loss 1.0617 (1.1529)	grad_norm 0.3709 (0.3771)	loss_scale 4096.0000 (2466.9912)	mem 17013MB
[2024-07-30 12:32:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:57 lr 0.000016	 wd 0.0500	time 0.3248 (0.3377)	loss 0.8163 (1.1515)	grad_norm 0.3849 (0.3768)	loss_scale 4096.0000 (2557.4414)	mem 17013MB
[2024-07-30 12:33:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:23 lr 0.000016	 wd 0.0500	time 0.3732 (0.3377)	loss 1.3122 (1.1522)	grad_norm 0.3630 (0.3769)	loss_scale 4096.0000 (2638.3756)	mem 17013MB
[2024-07-30 12:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:49 lr 0.000016	 wd 0.0500	time 0.2979 (0.3376)	loss 1.0399 (1.1530)	grad_norm 0.3731 (0.3765)	loss_scale 4096.0000 (2711.2204)	mem 17013MB
[2024-07-30 12:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:15 lr 0.000016	 wd 0.0500	time 0.2997 (0.3373)	loss 0.8130 (1.1523)	grad_norm 0.3282 (0.3765)	loss_scale 4096.0000 (2777.1309)	mem 17013MB
[2024-07-30 12:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:41 lr 0.000016	 wd 0.0500	time 0.3133 (0.3373)	loss 0.8092 (1.1537)	grad_norm 0.3779 (0.3769)	loss_scale 4096.0000 (2837.0522)	mem 17013MB
[2024-07-30 12:35:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:08 lr 0.000015	 wd 0.0500	time 0.3229 (0.3375)	loss 1.1894 (1.1542)	grad_norm 0.3561 (0.3766)	loss_scale 4096.0000 (2891.7653)	mem 17013MB
[2024-07-30 12:36:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:34 lr 0.000015	 wd 0.0500	time 0.3157 (0.3374)	loss 1.3487 (1.1545)	grad_norm 0.3649 (0.3769)	loss_scale 4096.0000 (2941.9209)	mem 17013MB
[2024-07-30 12:36:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.3211 (0.3365)	loss 0.9367 (1.1538)	grad_norm 0.3615 (0.3774)	loss_scale 4096.0000 (2988.0656)	mem 17013MB
[2024-07-30 12:36:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 12 training takes 0:14:04
[2024-07-30 12:36:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.543 (12.543)	Loss 0.5137 (0.5137)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 12:37:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.034 Acc@5 97.862
[2024-07-30 12:37:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-30 12:37:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.06%
[2024-07-30 12:37:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:22:54 lr 0.000015	 wd 0.0500	time 12.0601 (12.0601)	loss 1.3248 (1.3248)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 12:37:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:17:52 lr 0.000015	 wd 0.0500	time 0.3382 (0.4467)	loss 1.3379 (1.2183)	grad_norm 0.3623 (0.3899)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 12:38:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:51 lr 0.000015	 wd 0.0500	time 0.3045 (0.3872)	loss 1.3813 (1.2058)	grad_norm 0.3515 (0.3794)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 12:39:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:13:26 lr 0.000015	 wd 0.0500	time 0.3118 (0.3661)	loss 1.3873 (1.2060)	grad_norm 0.3587 (0.3777)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 12:39:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:12:31 lr 0.000015	 wd 0.0500	time 0.3333 (0.3573)	loss 1.4512 (1.1868)	grad_norm 0.3764 (nan)	loss_scale 2048.0000 (4055.1421)	mem 17013MB
[2024-07-30 12:40:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:11:44 lr 0.000015	 wd 0.0500	time 0.3103 (0.3517)	loss 1.2811 (1.1800)	grad_norm 0.4053 (nan)	loss_scale 2048.0000 (3654.5150)	mem 17013MB
[2024-07-30 12:40:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:11:01 lr 0.000015	 wd 0.0500	time 0.3072 (0.3480)	loss 1.1939 (1.1739)	grad_norm 0.3568 (nan)	loss_scale 2048.0000 (3387.2080)	mem 17013MB
[2024-07-30 12:41:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:10:23 lr 0.000015	 wd 0.0500	time 0.2877 (0.3460)	loss 1.1777 (1.1649)	grad_norm 0.3793 (nan)	loss_scale 2048.0000 (3196.1655)	mem 17013MB
[2024-07-30 12:41:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:45 lr 0.000015	 wd 0.0500	time 0.2984 (0.3437)	loss 1.2322 (1.1643)	grad_norm 0.3608 (nan)	loss_scale 2048.0000 (3052.8240)	mem 17013MB
[2024-07-30 12:42:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:09:08 lr 0.000015	 wd 0.0500	time 0.3558 (0.3427)	loss 1.3286 (1.1649)	grad_norm 0.3794 (nan)	loss_scale 2048.0000 (2941.3008)	mem 17013MB
[2024-07-30 12:42:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:33 lr 0.000015	 wd 0.0500	time 0.3408 (0.3416)	loss 1.3749 (1.1637)	grad_norm 0.3501 (nan)	loss_scale 2048.0000 (2852.0599)	mem 17013MB
[2024-07-30 12:43:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:57 lr 0.000015	 wd 0.0500	time 0.3434 (0.3403)	loss 1.3948 (1.1596)	grad_norm 0.3710 (nan)	loss_scale 2048.0000 (2779.0300)	mem 17013MB
[2024-07-30 12:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:22 lr 0.000015	 wd 0.0500	time 0.3349 (0.3397)	loss 1.1864 (1.1636)	grad_norm 0.3670 (nan)	loss_scale 2048.0000 (2718.1615)	mem 17013MB
[2024-07-30 12:44:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:47 lr 0.000015	 wd 0.0500	time 0.3248 (0.3392)	loss 0.7287 (1.1619)	grad_norm 0.3688 (nan)	loss_scale 2048.0000 (2666.6503)	mem 17013MB
[2024-07-30 12:45:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:06:13 lr 0.000015	 wd 0.0500	time 0.3135 (0.3385)	loss 1.3397 (1.1606)	grad_norm 0.3664 (nan)	loss_scale 2048.0000 (2622.4925)	mem 17013MB
[2024-07-30 12:45:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:38 lr 0.000015	 wd 0.0500	time 0.3119 (0.3381)	loss 1.2066 (1.1611)	grad_norm 0.4978 (nan)	loss_scale 2048.0000 (2584.2185)	mem 17013MB
[2024-07-30 12:46:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:05:04 lr 0.000015	 wd 0.0500	time 0.3360 (0.3378)	loss 0.9126 (1.1600)	grad_norm 0.3638 (nan)	loss_scale 2048.0000 (2550.7258)	mem 17013MB
[2024-07-30 12:46:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:30 lr 0.000015	 wd 0.0500	time 0.3110 (0.3372)	loss 1.0314 (1.1613)	grad_norm 0.3724 (nan)	loss_scale 2048.0000 (2521.1711)	mem 17013MB
[2024-07-30 12:47:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:56 lr 0.000015	 wd 0.0500	time 0.3186 (0.3372)	loss 1.2235 (1.1617)	grad_norm 0.3595 (nan)	loss_scale 2048.0000 (2494.8984)	mem 17013MB
[2024-07-30 12:47:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:22 lr 0.000015	 wd 0.0500	time 0.3130 (0.3370)	loss 1.0744 (1.1618)	grad_norm 0.3738 (nan)	loss_scale 2048.0000 (2471.3898)	mem 17013MB
[2024-07-30 12:48:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:48 lr 0.000015	 wd 0.0500	time 0.3083 (0.3366)	loss 1.3832 (1.1616)	grad_norm 0.3704 (nan)	loss_scale 2048.0000 (2450.2309)	mem 17013MB
[2024-07-30 12:48:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:15 lr 0.000014	 wd 0.0500	time 0.3102 (0.3363)	loss 1.3568 (1.1616)	grad_norm 0.3637 (nan)	loss_scale 2048.0000 (2431.0861)	mem 17013MB
[2024-07-30 12:49:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:41 lr 0.000014	 wd 0.0500	time 0.3107 (0.3361)	loss 1.1966 (1.1603)	grad_norm 0.3634 (nan)	loss_scale 2048.0000 (2413.6811)	mem 17013MB
[2024-07-30 12:50:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:07 lr 0.000014	 wd 0.0500	time 0.3060 (0.3360)	loss 1.3349 (1.1592)	grad_norm 0.3737 (nan)	loss_scale 2048.0000 (2397.7888)	mem 17013MB
[2024-07-30 12:50:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:34 lr 0.000014	 wd 0.0500	time 0.3189 (0.3359)	loss 0.9444 (1.1619)	grad_norm 0.3434 (nan)	loss_scale 2048.0000 (2383.2203)	mem 17013MB
[2024-07-30 12:51:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.2921 (0.3351)	loss 0.7895 (1.1606)	grad_norm 0.3607 (nan)	loss_scale 2048.0000 (2369.8169)	mem 17013MB
[2024-07-30 12:51:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 13 training takes 0:14:00
[2024-07-30 12:51:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.127 (12.127)	Loss 0.4878 (0.4878)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 12:51:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.084 Acc@5 97.888
[2024-07-30 12:51:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 12:51:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-30 12:51:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 12:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 12:51:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:39:10 lr 0.000014	 wd 0.0500	time 11.0114 (11.0114)	loss 1.4195 (1.4195)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:52:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:17:14 lr 0.000014	 wd 0.0500	time 0.2952 (0.4307)	loss 1.1821 (1.1772)	grad_norm 0.3586 (0.3859)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:52:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:30 lr 0.000014	 wd 0.0500	time 0.2928 (0.3782)	loss 1.4450 (1.1753)	grad_norm 0.3505 (0.3785)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:53:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:13:14 lr 0.000014	 wd 0.0500	time 0.3506 (0.3608)	loss 0.9292 (1.1785)	grad_norm 0.3717 (0.3773)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:12:19 lr 0.000014	 wd 0.0500	time 0.3167 (0.3516)	loss 1.5920 (1.1779)	grad_norm 0.3971 (0.3798)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:54:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:33 lr 0.000014	 wd 0.0500	time 0.3010 (0.3464)	loss 0.9737 (1.1736)	grad_norm 0.3679 (0.3777)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:55:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:54 lr 0.000014	 wd 0.0500	time 0.3365 (0.3440)	loss 0.9657 (1.1727)	grad_norm 0.3846 (0.3779)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:55:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:10:16 lr 0.000014	 wd 0.0500	time 0.3087 (0.3421)	loss 1.1289 (1.1733)	grad_norm 0.3967 (0.3774)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:56:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:39 lr 0.000014	 wd 0.0500	time 0.3006 (0.3403)	loss 0.8023 (1.1675)	grad_norm 0.3476 (0.3772)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:09:03 lr 0.000014	 wd 0.0500	time 0.3615 (0.3394)	loss 0.8550 (1.1665)	grad_norm 0.3819 (0.3787)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:57:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:28 lr 0.000014	 wd 0.0500	time 0.2986 (0.3385)	loss 0.9994 (1.1638)	grad_norm 0.3725 (0.3816)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:57:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:53 lr 0.000014	 wd 0.0500	time 0.2960 (0.3380)	loss 1.3828 (1.1632)	grad_norm 0.3860 (0.3812)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:58:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:19 lr 0.000014	 wd 0.0500	time 0.3226 (0.3373)	loss 1.4685 (1.1628)	grad_norm 0.4065 (0.3808)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:58:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:46 lr 0.000014	 wd 0.0500	time 0.3122 (0.3378)	loss 1.2423 (1.1660)	grad_norm 0.3790 (0.3928)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 12:59:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:12 lr 0.000014	 wd 0.0500	time 0.3537 (0.3382)	loss 1.2341 (1.1656)	grad_norm 0.3709 (0.3947)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:00:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:38 lr 0.000014	 wd 0.0500	time 0.3321 (0.3381)	loss 1.3878 (1.1646)	grad_norm 0.3614 (0.3931)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:00:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:05:04 lr 0.000014	 wd 0.0500	time 0.3199 (0.3380)	loss 0.7700 (1.1635)	grad_norm 0.3638 (0.3918)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:01:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:31 lr 0.000014	 wd 0.0500	time 0.3122 (0.3380)	loss 0.9391 (1.1625)	grad_norm 0.3511 (0.3911)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:01:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:57 lr 0.000013	 wd 0.0500	time 0.2925 (0.3378)	loss 0.8951 (1.1631)	grad_norm 0.3833 (0.3904)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:02:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:23 lr 0.000013	 wd 0.0500	time 0.3012 (0.3377)	loss 1.1560 (1.1634)	grad_norm 0.3865 (0.3906)	loss_scale 4096.0000 (2058.7733)	mem 17013MB
[2024-07-30 13:02:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:49 lr 0.000013	 wd 0.0500	time 0.3122 (0.3375)	loss 1.2110 (1.1628)	grad_norm 0.3621 (0.3897)	loss_scale 4096.0000 (2160.5837)	mem 17013MB
[2024-07-30 13:03:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:15 lr 0.000013	 wd 0.0500	time 0.3039 (0.3373)	loss 1.3433 (1.1621)	grad_norm 0.3693 (0.3891)	loss_scale 4096.0000 (2252.7025)	mem 17013MB
[2024-07-30 13:04:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:41 lr 0.000013	 wd 0.0500	time 0.3647 (0.3371)	loss 1.3038 (1.1613)	grad_norm 0.3657 (0.3885)	loss_scale 4096.0000 (2336.4507)	mem 17013MB
[2024-07-30 13:04:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:08 lr 0.000013	 wd 0.0500	time 0.3210 (0.3370)	loss 0.8544 (1.1602)	grad_norm 0.3667 (0.3879)	loss_scale 4096.0000 (2412.9196)	mem 17013MB
[2024-07-30 13:05:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:34 lr 0.000013	 wd 0.0500	time 0.3447 (0.3370)	loss 0.8642 (1.1594)	grad_norm 0.4002 (0.3872)	loss_scale 4096.0000 (2483.0187)	mem 17013MB
[2024-07-30 13:05:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2922 (0.3362)	loss 1.2452 (1.1583)	grad_norm 0.3741 (0.3868)	loss_scale 4096.0000 (2547.5122)	mem 17013MB
[2024-07-30 13:05:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 14 training takes 0:14:03
[2024-07-30 13:05:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.373 (12.373)	Loss 0.4780 (0.4780)	Acc@1 92.188 (92.188)	Acc@5 98.047 (98.047)	Mem 17013MB
[2024-07-30 13:06:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.150 Acc@5 97.908
[2024-07-30 13:06:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-30 13:06:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 13:06:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 13:06:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 13:06:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:56:15 lr 0.000013	 wd 0.0500	time 11.4210 (11.4210)	loss 1.2564 (1.2564)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:06:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:17:38 lr 0.000013	 wd 0.0500	time 0.2944 (0.4407)	loss 0.7739 (1.1639)	grad_norm 0.3703 (0.3741)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:07:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:44 lr 0.000013	 wd 0.0500	time 0.3429 (0.3844)	loss 1.4378 (1.1500)	grad_norm 0.6772 (nan)	loss_scale 2048.0000 (3178.9851)	mem 17013MB
[2024-07-30 13:08:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:13:23 lr 0.000013	 wd 0.0500	time 0.3458 (0.3650)	loss 0.9291 (1.1498)	grad_norm 0.3741 (nan)	loss_scale 2048.0000 (2803.2425)	mem 17013MB
[2024-07-30 13:08:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:12:28 lr 0.000013	 wd 0.0500	time 0.3414 (0.3560)	loss 0.8025 (1.1485)	grad_norm 0.3576 (nan)	loss_scale 2048.0000 (2614.9027)	mem 17013MB
[2024-07-30 13:09:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:11:41 lr 0.000013	 wd 0.0500	time 0.3069 (0.3504)	loss 0.9442 (1.1597)	grad_norm 0.3804 (nan)	loss_scale 2048.0000 (2501.7485)	mem 17013MB
[2024-07-30 13:09:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:11:00 lr 0.000013	 wd 0.0500	time 0.3202 (0.3472)	loss 1.3801 (1.1625)	grad_norm 0.3701 (nan)	loss_scale 2048.0000 (2426.2496)	mem 17013MB
[2024-07-30 13:10:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:10:21 lr 0.000013	 wd 0.0500	time 0.3033 (0.3447)	loss 1.2408 (1.1625)	grad_norm 0.3723 (nan)	loss_scale 2048.0000 (2372.2910)	mem 17013MB
[2024-07-30 13:10:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:44 lr 0.000013	 wd 0.0500	time 0.3230 (0.3433)	loss 1.3951 (1.1643)	grad_norm 0.3754 (nan)	loss_scale 2048.0000 (2331.8052)	mem 17013MB
[2024-07-30 13:11:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:09:07 lr 0.000013	 wd 0.0500	time 0.3280 (0.3419)	loss 1.2588 (1.1639)	grad_norm 0.3783 (nan)	loss_scale 2048.0000 (2300.3063)	mem 17013MB
[2024-07-30 13:11:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:08:32 lr 0.000013	 wd 0.0500	time 0.3137 (0.3411)	loss 0.8560 (1.1639)	grad_norm 0.3632 (nan)	loss_scale 2048.0000 (2275.1009)	mem 17013MB
[2024-07-30 13:12:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:57 lr 0.000013	 wd 0.0500	time 0.3401 (0.3403)	loss 0.8600 (1.1647)	grad_norm 0.3668 (nan)	loss_scale 2048.0000 (2254.4741)	mem 17013MB
[2024-07-30 13:13:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:07:22 lr 0.000013	 wd 0.0500	time 0.3132 (0.3397)	loss 0.8390 (1.1648)	grad_norm 0.5351 (nan)	loss_scale 2048.0000 (2237.2823)	mem 17013MB
[2024-07-30 13:13:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:47 lr 0.000013	 wd 0.0500	time 0.3257 (0.3392)	loss 0.8724 (1.1659)	grad_norm 0.3639 (nan)	loss_scale 2048.0000 (2222.7333)	mem 17013MB
[2024-07-30 13:14:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:06:13 lr 0.000012	 wd 0.0500	time 0.3023 (0.3388)	loss 0.9434 (1.1660)	grad_norm 0.3634 (nan)	loss_scale 2048.0000 (2210.2612)	mem 17013MB
[2024-07-30 13:14:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:38 lr 0.000012	 wd 0.0500	time 0.3228 (0.3381)	loss 1.0246 (1.1622)	grad_norm 0.3645 (nan)	loss_scale 2048.0000 (2199.4510)	mem 17013MB
[2024-07-30 13:15:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:05:04 lr 0.000012	 wd 0.0500	time 0.4090 (0.3381)	loss 1.0172 (1.1629)	grad_norm 0.3778 (nan)	loss_scale 2048.0000 (2189.9913)	mem 17013MB
[2024-07-30 13:15:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:31 lr 0.000012	 wd 0.0500	time 0.3370 (0.3382)	loss 0.8247 (1.1629)	grad_norm 0.3989 (nan)	loss_scale 2048.0000 (2181.6437)	mem 17013MB
[2024-07-30 13:16:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:57 lr 0.000012	 wd 0.0500	time 0.3235 (0.3378)	loss 1.6644 (1.1610)	grad_norm 0.3967 (nan)	loss_scale 2048.0000 (2174.2232)	mem 17013MB
[2024-07-30 13:16:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:23 lr 0.000012	 wd 0.0500	time 0.3233 (0.3375)	loss 0.9788 (1.1592)	grad_norm 0.4125 (nan)	loss_scale 2048.0000 (2167.5834)	mem 17013MB
[2024-07-30 13:17:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:49 lr 0.000012	 wd 0.0500	time 0.3276 (0.3372)	loss 0.8453 (1.1605)	grad_norm 0.3760 (nan)	loss_scale 2048.0000 (2161.6072)	mem 17013MB
[2024-07-30 13:18:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:15 lr 0.000012	 wd 0.0500	time 0.3186 (0.3368)	loss 0.7870 (1.1592)	grad_norm 0.3884 (nan)	loss_scale 2048.0000 (2156.1999)	mem 17013MB
[2024-07-30 13:18:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:41 lr 0.000012	 wd 0.0500	time 0.3161 (0.3365)	loss 1.0837 (1.1588)	grad_norm 0.3517 (nan)	loss_scale 2048.0000 (2151.2840)	mem 17013MB
[2024-07-30 13:19:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:07 lr 0.000012	 wd 0.0500	time 0.3127 (0.3364)	loss 1.0215 (1.1591)	grad_norm 0.3826 (nan)	loss_scale 2048.0000 (2146.7953)	mem 17013MB
[2024-07-30 13:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:34 lr 0.000012	 wd 0.0500	time 0.3185 (0.3363)	loss 1.4649 (1.1595)	grad_norm 0.3852 (nan)	loss_scale 2048.0000 (2142.6805)	mem 17013MB
[2024-07-30 13:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2842 (0.3354)	loss 1.5073 (1.1600)	grad_norm 0.3824 (nan)	loss_scale 2048.0000 (2138.8948)	mem 17013MB
[2024-07-30 13:20:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 15 training takes 0:14:01
[2024-07-30 13:20:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.124 (12.124)	Loss 0.5327 (0.5327)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 13:20:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.092 Acc@5 97.862
[2024-07-30 13:20:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 13:20:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 13:20:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:07:44 lr 0.000012	 wd 0.0500	time 11.6964 (11.6964)	loss 1.2023 (1.2023)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:17:46 lr 0.000012	 wd 0.0500	time 0.3036 (0.4440)	loss 1.0434 (1.1699)	grad_norm 0.3730 (0.3766)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:22:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:15:06 lr 0.000012	 wd 0.0500	time 0.3300 (0.3936)	loss 1.2524 (1.1599)	grad_norm 0.3691 (0.4001)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:22:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:13:37 lr 0.000012	 wd 0.0500	time 0.2972 (0.3711)	loss 1.0069 (1.1530)	grad_norm 0.3622 (0.3967)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:12:35 lr 0.000012	 wd 0.0500	time 0.3372 (0.3593)	loss 0.9251 (1.1417)	grad_norm 0.3804 (0.3909)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:23:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:47 lr 0.000012	 wd 0.0500	time 0.3440 (0.3534)	loss 1.5236 (1.1445)	grad_norm 0.3784 (0.3879)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:24:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:11:07 lr 0.000012	 wd 0.0500	time 0.3068 (0.3507)	loss 1.4447 (1.1453)	grad_norm 0.3391 (0.3877)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:24:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:10:27 lr 0.000012	 wd 0.0500	time 0.3305 (0.3482)	loss 1.0693 (1.1478)	grad_norm 0.3717 (0.3874)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:25:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:48 lr 0.000012	 wd 0.0500	time 0.3349 (0.3459)	loss 1.3136 (1.1493)	grad_norm 0.3842 (0.3859)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:25:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:09:11 lr 0.000012	 wd 0.0500	time 0.3132 (0.3442)	loss 1.1363 (1.1504)	grad_norm 0.3667 (0.3844)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:26:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:35 lr 0.000011	 wd 0.0500	time 0.3479 (0.3434)	loss 1.3518 (1.1503)	grad_norm 0.3615 (0.3837)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:27:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:08:01 lr 0.000011	 wd 0.0500	time 0.2960 (0.3435)	loss 0.8461 (1.1544)	grad_norm 0.3882 (0.3829)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:27:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:27 lr 0.000011	 wd 0.0500	time 0.3261 (0.3435)	loss 0.8733 (1.1521)	grad_norm 0.3639 (0.3830)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:28:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:51 lr 0.000011	 wd 0.0500	time 0.3113 (0.3420)	loss 1.2961 (1.1550)	grad_norm 0.3951 (0.3820)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:28:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:06:17 lr 0.000011	 wd 0.0500	time 0.3054 (0.3424)	loss 1.2297 (1.1550)	grad_norm 0.3776 (0.3817)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:29:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:42 lr 0.000011	 wd 0.0500	time 0.3260 (0.3415)	loss 1.2897 (1.1573)	grad_norm 0.3535 (0.3819)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:29:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:05:07 lr 0.000011	 wd 0.0500	time 0.3330 (0.3408)	loss 1.1204 (1.1579)	grad_norm 0.3863 (0.3817)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:30:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:33 lr 0.000011	 wd 0.0500	time 0.2962 (0.3406)	loss 1.0740 (1.1570)	grad_norm 0.4574 (0.3815)	loss_scale 4096.0000 (2158.7678)	mem 17013MB
[2024-07-30 13:30:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:58 lr 0.000011	 wd 0.0500	time 0.2968 (0.3400)	loss 0.8749 (1.1559)	grad_norm 0.3396 (0.3814)	loss_scale 4096.0000 (2266.3320)	mem 17013MB
[2024-07-30 13:31:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:24 lr 0.000011	 wd 0.0500	time 0.3000 (0.3400)	loss 1.0343 (1.1567)	grad_norm 0.4048 (0.3811)	loss_scale 4096.0000 (2362.5797)	mem 17013MB
[2024-07-30 13:32:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:50 lr 0.000011	 wd 0.0500	time 0.3410 (0.3397)	loss 1.3546 (1.1574)	grad_norm 0.3556 (0.3810)	loss_scale 4096.0000 (2449.2074)	mem 17013MB
[2024-07-30 13:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:16 lr 0.000011	 wd 0.0500	time 0.3554 (0.3398)	loss 1.3579 (1.1582)	grad_norm 0.3507 (0.3807)	loss_scale 4096.0000 (2527.5888)	mem 17013MB
[2024-07-30 13:33:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:42 lr 0.000011	 wd 0.0500	time 0.3001 (0.3394)	loss 1.0098 (1.1578)	grad_norm 0.3815 (0.3805)	loss_scale 4096.0000 (2598.8478)	mem 17013MB
[2024-07-30 13:33:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:08 lr 0.000011	 wd 0.0500	time 0.3301 (0.3393)	loss 0.8890 (1.1560)	grad_norm 0.7800 (0.3805)	loss_scale 4096.0000 (2663.9131)	mem 17013MB
[2024-07-30 13:34:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:34 lr 0.000011	 wd 0.0500	time 0.3017 (0.3389)	loss 1.2130 (1.1550)	grad_norm 0.3558 (0.3803)	loss_scale 4096.0000 (2723.5585)	mem 17013MB
[2024-07-30 13:34:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.3161 (0.3378)	loss 0.8443 (1.1553)	grad_norm 0.3805 (0.3801)	loss_scale 4096.0000 (2778.4342)	mem 17013MB
[2024-07-30 13:34:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 16 training takes 0:14:07
[2024-07-30 13:35:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.989 (11.989)	Loss 0.5005 (0.5005)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 13:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.136 Acc@5 97.876
[2024-07-30 13:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 13:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 13:35:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:21:26 lr 0.000011	 wd 0.0500	time 12.0252 (12.0252)	loss 1.4021 (1.4021)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:36:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:17:51 lr 0.000011	 wd 0.0500	time 0.3437 (0.4461)	loss 1.2289 (1.1386)	grad_norm 0.3549 (0.3848)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:36:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:46 lr 0.000011	 wd 0.0500	time 0.3031 (0.3852)	loss 1.2802 (1.1386)	grad_norm 0.3720 (0.3811)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:37:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:13:29 lr 0.000011	 wd 0.0500	time 0.3051 (0.3678)	loss 1.2356 (1.1540)	grad_norm 0.3731 (0.3807)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:37:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:29 lr 0.000011	 wd 0.0500	time 0.3250 (0.3564)	loss 1.4154 (1.1522)	grad_norm 0.3691 (0.3798)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:38:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:43 lr 0.000010	 wd 0.0500	time 0.3065 (0.3512)	loss 1.3596 (1.1564)	grad_norm 0.3784 (0.3797)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:38:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:11:00 lr 0.000010	 wd 0.0500	time 0.3145 (0.3471)	loss 0.8048 (1.1575)	grad_norm 0.3786 (0.3794)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:39:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:10:21 lr 0.000010	 wd 0.0500	time 0.3010 (0.3449)	loss 1.2787 (1.1552)	grad_norm 0.3762 (0.3820)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:39:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:42 lr 0.000010	 wd 0.0500	time 0.3033 (0.3423)	loss 1.1818 (1.1534)	grad_norm 0.3760 (0.3811)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 13:40:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:09:10 lr 0.000010	 wd 0.0500	time 0.2950 (0.3434)	loss 1.2888 (1.1551)	grad_norm 0.3785 (nan)	loss_scale 2048.0000 (3950.5261)	mem 17013MB
[2024-07-30 13:41:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:33 lr 0.000010	 wd 0.0500	time 0.3145 (0.3419)	loss 1.0317 (1.1544)	grad_norm 0.3810 (nan)	loss_scale 2048.0000 (3760.4635)	mem 17013MB
[2024-07-30 13:41:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:58 lr 0.000010	 wd 0.0500	time 0.3321 (0.3410)	loss 1.0267 (1.1589)	grad_norm 0.3659 (nan)	loss_scale 2048.0000 (3604.9264)	mem 17013MB
[2024-07-30 13:42:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:22 lr 0.000010	 wd 0.0500	time 0.3220 (0.3402)	loss 0.7612 (1.1588)	grad_norm 0.3641 (nan)	loss_scale 2048.0000 (3475.2906)	mem 17013MB
[2024-07-30 13:42:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:48 lr 0.000010	 wd 0.0500	time 0.2916 (0.3396)	loss 1.4180 (1.1559)	grad_norm 0.3817 (nan)	loss_scale 2048.0000 (3365.5834)	mem 17013MB
[2024-07-30 13:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:06:13 lr 0.000010	 wd 0.0500	time 0.2961 (0.3393)	loss 1.3167 (1.1560)	grad_norm 0.3483 (nan)	loss_scale 2048.0000 (3271.5375)	mem 17013MB
[2024-07-30 13:43:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:39 lr 0.000010	 wd 0.0500	time 0.3213 (0.3384)	loss 1.4584 (1.1566)	grad_norm 0.3678 (nan)	loss_scale 2048.0000 (3190.0227)	mem 17013MB
[2024-07-30 13:44:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:05:04 lr 0.000010	 wd 0.0500	time 0.2967 (0.3379)	loss 1.3857 (1.1565)	grad_norm 0.3674 (nan)	loss_scale 2048.0000 (3118.6908)	mem 17013MB
[2024-07-30 13:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:30 lr 0.000010	 wd 0.0500	time 0.2954 (0.3372)	loss 1.5263 (1.1583)	grad_norm 0.3707 (nan)	loss_scale 2048.0000 (3055.7460)	mem 17013MB
[2024-07-30 13:45:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:56 lr 0.000010	 wd 0.0500	time 0.3093 (0.3373)	loss 0.9726 (1.1577)	grad_norm 0.3867 (nan)	loss_scale 2048.0000 (2999.7912)	mem 17013MB
[2024-07-30 13:45:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:22 lr 0.000010	 wd 0.0500	time 0.3020 (0.3368)	loss 1.2410 (1.1567)	grad_norm 0.3825 (nan)	loss_scale 2048.0000 (2949.7233)	mem 17013MB
[2024-07-30 13:46:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:49 lr 0.000010	 wd 0.0500	time 0.3026 (0.3368)	loss 1.1994 (1.1569)	grad_norm 0.4005 (nan)	loss_scale 2048.0000 (2904.6597)	mem 17013MB
[2024-07-30 13:47:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:15 lr 0.000010	 wd 0.0500	time 0.4072 (0.3367)	loss 1.4668 (1.1568)	grad_norm 1.8299 (nan)	loss_scale 2048.0000 (2863.8858)	mem 17013MB
[2024-07-30 13:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:41 lr 0.000010	 wd 0.0500	time 0.3582 (0.3368)	loss 1.3674 (1.1567)	grad_norm 0.3661 (nan)	loss_scale 2048.0000 (2826.8169)	mem 17013MB
[2024-07-30 13:48:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:08 lr 0.000010	 wd 0.0500	time 0.3092 (0.3368)	loss 0.7968 (1.1561)	grad_norm 0.3909 (nan)	loss_scale 2048.0000 (2792.9700)	mem 17013MB
[2024-07-30 13:48:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:34 lr 0.000010	 wd 0.0500	time 0.3041 (0.3366)	loss 0.9311 (1.1548)	grad_norm 0.3848 (nan)	loss_scale 2048.0000 (2761.9425)	mem 17013MB
[2024-07-30 13:49:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2907 (0.3359)	loss 1.4525 (1.1545)	grad_norm 0.3979 (nan)	loss_scale 2048.0000 (2733.3962)	mem 17013MB
[2024-07-30 13:49:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 17 training takes 0:14:02
[2024-07-30 13:49:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.384 (12.384)	Loss 0.4902 (0.4902)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 13:49:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.148 Acc@5 97.872
[2024-07-30 13:49:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 13:49:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 13:50:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:13:49 lr 0.000009	 wd 0.0500	time 11.8423 (11.8423)	loss 1.4396 (1.4396)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:50:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:17:31 lr 0.000009	 wd 0.0500	time 0.3492 (0.4379)	loss 1.4829 (1.1887)	grad_norm 0.3771 (0.3878)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:51:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:39 lr 0.000009	 wd 0.0500	time 0.2905 (0.3820)	loss 1.0310 (1.1802)	grad_norm 0.4033 (0.3886)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:51:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:13:18 lr 0.000009	 wd 0.0500	time 0.2885 (0.3625)	loss 1.5037 (1.1790)	grad_norm 0.3657 (0.3976)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:52:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:12:24 lr 0.000009	 wd 0.0500	time 0.3016 (0.3543)	loss 0.7507 (1.1776)	grad_norm 0.3929 (0.3932)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:52:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:11:38 lr 0.000009	 wd 0.0500	time 0.3057 (0.3487)	loss 1.2822 (1.1693)	grad_norm 0.3513 (0.3927)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:53:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:10:57 lr 0.000009	 wd 0.0500	time 0.3054 (0.3456)	loss 1.4911 (1.1619)	grad_norm 0.3822 (0.3896)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:53:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:10:17 lr 0.000009	 wd 0.0500	time 0.3626 (0.3424)	loss 1.4496 (1.1669)	grad_norm 0.3862 (0.3911)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:54:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:41 lr 0.000009	 wd 0.0500	time 0.3078 (0.3415)	loss 1.3216 (1.1698)	grad_norm 0.3678 (0.3922)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:54:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:09:04 lr 0.000009	 wd 0.0500	time 0.3416 (0.3402)	loss 0.9159 (1.1696)	grad_norm 0.3784 (0.3904)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:55:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:29 lr 0.000009	 wd 0.0500	time 0.3296 (0.3392)	loss 1.4247 (1.1654)	grad_norm 0.3643 (0.3888)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:56:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:54 lr 0.000009	 wd 0.0500	time 0.3485 (0.3385)	loss 0.9956 (1.1673)	grad_norm 0.3623 (0.3881)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:19 lr 0.000009	 wd 0.0500	time 0.3557 (0.3374)	loss 1.0541 (1.1604)	grad_norm 0.3695 (0.3878)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:57:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:45 lr 0.000009	 wd 0.0500	time 0.3168 (0.3372)	loss 1.4695 (1.1607)	grad_norm 0.3739 (0.3874)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:57:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:06:11 lr 0.000009	 wd 0.0500	time 0.3107 (0.3371)	loss 1.1508 (1.1610)	grad_norm 0.3484 (0.3866)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:58:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:37 lr 0.000009	 wd 0.0500	time 0.3032 (0.3371)	loss 1.3103 (1.1606)	grad_norm 0.3541 (0.3854)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:58:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:05:03 lr 0.000009	 wd 0.0500	time 0.3260 (0.3366)	loss 1.4521 (1.1626)	grad_norm 0.4043 (0.3874)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:59:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:29 lr 0.000009	 wd 0.0500	time 0.3507 (0.3362)	loss 1.3278 (1.1645)	grad_norm 0.3710 (0.3870)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 13:59:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:56 lr 0.000009	 wd 0.0500	time 0.3428 (0.3362)	loss 0.7219 (1.1648)	grad_norm 0.3744 (0.3924)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:00:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:22 lr 0.000009	 wd 0.0500	time 0.3262 (0.3361)	loss 1.4246 (1.1644)	grad_norm 0.3668 (0.3928)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:01:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:48 lr 0.000008	 wd 0.0500	time 0.3008 (0.3362)	loss 1.0699 (1.1641)	grad_norm 0.3680 (0.3928)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:01:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:15 lr 0.000008	 wd 0.0500	time 0.2881 (0.3362)	loss 1.3228 (1.1630)	grad_norm 0.3697 (0.3924)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:02:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:41 lr 0.000008	 wd 0.0500	time 0.3508 (0.3363)	loss 1.0201 (1.1632)	grad_norm 0.3487 (0.3916)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:02:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:07 lr 0.000008	 wd 0.0500	time 0.2916 (0.3364)	loss 1.3106 (1.1638)	grad_norm 0.3639 (0.3910)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:03:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.3278 (0.3365)	loss 1.4242 (1.1644)	grad_norm 0.3886 (0.3904)	loss_scale 4096.0000 (2104.2965)	mem 17013MB
[2024-07-30 14:03:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2913 (0.3358)	loss 1.2613 (1.1647)	grad_norm 0.3637 (0.3911)	loss_scale 4096.0000 (2183.9328)	mem 17013MB
[2024-07-30 14:03:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 18 training takes 0:14:02
[2024-07-30 14:04:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.571 (11.571)	Loss 0.5020 (0.5020)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 14:04:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.096 Acc@5 97.868
[2024-07-30 14:04:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 14:04:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 14:04:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:11:05 lr 0.000008	 wd 0.0500	time 11.7766 (11.7766)	loss 0.7971 (0.7971)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:05:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:40 lr 0.000008	 wd 0.0500	time 0.2957 (0.4413)	loss 1.5959 (1.2110)	grad_norm 0.3551 (0.3734)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:05:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:42 lr 0.000008	 wd 0.0500	time 0.2969 (0.3832)	loss 1.1772 (1.1806)	grad_norm 0.3951 (0.3763)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:06:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:13:19 lr 0.000008	 wd 0.0500	time 0.3197 (0.3629)	loss 1.3980 (1.1671)	grad_norm 0.3844 (0.3756)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:06:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:12:26 lr 0.000008	 wd 0.0500	time 0.2905 (0.3550)	loss 0.9631 (1.1605)	grad_norm 0.3661 (0.3759)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:07:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:40 lr 0.000008	 wd 0.0500	time 0.3394 (0.3498)	loss 1.4481 (1.1648)	grad_norm 0.3863 (0.3772)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:07:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:11:01 lr 0.000008	 wd 0.0500	time 0.3330 (0.3480)	loss 0.8462 (1.1543)	grad_norm 0.4007 (nan)	loss_scale 2048.0000 (3857.4642)	mem 17013MB
[2024-07-30 14:08:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:10:23 lr 0.000008	 wd 0.0500	time 0.3279 (0.3458)	loss 1.2372 (1.1523)	grad_norm 0.3560 (nan)	loss_scale 2048.0000 (3599.3381)	mem 17013MB
[2024-07-30 14:08:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:46 lr 0.000008	 wd 0.0500	time 0.3485 (0.3444)	loss 1.5193 (1.1541)	grad_norm 0.3871 (nan)	loss_scale 2048.0000 (3405.6629)	mem 17013MB
[2024-07-30 14:09:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:09:10 lr 0.000008	 wd 0.0500	time 0.3156 (0.3435)	loss 1.2755 (1.1546)	grad_norm 0.3849 (nan)	loss_scale 2048.0000 (3254.9789)	mem 17013MB
[2024-07-30 14:10:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:33 lr 0.000008	 wd 0.0500	time 0.3000 (0.3422)	loss 1.4510 (1.1561)	grad_norm 0.3580 (nan)	loss_scale 2048.0000 (3134.4016)	mem 17013MB
[2024-07-30 14:10:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:58 lr 0.000008	 wd 0.0500	time 0.3197 (0.3416)	loss 1.1777 (1.1550)	grad_norm 0.4034 (nan)	loss_scale 2048.0000 (3035.7275)	mem 17013MB
[2024-07-30 14:11:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:23 lr 0.000008	 wd 0.0500	time 0.3529 (0.3404)	loss 1.3225 (1.1547)	grad_norm 0.3749 (nan)	loss_scale 2048.0000 (2953.4854)	mem 17013MB
[2024-07-30 14:11:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:48 lr 0.000008	 wd 0.0500	time 0.3296 (0.3399)	loss 0.7790 (1.1536)	grad_norm 0.3833 (nan)	loss_scale 2048.0000 (2883.8862)	mem 17013MB
[2024-07-30 14:12:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:06:14 lr 0.000008	 wd 0.0500	time 0.3578 (0.3396)	loss 0.8020 (1.1529)	grad_norm 0.3744 (nan)	loss_scale 2048.0000 (2824.2227)	mem 17013MB
[2024-07-30 14:12:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:39 lr 0.000008	 wd 0.0500	time 0.3148 (0.3389)	loss 1.3626 (1.1541)	grad_norm 0.3568 (nan)	loss_scale 2048.0000 (2772.5090)	mem 17013MB
[2024-07-30 14:13:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:05:05 lr 0.000007	 wd 0.0500	time 0.2965 (0.3388)	loss 1.4123 (1.1558)	grad_norm 0.3688 (nan)	loss_scale 2048.0000 (2727.2555)	mem 17013MB
[2024-07-30 14:13:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:31 lr 0.000007	 wd 0.0500	time 0.3464 (0.3381)	loss 0.8945 (1.1563)	grad_norm 0.3975 (nan)	loss_scale 2048.0000 (2687.3228)	mem 17013MB
[2024-07-30 14:14:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:57 lr 0.000007	 wd 0.0500	time 0.3155 (0.3380)	loss 1.3275 (1.1587)	grad_norm 0.3732 (nan)	loss_scale 2048.0000 (2651.8245)	mem 17013MB
[2024-07-30 14:15:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:23 lr 0.000007	 wd 0.0500	time 0.3060 (0.3376)	loss 1.0983 (1.1577)	grad_norm 0.3954 (nan)	loss_scale 2048.0000 (2620.0610)	mem 17013MB
[2024-07-30 14:15:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:49 lr 0.000007	 wd 0.0500	time 0.3287 (0.3376)	loss 1.4711 (1.1589)	grad_norm 0.3823 (nan)	loss_scale 2048.0000 (2591.4723)	mem 17013MB
[2024-07-30 14:16:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:15 lr 0.000007	 wd 0.0500	time 0.3292 (0.3376)	loss 1.5769 (1.1602)	grad_norm 0.3590 (nan)	loss_scale 2048.0000 (2565.6050)	mem 17013MB
[2024-07-30 14:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:41 lr 0.000007	 wd 0.0500	time 0.2901 (0.3372)	loss 1.1392 (1.1594)	grad_norm 0.3982 (nan)	loss_scale 2048.0000 (2542.0881)	mem 17013MB
[2024-07-30 14:17:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:08 lr 0.000007	 wd 0.0500	time 0.3244 (0.3370)	loss 1.0063 (1.1596)	grad_norm 0.3635 (nan)	loss_scale 2048.0000 (2520.6154)	mem 17013MB
[2024-07-30 14:17:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:34 lr 0.000007	 wd 0.0500	time 0.3000 (0.3368)	loss 1.1024 (1.1587)	grad_norm 0.3791 (nan)	loss_scale 2048.0000 (2500.9313)	mem 17013MB
[2024-07-30 14:18:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2912 (0.3361)	loss 0.8081 (1.1589)	grad_norm 0.3766 (nan)	loss_scale 2048.0000 (2482.8213)	mem 17013MB
[2024-07-30 14:18:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 19 training takes 0:14:03
[2024-07-30 14:18:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.581 (11.581)	Loss 0.4861 (0.4861)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 14:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.148 Acc@5 97.890
[2024-07-30 14:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 14:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 14:19:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:17:39 lr 0.000007	 wd 0.0500	time 11.9342 (11.9342)	loss 1.5239 (1.5239)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:19:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:17:37 lr 0.000007	 wd 0.0500	time 0.3104 (0.4403)	loss 1.0646 (1.1688)	grad_norm 0.3772 (0.3882)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:20:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:41 lr 0.000007	 wd 0.0500	time 0.2941 (0.3831)	loss 1.3140 (1.1523)	grad_norm 0.3933 (0.3833)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:20:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:13:22 lr 0.000007	 wd 0.0500	time 0.3191 (0.3646)	loss 0.6966 (1.1644)	grad_norm 0.3996 (0.3805)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:21:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:12:29 lr 0.000007	 wd 0.0500	time 0.3507 (0.3564)	loss 1.3060 (1.1637)	grad_norm 0.3716 (0.4392)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:21:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:11:48 lr 0.000007	 wd 0.0500	time 0.3061 (0.3540)	loss 1.2763 (1.1616)	grad_norm 0.3655 (0.4269)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:22:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:11:05 lr 0.000007	 wd 0.0500	time 0.3069 (0.3499)	loss 1.2493 (1.1665)	grad_norm 0.3980 (0.4191)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:22:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:10:24 lr 0.000007	 wd 0.0500	time 0.3330 (0.3468)	loss 1.2649 (1.1649)	grad_norm 0.3680 (0.4133)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:23:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:46 lr 0.000007	 wd 0.0500	time 0.3419 (0.3448)	loss 1.1625 (1.1652)	grad_norm 0.3711 (0.4113)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:24:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:09:11 lr 0.000007	 wd 0.0500	time 0.3098 (0.3440)	loss 1.2579 (1.1645)	grad_norm 0.3764 (0.4074)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:24:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:08:35 lr 0.000007	 wd 0.0500	time 0.3170 (0.3433)	loss 1.3885 (1.1664)	grad_norm 0.3574 (0.4049)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:25:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:08:00 lr 0.000007	 wd 0.0500	time 0.3127 (0.3425)	loss 1.0703 (1.1642)	grad_norm 0.3912 (0.4035)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:25:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:25 lr 0.000006	 wd 0.0500	time 0.3020 (0.3419)	loss 1.2214 (1.1603)	grad_norm 0.3580 (0.4018)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:26:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:50 lr 0.000006	 wd 0.0500	time 0.2899 (0.3414)	loss 1.3413 (1.1599)	grad_norm 0.3708 (0.4005)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:26:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:06:15 lr 0.000006	 wd 0.0500	time 0.3447 (0.3411)	loss 1.0729 (1.1613)	grad_norm 0.3957 (0.4008)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:27:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:41 lr 0.000006	 wd 0.0500	time 0.3457 (0.3406)	loss 1.3172 (1.1640)	grad_norm 0.3614 (0.4006)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:27:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:05:07 lr 0.000006	 wd 0.0500	time 0.3224 (0.3414)	loss 1.2886 (1.1660)	grad_norm 0.3828 (0.4064)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:28:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:33 lr 0.000006	 wd 0.0500	time 0.3257 (0.3408)	loss 1.6102 (1.1673)	grad_norm 0.3562 (0.4087)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:29:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:59 lr 0.000006	 wd 0.0500	time 0.2917 (0.3406)	loss 0.7649 (1.1650)	grad_norm 0.3708 (0.4080)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:29:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:24 lr 0.000006	 wd 0.0500	time 0.3167 (0.3403)	loss 0.8333 (1.1662)	grad_norm 0.3672 (0.4069)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:30:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:50 lr 0.000006	 wd 0.0500	time 0.3679 (0.3400)	loss 1.0998 (1.1653)	grad_norm 0.7708 (0.4082)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 14:30:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:16 lr 0.000006	 wd 0.0500	time 0.3352 (0.3397)	loss 1.2351 (1.1653)	grad_norm 0.3502 (0.4067)	loss_scale 4096.0000 (2118.1837)	mem 17013MB
[2024-07-30 14:31:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:42 lr 0.000006	 wd 0.0500	time 0.3039 (0.3395)	loss 1.3842 (1.1664)	grad_norm 0.3779 (0.4058)	loss_scale 4096.0000 (2208.0436)	mem 17013MB
[2024-07-30 14:31:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:08 lr 0.000006	 wd 0.0500	time 0.3179 (0.3399)	loss 0.8668 (1.1676)	grad_norm 0.3680 (0.4047)	loss_scale 4096.0000 (2290.0930)	mem 17013MB
[2024-07-30 14:32:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:34 lr 0.000006	 wd 0.0500	time 0.3066 (0.3396)	loss 0.7873 (1.1670)	grad_norm 0.3770 (0.4034)	loss_scale 4096.0000 (2365.3078)	mem 17013MB
[2024-07-30 14:32:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2923 (0.3388)	loss 1.0471 (1.1688)	grad_norm 0.3598 (0.4026)	loss_scale 4096.0000 (2434.5078)	mem 17013MB
[2024-07-30 14:33:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 20 training takes 0:14:10
[2024-07-30 14:33:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_20.pth saving......
[2024-07-30 14:33:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_20.pth saved !!!
[2024-07-30 14:33:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.701 (11.701)	Loss 0.4961 (0.4961)	Acc@1 92.383 (92.383)	Acc@5 98.047 (98.047)	Mem 17013MB
[2024-07-30 14:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.138 Acc@5 97.888
[2024-07-30 14:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 14:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 14:33:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:55:43 lr 0.000006	 wd 0.0500	time 11.4082 (11.4082)	loss 0.9990 (0.9990)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:34:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:17:44 lr 0.000006	 wd 0.0500	time 0.3186 (0.4431)	loss 1.3581 (1.1889)	grad_norm 0.3896 (0.3899)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:34:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:14:47 lr 0.000006	 wd 0.0500	time 0.3356 (0.3857)	loss 1.3147 (1.1833)	grad_norm 0.3744 (0.3860)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:35:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:13:28 lr 0.000006	 wd 0.0500	time 0.2953 (0.3670)	loss 1.1836 (1.1730)	grad_norm 0.3906 (0.3840)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:35:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:12:31 lr 0.000006	 wd 0.0500	time 0.2970 (0.3573)	loss 1.3624 (1.1650)	grad_norm 0.3720 (0.3824)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:36:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:44 lr 0.000006	 wd 0.0500	time 0.2983 (0.3518)	loss 0.9051 (1.1600)	grad_norm 0.3715 (0.3811)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:36:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:11:04 lr 0.000006	 wd 0.0500	time 0.3247 (0.3491)	loss 1.2987 (1.1553)	grad_norm 0.3612 (0.3805)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:37:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:10:23 lr 0.000006	 wd 0.0500	time 0.3126 (0.3459)	loss 1.2377 (1.1529)	grad_norm 0.3652 (0.3800)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:38:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:45 lr 0.000006	 wd 0.0500	time 0.3848 (0.3440)	loss 1.2127 (1.1514)	grad_norm 0.3794 (0.3792)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:38:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:09:08 lr 0.000005	 wd 0.0500	time 0.3237 (0.3425)	loss 1.4360 (1.1544)	grad_norm 0.3823 (0.3808)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:39:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:32 lr 0.000005	 wd 0.0500	time 0.3148 (0.3414)	loss 1.4933 (1.1578)	grad_norm 0.3646 (0.3828)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:39:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:57 lr 0.000005	 wd 0.0500	time 0.2928 (0.3407)	loss 0.8036 (1.1609)	grad_norm 0.3709 (0.3822)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:40:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:22 lr 0.000005	 wd 0.0500	time 0.2959 (0.3400)	loss 1.3902 (1.1579)	grad_norm 0.3975 (0.3862)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:40:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:48 lr 0.000005	 wd 0.0500	time 0.3334 (0.3395)	loss 1.1952 (1.1583)	grad_norm 0.3618 (0.3862)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:41:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:06:13 lr 0.000005	 wd 0.0500	time 0.3209 (0.3389)	loss 0.8306 (1.1557)	grad_norm 0.4009 (0.3857)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:41:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:40 lr 0.000005	 wd 0.0500	time 0.2981 (0.3395)	loss 0.8444 (1.1573)	grad_norm 0.3708 (0.3853)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:42:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:05:05 lr 0.000005	 wd 0.0500	time 0.3643 (0.3391)	loss 1.0488 (1.1539)	grad_norm 0.3992 (0.3849)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:43:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:32 lr 0.000005	 wd 0.0500	time 0.3207 (0.3400)	loss 1.3460 (1.1547)	grad_norm 0.3674 (0.3846)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:43:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:58 lr 0.000005	 wd 0.0500	time 0.3328 (0.3396)	loss 0.7714 (1.1552)	grad_norm 0.3822 (0.3841)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:44:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:24 lr 0.000005	 wd 0.0500	time 0.3186 (0.3394)	loss 0.8445 (1.1559)	grad_norm 0.3676 (0.3837)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:44:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:50 lr 0.000005	 wd 0.0500	time 0.2994 (0.3393)	loss 1.0664 (1.1550)	grad_norm 0.4050 (0.3837)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:16 lr 0.000005	 wd 0.0500	time 0.3461 (0.3392)	loss 1.1345 (1.1547)	grad_norm 0.3666 (0.3838)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:42 lr 0.000005	 wd 0.0500	time 0.3467 (0.3390)	loss 1.4454 (1.1533)	grad_norm 0.4222 (0.3835)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:46:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:08 lr 0.000005	 wd 0.0500	time 0.3060 (0.3388)	loss 1.5347 (1.1550)	grad_norm 0.4079 (0.3838)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:47:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:34 lr 0.000005	 wd 0.0500	time 0.3548 (0.3386)	loss 1.2224 (1.1568)	grad_norm 0.3876 (0.3837)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:47:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2918 (0.3376)	loss 1.4830 (1.1572)	grad_norm 0.3697 (0.3834)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:47:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 21 training takes 0:14:07
[2024-07-30 14:47:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.029 (12.029)	Loss 0.5142 (0.5142)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 14:48:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.150 Acc@5 97.862
[2024-07-30 14:48:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-30 14:48:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-30 14:48:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 14:48:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 14:48:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:38:06 lr 0.000005	 wd 0.0500	time 10.9859 (10.9859)	loss 0.9904 (0.9904)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:48:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:17:30 lr 0.000005	 wd 0.0500	time 0.3060 (0.4372)	loss 1.4747 (1.1740)	grad_norm 0.4010 (0.3813)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:49:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:48 lr 0.000005	 wd 0.0500	time 0.2936 (0.3860)	loss 0.7456 (1.1667)	grad_norm 0.3650 (0.3867)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:13:34 lr 0.000005	 wd 0.0500	time 0.3257 (0.3697)	loss 1.3679 (1.1720)	grad_norm 0.3713 (0.3994)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:50:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:12:33 lr 0.000005	 wd 0.0500	time 0.3188 (0.3586)	loss 0.7548 (1.1778)	grad_norm 0.3636 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:51:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:48 lr 0.000005	 wd 0.0500	time 0.3115 (0.3540)	loss 1.1897 (1.1752)	grad_norm 0.5046 (0.3969)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:51:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:11:09 lr 0.000005	 wd 0.0500	time 0.3429 (0.3521)	loss 0.8777 (1.1768)	grad_norm 0.3575 (0.3964)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 14:52:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:10:30 lr 0.000005	 wd 0.0500	time 0.3032 (0.3500)	loss 1.3477 (1.1715)	grad_norm 0.3837 (nan)	loss_scale 2048.0000 (3920.7076)	mem 17013MB
[2024-07-30 14:52:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:51 lr 0.000004	 wd 0.0500	time 0.3814 (0.3473)	loss 0.9275 (1.1703)	grad_norm 0.3568 (nan)	loss_scale 2048.0000 (3686.9114)	mem 17013MB
[2024-07-30 14:53:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:09:15 lr 0.000004	 wd 0.0500	time 0.2946 (0.3467)	loss 1.0990 (1.1691)	grad_norm 0.3827 (nan)	loss_scale 2048.0000 (3505.0122)	mem 17013MB
[2024-07-30 14:53:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:39 lr 0.000004	 wd 0.0500	time 0.3398 (0.3457)	loss 1.2678 (1.1694)	grad_norm 0.3644 (nan)	loss_scale 2048.0000 (3359.4565)	mem 17013MB
[2024-07-30 14:54:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:08:02 lr 0.000004	 wd 0.0500	time 0.2973 (0.3445)	loss 0.8043 (1.1665)	grad_norm 0.3741 (nan)	loss_scale 2048.0000 (3240.3415)	mem 17013MB
[2024-07-30 14:55:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:30 lr 0.000004	 wd 0.0500	time 0.3225 (0.3460)	loss 1.3705 (1.1675)	grad_norm 0.3663 (nan)	loss_scale 2048.0000 (3141.0624)	mem 17013MB
[2024-07-30 14:55:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:54 lr 0.000004	 wd 0.0500	time 0.2961 (0.3447)	loss 0.8544 (1.1612)	grad_norm 0.3591 (nan)	loss_scale 2048.0000 (3057.0453)	mem 17013MB
[2024-07-30 14:56:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:06:18 lr 0.000004	 wd 0.0500	time 0.3558 (0.3438)	loss 1.0060 (1.1558)	grad_norm 0.3811 (nan)	loss_scale 2048.0000 (2985.0221)	mem 17013MB
[2024-07-30 14:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:43 lr 0.000004	 wd 0.0500	time 0.3062 (0.3429)	loss 1.1984 (1.1554)	grad_norm 0.3564 (nan)	loss_scale 2048.0000 (2922.5956)	mem 17013MB
[2024-07-30 14:57:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:05:08 lr 0.000004	 wd 0.0500	time 0.3368 (0.3424)	loss 1.5173 (1.1555)	grad_norm 0.3649 (nan)	loss_scale 2048.0000 (2867.9675)	mem 17013MB
[2024-07-30 14:57:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:34 lr 0.000004	 wd 0.0500	time 0.3295 (0.3419)	loss 0.9001 (1.1537)	grad_norm 0.3757 (nan)	loss_scale 2048.0000 (2819.7625)	mem 17013MB
[2024-07-30 14:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:04:00 lr 0.000004	 wd 0.0500	time 0.3254 (0.3426)	loss 1.4116 (1.1523)	grad_norm 0.3705 (nan)	loss_scale 2048.0000 (2776.9106)	mem 17013MB
[2024-07-30 14:58:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:25 lr 0.000004	 wd 0.0500	time 0.2856 (0.3421)	loss 0.9390 (1.1531)	grad_norm 0.3789 (nan)	loss_scale 2048.0000 (2738.5671)	mem 17013MB
[2024-07-30 14:59:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:51 lr 0.000004	 wd 0.0500	time 0.3134 (0.3419)	loss 0.9235 (1.1542)	grad_norm 0.3613 (nan)	loss_scale 2048.0000 (2704.0560)	mem 17013MB
[2024-07-30 15:00:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:17 lr 0.000004	 wd 0.0500	time 0.3266 (0.3415)	loss 1.3000 (1.1557)	grad_norm 0.3698 (nan)	loss_scale 2048.0000 (2672.8301)	mem 17013MB
[2024-07-30 15:00:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:43 lr 0.000004	 wd 0.0500	time 0.3342 (0.3411)	loss 1.0851 (1.1556)	grad_norm 0.3770 (nan)	loss_scale 2048.0000 (2644.4416)	mem 17013MB
[2024-07-30 15:01:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:08 lr 0.000004	 wd 0.0500	time 0.3084 (0.3407)	loss 1.1407 (1.1556)	grad_norm 0.3641 (nan)	loss_scale 2048.0000 (2618.5206)	mem 17013MB
[2024-07-30 15:01:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:34 lr 0.000004	 wd 0.0500	time 0.3522 (0.3405)	loss 1.3415 (1.1567)	grad_norm 0.3922 (nan)	loss_scale 2048.0000 (2594.7589)	mem 17013MB
[2024-07-30 15:02:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2911 (0.3395)	loss 1.1039 (1.1563)	grad_norm 0.3603 (nan)	loss_scale 2048.0000 (2572.8972)	mem 17013MB
[2024-07-30 15:02:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 22 training takes 0:14:12
[2024-07-30 15:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.420 (11.420)	Loss 0.4990 (0.4990)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 15:02:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.170 Acc@5 97.864
[2024-07-30 15:02:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-30 15:02:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-30 15:02:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 15:02:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 15:03:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:38:00 lr 0.000004	 wd 0.0500	time 10.9835 (10.9835)	loss 0.7672 (0.7672)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:03:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:17:13 lr 0.000004	 wd 0.0500	time 0.2942 (0.4301)	loss 1.4745 (1.1457)	grad_norm 0.3908 (0.4159)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:04:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:14:31 lr 0.000004	 wd 0.0500	time 0.3235 (0.3785)	loss 0.8509 (1.1503)	grad_norm 0.3947 (0.3975)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:04:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:13:22 lr 0.000004	 wd 0.0500	time 0.3457 (0.3644)	loss 1.4193 (1.1414)	grad_norm 0.3771 (0.3978)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:05:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:12:29 lr 0.000004	 wd 0.0500	time 0.3147 (0.3567)	loss 0.9017 (1.1470)	grad_norm 0.3794 (0.3957)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:05:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:44 lr 0.000004	 wd 0.0500	time 0.3310 (0.3520)	loss 0.8155 (1.1502)	grad_norm 0.3577 (nan)	loss_scale 1024.0000 (1945.8044)	mem 17013MB
[2024-07-30 15:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:11:02 lr 0.000004	 wd 0.0500	time 0.3121 (0.3482)	loss 1.4335 (1.1494)	grad_norm 0.3958 (nan)	loss_scale 1024.0000 (1792.4260)	mem 17013MB
[2024-07-30 15:06:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:10:24 lr 0.000004	 wd 0.0500	time 0.3016 (0.3465)	loss 1.1370 (1.1528)	grad_norm 0.3872 (nan)	loss_scale 1024.0000 (1682.8074)	mem 17013MB
[2024-07-30 15:07:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:47 lr 0.000003	 wd 0.0500	time 0.3652 (0.3450)	loss 1.4382 (1.1544)	grad_norm 0.3628 (nan)	loss_scale 1024.0000 (1600.5593)	mem 17013MB
[2024-07-30 15:07:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:09:10 lr 0.000003	 wd 0.0500	time 0.3552 (0.3436)	loss 1.1409 (1.1551)	grad_norm 0.3761 (nan)	loss_scale 1024.0000 (1536.5683)	mem 17013MB
[2024-07-30 15:08:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:34 lr 0.000003	 wd 0.0500	time 0.3787 (0.3426)	loss 1.1301 (1.1507)	grad_norm 0.3479 (nan)	loss_scale 1024.0000 (1485.3626)	mem 17013MB
[2024-07-30 15:09:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:59 lr 0.000003	 wd 0.0500	time 0.3054 (0.3420)	loss 1.0396 (1.1535)	grad_norm 0.3916 (nan)	loss_scale 1024.0000 (1443.4587)	mem 17013MB
[2024-07-30 15:09:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:24 lr 0.000003	 wd 0.0500	time 0.2988 (0.3416)	loss 1.4050 (1.1565)	grad_norm 0.3724 (nan)	loss_scale 1024.0000 (1408.5329)	mem 17013MB
[2024-07-30 15:10:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:50 lr 0.000003	 wd 0.0500	time 0.3292 (0.3411)	loss 1.2458 (1.1572)	grad_norm 0.3534 (nan)	loss_scale 1024.0000 (1378.9762)	mem 17013MB
[2024-07-30 15:10:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:06:15 lr 0.000003	 wd 0.0500	time 0.3056 (0.3407)	loss 1.2721 (1.1574)	grad_norm 0.3985 (nan)	loss_scale 1024.0000 (1353.6388)	mem 17013MB
[2024-07-30 15:11:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:41 lr 0.000003	 wd 0.0500	time 0.2927 (0.3403)	loss 1.2886 (1.1558)	grad_norm 0.3734 (nan)	loss_scale 1024.0000 (1331.6775)	mem 17013MB
[2024-07-30 15:11:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:05:07 lr 0.000003	 wd 0.0500	time 0.3379 (0.3410)	loss 1.1365 (1.1591)	grad_norm 0.3585 (nan)	loss_scale 1024.0000 (1312.4597)	mem 17013MB
[2024-07-30 15:12:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:34 lr 0.000003	 wd 0.0500	time 0.3134 (0.3422)	loss 0.7185 (1.1564)	grad_norm 0.3906 (nan)	loss_scale 1024.0000 (1295.5015)	mem 17013MB
[2024-07-30 15:13:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:04:00 lr 0.000003	 wd 0.0500	time 0.3748 (0.3425)	loss 1.3064 (1.1556)	grad_norm 0.4002 (nan)	loss_scale 1024.0000 (1280.4264)	mem 17013MB
[2024-07-30 15:13:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:25 lr 0.000003	 wd 0.0500	time 0.3023 (0.3420)	loss 1.2925 (1.1550)	grad_norm 0.3825 (nan)	loss_scale 1024.0000 (1266.9374)	mem 17013MB
[2024-07-30 15:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:51 lr 0.000003	 wd 0.0500	time 0.3173 (0.3421)	loss 0.9257 (1.1547)	grad_norm 0.3843 (nan)	loss_scale 1024.0000 (1254.7966)	mem 17013MB
[2024-07-30 15:14:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:17 lr 0.000003	 wd 0.0500	time 0.2978 (0.3418)	loss 1.3895 (1.1550)	grad_norm 0.3768 (nan)	loss_scale 1024.0000 (1243.8115)	mem 17013MB
[2024-07-30 15:15:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:43 lr 0.000003	 wd 0.0500	time 0.3580 (0.3415)	loss 1.3139 (1.1535)	grad_norm 0.3871 (nan)	loss_scale 1024.0000 (1233.8246)	mem 17013MB
[2024-07-30 15:15:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:08 lr 0.000003	 wd 0.0500	time 0.3096 (0.3412)	loss 1.3645 (1.1536)	grad_norm 0.3602 (nan)	loss_scale 1024.0000 (1224.7058)	mem 17013MB
[2024-07-30 15:16:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:34 lr 0.000003	 wd 0.0500	time 2.1748 (0.3418)	loss 0.8151 (1.1534)	grad_norm 0.3712 (nan)	loss_scale 1024.0000 (1216.3465)	mem 17013MB
[2024-07-30 15:17:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2917 (0.3408)	loss 0.8645 (1.1527)	grad_norm 0.3657 (nan)	loss_scale 1024.0000 (1208.6557)	mem 17013MB
[2024-07-30 15:17:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 23 training takes 0:14:16
[2024-07-30 15:17:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 10.736 (10.736)	Loss 0.5166 (0.5166)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 15:17:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.148 Acc@5 97.862
[2024-07-30 15:17:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 15:17:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-30 15:17:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:24:54 lr 0.000003	 wd 0.0500	time 12.1083 (12.1083)	loss 1.2226 (1.2226)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:18:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:17:40 lr 0.000003	 wd 0.0500	time 0.3127 (0.4414)	loss 0.9462 (1.1597)	grad_norm 0.3573 (0.3780)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:18:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:14:50 lr 0.000003	 wd 0.0500	time 0.3267 (0.3868)	loss 1.2288 (1.1492)	grad_norm 0.3546 (0.3781)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:19:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:13:33 lr 0.000003	 wd 0.0500	time 0.3014 (0.3693)	loss 1.3746 (1.1634)	grad_norm 0.3674 (0.3809)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:20:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:12:36 lr 0.000003	 wd 0.0500	time 0.2926 (0.3598)	loss 0.7237 (1.1632)	grad_norm 0.5995 (0.3831)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:20:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:11:49 lr 0.000003	 wd 0.0500	time 0.3198 (0.3543)	loss 1.4264 (1.1634)	grad_norm 0.4072 (0.3857)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:21:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:11:07 lr 0.000003	 wd 0.0500	time 0.3321 (0.3507)	loss 0.6875 (1.1584)	grad_norm 0.3778 (0.3880)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:21:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:10:26 lr 0.000003	 wd 0.0500	time 0.2988 (0.3474)	loss 1.1951 (1.1570)	grad_norm 0.3601 (0.3871)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:22:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:48 lr 0.000003	 wd 0.0500	time 0.3235 (0.3456)	loss 1.2008 (1.1591)	grad_norm 0.3788 (0.3862)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:22:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:09:11 lr 0.000003	 wd 0.0500	time 0.3372 (0.3441)	loss 0.9492 (1.1628)	grad_norm 0.3574 (0.3857)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:23:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:34 lr 0.000003	 wd 0.0500	time 0.3298 (0.3427)	loss 0.8026 (1.1598)	grad_norm 0.3770 (0.3865)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:23:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:08:03 lr 0.000003	 wd 0.0500	time 0.3246 (0.3451)	loss 0.8636 (1.1592)	grad_norm 0.3946 (0.3881)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:24:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:27 lr 0.000002	 wd 0.0500	time 0.3594 (0.3439)	loss 0.8988 (1.1572)	grad_norm 0.3753 (0.3872)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:25:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:52 lr 0.000002	 wd 0.0500	time 0.3344 (0.3431)	loss 0.7714 (1.1542)	grad_norm 0.3876 (0.3863)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:25:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:06:17 lr 0.000002	 wd 0.0500	time 0.3040 (0.3425)	loss 1.4438 (1.1553)	grad_norm 0.3774 (0.3866)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:26:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:42 lr 0.000002	 wd 0.0500	time 0.3356 (0.3414)	loss 1.2448 (1.1515)	grad_norm 0.3907 (0.3860)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:26:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:05:09 lr 0.000002	 wd 0.0500	time 0.3088 (0.3430)	loss 1.2905 (1.1504)	grad_norm 0.3778 (0.3878)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:27:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:34 lr 0.000002	 wd 0.0500	time 0.3388 (0.3425)	loss 1.5032 (1.1519)	grad_norm 0.3789 (0.3880)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:27:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:04:00 lr 0.000002	 wd 0.0500	time 0.3221 (0.3419)	loss 1.0863 (1.1518)	grad_norm 0.3757 (0.3878)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:28:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:25 lr 0.000002	 wd 0.0500	time 0.3122 (0.3417)	loss 0.9145 (1.1530)	grad_norm 0.3612 (0.3876)	loss_scale 1024.0000 (1024.0000)	mem 17013MB
[2024-07-30 15:28:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:51 lr 0.000002	 wd 0.0500	time 0.3293 (0.3411)	loss 1.2808 (1.1530)	grad_norm 0.3947 (0.3875)	loss_scale 2048.0000 (1050.6107)	mem 17013MB
[2024-07-30 15:29:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:16 lr 0.000002	 wd 0.0500	time 0.2974 (0.3407)	loss 0.9984 (1.1533)	grad_norm 0.4022 (0.3873)	loss_scale 2048.0000 (1098.0828)	mem 17013MB
[2024-07-30 15:30:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:42 lr 0.000002	 wd 0.0500	time 0.2980 (0.3404)	loss 1.0090 (1.1548)	grad_norm 0.4036 (0.3883)	loss_scale 2048.0000 (1141.2413)	mem 17013MB
[2024-07-30 15:30:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:08 lr 0.000002	 wd 0.0500	time 0.3070 (0.3398)	loss 1.3783 (1.1572)	grad_norm 0.4030 (0.3880)	loss_scale 2048.0000 (1180.6484)	mem 17013MB
[2024-07-30 15:31:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:34 lr 0.000002	 wd 0.0500	time 0.3233 (0.3395)	loss 1.2216 (1.1551)	grad_norm 0.3563 (0.3878)	loss_scale 2048.0000 (1216.7730)	mem 17013MB
[2024-07-30 15:31:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2913 (0.3384)	loss 0.9394 (1.1557)	grad_norm 0.3745 (0.3875)	loss_scale 2048.0000 (1250.0088)	mem 17013MB
[2024-07-30 15:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 24 training takes 0:14:10
[2024-07-30 15:31:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.508 (12.508)	Loss 0.4961 (0.4961)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 15:32:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.166 Acc@5 97.892
[2024-07-30 15:32:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-30 15:32:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-30 15:32:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:53:50 lr 0.000002	 wd 0.0500	time 11.3631 (11.3631)	loss 1.2668 (1.2668)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:33:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:17:48 lr 0.000002	 wd 0.0500	time 0.3289 (0.4450)	loss 1.1934 (1.2129)	grad_norm 0.3703 (0.4456)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:33:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:14:48 lr 0.000002	 wd 0.0500	time 0.3108 (0.3861)	loss 1.4334 (1.1825)	grad_norm 0.3728 (0.4184)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:34:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:13:28 lr 0.000002	 wd 0.0500	time 0.3332 (0.3670)	loss 1.1347 (1.1662)	grad_norm 0.3863 (0.4076)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:34:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:12:41 lr 0.000002	 wd 0.0500	time 0.2861 (0.3625)	loss 1.3002 (1.1671)	grad_norm 0.3564 (0.4016)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:12:05 lr 0.000002	 wd 0.0500	time 3.1958 (0.3624)	loss 1.4080 (1.1654)	grad_norm 0.3786 (0.3994)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:35:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:11:19 lr 0.000002	 wd 0.0500	time 0.3262 (0.3571)	loss 0.8284 (1.1555)	grad_norm 0.3766 (0.3952)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:36:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:10:37 lr 0.000002	 wd 0.0500	time 0.3038 (0.3539)	loss 0.8047 (1.1571)	grad_norm 0.3900 (0.3959)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:37:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:10:03 lr 0.000002	 wd 0.0500	time 0.3337 (0.3547)	loss 1.3573 (1.1561)	grad_norm 0.3995 (0.3944)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:37:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:09:25 lr 0.000002	 wd 0.0500	time 0.2989 (0.3532)	loss 1.4329 (1.1560)	grad_norm 0.4032 (0.3926)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:38:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:47 lr 0.000002	 wd 0.0500	time 0.3481 (0.3511)	loss 1.5112 (1.1574)	grad_norm 0.3965 (0.3925)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:38:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:08:09 lr 0.000002	 wd 0.0500	time 0.2958 (0.3490)	loss 1.5480 (1.1582)	grad_norm 0.4172 (0.3911)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:39:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:34 lr 0.000002	 wd 0.0500	time 0.3475 (0.3493)	loss 0.9752 (1.1547)	grad_norm 0.3608 (0.3910)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:39:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:59 lr 0.000002	 wd 0.0500	time 0.3142 (0.3488)	loss 0.8250 (1.1536)	grad_norm 0.4044 (0.3905)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:06:23 lr 0.000002	 wd 0.0500	time 0.3407 (0.3476)	loss 0.8001 (1.1531)	grad_norm 0.3897 (0.3955)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:40:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:47 lr 0.000002	 wd 0.0500	time 0.3287 (0.3469)	loss 0.8597 (1.1533)	grad_norm 0.3848 (0.3947)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:41:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:05:11 lr 0.000002	 wd 0.0500	time 0.3000 (0.3459)	loss 0.8067 (1.1524)	grad_norm 0.3732 (0.3946)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:42:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:37 lr 0.000002	 wd 0.0500	time 0.3415 (0.3455)	loss 0.7568 (1.1502)	grad_norm 0.3746 (0.3947)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:42:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:04:02 lr 0.000002	 wd 0.0500	time 0.2963 (0.3452)	loss 0.8791 (1.1521)	grad_norm 0.3691 (0.3939)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:27 lr 0.000002	 wd 0.0500	time 0.3301 (0.3449)	loss 1.6532 (1.1545)	grad_norm 0.3684 (0.3935)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:43:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:52 lr 0.000002	 wd 0.0500	time 0.3024 (0.3444)	loss 0.9908 (1.1550)	grad_norm 0.3685 (0.3930)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:44:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:18 lr 0.000002	 wd 0.0500	time 0.3113 (0.3439)	loss 1.2743 (1.1562)	grad_norm 0.3624 (0.3956)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:44:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:43 lr 0.000001	 wd 0.0500	time 0.3497 (0.3437)	loss 1.1111 (1.1565)	grad_norm 0.3749 (0.3949)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:45:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:09 lr 0.000001	 wd 0.0500	time 0.3786 (0.3433)	loss 1.3393 (1.1559)	grad_norm 0.3672 (0.3945)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:46:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3343 (0.3430)	loss 1.3509 (1.1550)	grad_norm 0.3382 (0.3939)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:46:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2925 (0.3420)	loss 0.7468 (1.1539)	grad_norm 0.4047 (0.3935)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:46:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 25 training takes 0:14:20
[2024-07-30 15:46:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.598 (12.598)	Loss 0.5220 (0.5220)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 15:47:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.132 Acc@5 97.862
[2024-07-30 15:47:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 15:47:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-30 15:47:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:15:39 lr 0.000001	 wd 0.0500	time 11.8862 (11.8862)	loss 1.1655 (1.1655)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:47:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:17:36 lr 0.000001	 wd 0.0500	time 0.3508 (0.4400)	loss 1.3046 (1.1671)	grad_norm 0.3622 (0.3798)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:48:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:14:42 lr 0.000001	 wd 0.0500	time 0.3377 (0.3834)	loss 1.0811 (1.1661)	grad_norm 0.3878 (0.3995)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:49:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:13:28 lr 0.000001	 wd 0.0500	time 0.3079 (0.3670)	loss 0.9140 (1.1482)	grad_norm 0.3635 (0.4172)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:49:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:12:32 lr 0.000001	 wd 0.0500	time 0.3249 (0.3582)	loss 1.3454 (1.1466)	grad_norm 0.4023 (0.4085)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:50:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:49 lr 0.000001	 wd 0.0500	time 0.3003 (0.3545)	loss 1.3164 (1.1555)	grad_norm 0.3771 (0.4263)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:50:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:11:07 lr 0.000001	 wd 0.0500	time 0.3295 (0.3510)	loss 0.7827 (1.1594)	grad_norm 0.3774 (0.4193)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:51:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:10:32 lr 0.000001	 wd 0.0500	time 0.3317 (0.3511)	loss 1.3967 (1.1553)	grad_norm 0.3488 (0.4149)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:52 lr 0.000001	 wd 0.0500	time 0.2901 (0.3480)	loss 1.2393 (1.1593)	grad_norm 0.3855 (0.4103)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:52:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:09:14 lr 0.000001	 wd 0.0500	time 0.3480 (0.3464)	loss 1.2736 (1.1584)	grad_norm 0.3674 (0.4101)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 15:52:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:37 lr 0.000001	 wd 0.0500	time 0.3238 (0.3448)	loss 1.1137 (1.1573)	grad_norm 0.3678 (0.4070)	loss_scale 4096.0000 (2162.5734)	mem 17013MB
[2024-07-30 15:53:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:08:01 lr 0.000001	 wd 0.0500	time 0.3009 (0.3431)	loss 0.9305 (1.1573)	grad_norm 0.3629 (0.4044)	loss_scale 4096.0000 (2338.1798)	mem 17013MB
[2024-07-30 15:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:26 lr 0.000001	 wd 0.0500	time 0.3402 (0.3427)	loss 1.0770 (1.1589)	grad_norm 0.3750 (0.4027)	loss_scale 4096.0000 (2484.5429)	mem 17013MB
[2024-07-30 15:54:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:52 lr 0.000001	 wd 0.0500	time 0.3254 (0.3429)	loss 1.2721 (1.1604)	grad_norm 0.3936 (0.4008)	loss_scale 4096.0000 (2608.4058)	mem 17013MB
[2024-07-30 15:55:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:06:16 lr 0.000001	 wd 0.0500	time 0.3085 (0.3418)	loss 1.3791 (1.1613)	grad_norm 0.4451 (0.3994)	loss_scale 4096.0000 (2714.5867)	mem 17013MB
[2024-07-30 15:55:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:42 lr 0.000001	 wd 0.0500	time 0.2988 (0.3415)	loss 1.4072 (1.1630)	grad_norm 0.3653 (0.3983)	loss_scale 4096.0000 (2806.6196)	mem 17013MB
[2024-07-30 15:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:05:07 lr 0.000001	 wd 0.0500	time 0.3104 (0.3407)	loss 1.2041 (1.1604)	grad_norm 0.3714 (0.3972)	loss_scale 4096.0000 (2887.1555)	mem 17013MB
[2024-07-30 15:56:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:32 lr 0.000001	 wd 0.0500	time 0.3214 (0.3400)	loss 1.2964 (1.1586)	grad_norm 0.3836 (0.3961)	loss_scale 4096.0000 (2958.2222)	mem 17013MB
[2024-07-30 15:57:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:58 lr 0.000001	 wd 0.0500	time 0.3312 (0.3398)	loss 0.8857 (1.1585)	grad_norm 0.3920 (0.3958)	loss_scale 4096.0000 (3021.3970)	mem 17013MB
[2024-07-30 15:57:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:24 lr 0.000001	 wd 0.0500	time 0.3033 (0.3395)	loss 1.2939 (1.1596)	grad_norm 0.3933 (0.3946)	loss_scale 4096.0000 (3077.9253)	mem 17013MB
[2024-07-30 15:58:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:50 lr 0.000001	 wd 0.0500	time 0.3256 (0.3390)	loss 1.1838 (1.1581)	grad_norm 0.3716 (0.3944)	loss_scale 4096.0000 (3128.8036)	mem 17013MB
[2024-07-30 15:59:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:16 lr 0.000001	 wd 0.0500	time 0.3113 (0.3386)	loss 1.3687 (1.1570)	grad_norm 0.3866 (0.3937)	loss_scale 4096.0000 (3174.8386)	mem 17013MB
[2024-07-30 15:59:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:42 lr 0.000001	 wd 0.0500	time 0.3252 (0.3390)	loss 1.2777 (1.1554)	grad_norm 0.3727 (0.3930)	loss_scale 4096.0000 (3216.6906)	mem 17013MB
[2024-07-30 16:00:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3238 (0.3387)	loss 1.2858 (1.1556)	grad_norm 0.3495 (0.3923)	loss_scale 4096.0000 (3254.9048)	mem 17013MB
[2024-07-30 16:00:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.2985 (0.3384)	loss 0.7132 (1.1560)	grad_norm 0.3707 (0.3917)	loss_scale 4096.0000 (3289.9359)	mem 17013MB
[2024-07-30 16:01:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2919 (0.3374)	loss 0.9378 (1.1566)	grad_norm 0.3849 (0.3914)	loss_scale 4096.0000 (3322.1655)	mem 17013MB
[2024-07-30 16:01:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 26 training takes 0:14:08
[2024-07-30 16:01:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.105 (11.105)	Loss 0.5039 (0.5039)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 16:01:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.144 Acc@5 97.870
[2024-07-30 16:01:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 16:01:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-30 16:02:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:06:56 lr 0.000001	 wd 0.0500	time 11.6774 (11.6774)	loss 1.3992 (1.3992)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:18:03 lr 0.000001	 wd 0.0500	time 0.3082 (0.4510)	loss 1.2714 (1.1475)	grad_norm 0.3438 (0.3780)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:03:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:14:53 lr 0.000001	 wd 0.0500	time 0.3081 (0.3881)	loss 1.3797 (1.1594)	grad_norm 0.3916 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:03:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:13:28 lr 0.000001	 wd 0.0500	time 0.3327 (0.3671)	loss 1.3411 (1.1588)	grad_norm 0.3741 (0.3801)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:04:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:12:31 lr 0.000001	 wd 0.0500	time 0.3157 (0.3577)	loss 0.9951 (1.1552)	grad_norm 0.3796 (0.3821)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:04:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:53 lr 0.000001	 wd 0.0500	time 0.3366 (0.3565)	loss 0.6709 (1.1564)	grad_norm 0.3880 (0.3814)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:05:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:11:12 lr 0.000001	 wd 0.0500	time 0.3242 (0.3536)	loss 1.3138 (1.1563)	grad_norm 0.4079 (0.3808)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:05:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:10:31 lr 0.000001	 wd 0.0500	time 0.3298 (0.3502)	loss 0.9821 (1.1580)	grad_norm 0.3868 (0.3907)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:06:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:51 lr 0.000001	 wd 0.0500	time 0.3358 (0.3473)	loss 0.9284 (1.1596)	grad_norm 0.3831 (0.3935)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:07:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:09:13 lr 0.000001	 wd 0.0500	time 0.3307 (0.3452)	loss 1.2348 (1.1576)	grad_norm 0.3587 (0.3920)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:07:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:36 lr 0.000001	 wd 0.0500	time 0.3404 (0.3439)	loss 1.1136 (1.1543)	grad_norm 0.3365 (0.3954)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:08:00 lr 0.000001	 wd 0.0500	time 0.3562 (0.3428)	loss 1.1306 (1.1563)	grad_norm 0.3689 (0.4002)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:08:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:25 lr 0.000001	 wd 0.0500	time 0.3406 (0.3421)	loss 0.7173 (1.1570)	grad_norm 0.3683 (0.3993)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:09:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:50 lr 0.000001	 wd 0.0500	time 0.3461 (0.3413)	loss 0.8084 (1.1538)	grad_norm 0.3807 (0.3991)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:09:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:06:15 lr 0.000001	 wd 0.0500	time 0.3123 (0.3408)	loss 0.8294 (1.1551)	grad_norm 0.3624 (0.3990)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:10:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:41 lr 0.000001	 wd 0.0500	time 0.2862 (0.3405)	loss 1.4423 (1.1561)	grad_norm 0.3602 (0.3984)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:10:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:05:06 lr 0.000001	 wd 0.0500	time 0.3066 (0.3399)	loss 1.3151 (1.1578)	grad_norm 0.3995 (0.3972)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:11:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:32 lr 0.000001	 wd 0.0500	time 0.3360 (0.3395)	loss 1.3413 (1.1595)	grad_norm 0.3807 (0.3976)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:58 lr 0.000001	 wd 0.0500	time 0.3304 (0.3394)	loss 1.3983 (1.1591)	grad_norm 0.3687 (0.3965)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:12:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:24 lr 0.000001	 wd 0.0500	time 0.3058 (0.3391)	loss 1.3712 (1.1599)	grad_norm 0.6627 (0.3972)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:13:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:50 lr 0.000001	 wd 0.0500	time 0.3123 (0.3388)	loss 1.4062 (1.1607)	grad_norm 0.3584 (0.3972)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:13:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:16 lr 0.000001	 wd 0.0500	time 0.3315 (0.3385)	loss 1.4958 (1.1597)	grad_norm 0.3844 (0.3963)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:14:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:42 lr 0.000001	 wd 0.0500	time 0.3196 (0.3386)	loss 1.1825 (1.1610)	grad_norm 0.3785 (0.3958)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:14:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3161 (0.3384)	loss 1.2906 (1.1608)	grad_norm 0.3778 (0.3951)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:15:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3548 (0.3386)	loss 1.3424 (1.1624)	grad_norm 0.4755 (0.3949)	loss_scale 4096.0000 (4096.0000)	mem 17013MB
[2024-07-30 16:15:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2917 (0.3377)	loss 1.2648 (1.1615)	grad_norm 0.3859 (0.3945)	loss_scale 8192.0000 (4190.9892)	mem 17013MB
[2024-07-30 16:16:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 27 training takes 0:14:11
[2024-07-30 16:16:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.453 (12.453)	Loss 0.4912 (0.4912)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 16:16:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.212 Acc@5 97.876
[2024-07-30 16:16:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-30 16:16:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-07-30 16:16:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 16:16:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 16:16:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][0/2502]	eta 6:58:34 lr 0.000001	 wd 0.0500	time 10.0379 (10.0379)	loss 0.9609 (0.9609)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 17013MB
[2024-07-30 16:17:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:17:04 lr 0.000000	 wd 0.0500	time 0.2899 (0.4264)	loss 0.9059 (1.1971)	grad_norm 0.3778 (0.4070)	loss_scale 8192.0000 (8192.0000)	mem 17013MB
[2024-07-30 16:17:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:14:26 lr 0.000000	 wd 0.0500	time 0.3112 (0.3766)	loss 1.5457 (1.1835)	grad_norm 0.3960 (nan)	loss_scale 4096.0000 (7825.1940)	mem 17013MB
[2024-07-30 16:18:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:13:18 lr 0.000000	 wd 0.0500	time 0.3227 (0.3626)	loss 1.4352 (1.1773)	grad_norm 0.3898 (nan)	loss_scale 4096.0000 (6586.2591)	mem 17013MB
[2024-07-30 16:18:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:12:23 lr 0.000000	 wd 0.0500	time 0.3004 (0.3536)	loss 0.7286 (1.1688)	grad_norm 0.3555 (nan)	loss_scale 4096.0000 (5965.2469)	mem 17013MB
[2024-07-30 16:19:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:39 lr 0.000000	 wd 0.0500	time 0.3086 (0.3494)	loss 0.7937 (1.1738)	grad_norm 0.3648 (nan)	loss_scale 4096.0000 (5592.1437)	mem 17013MB
[2024-07-30 16:20:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:10:59 lr 0.000000	 wd 0.0500	time 0.3499 (0.3468)	loss 0.8792 (1.1664)	grad_norm 0.3981 (nan)	loss_scale 4096.0000 (5343.2013)	mem 17013MB
[2024-07-30 16:20:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:10:20 lr 0.000000	 wd 0.0500	time 0.3030 (0.3446)	loss 1.2790 (1.1633)	grad_norm 0.3969 (nan)	loss_scale 2048.0000 (4966.6191)	mem 17013MB
[2024-07-30 16:21:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:43 lr 0.000000	 wd 0.0500	time 0.3446 (0.3427)	loss 0.8395 (1.1637)	grad_norm 0.3914 (nan)	loss_scale 2048.0000 (4602.2472)	mem 17013MB
[2024-07-30 16:21:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:09:06 lr 0.000000	 wd 0.0500	time 0.3433 (0.3413)	loss 0.8581 (1.1603)	grad_norm 0.3809 (nan)	loss_scale 2048.0000 (4318.7569)	mem 17013MB
[2024-07-30 16:22:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:32 lr 0.000000	 wd 0.0500	time 0.3098 (0.3409)	loss 1.0082 (1.1619)	grad_norm 0.3853 (nan)	loss_scale 2048.0000 (4091.9081)	mem 17013MB
[2024-07-30 16:22:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:58 lr 0.000000	 wd 0.0500	time 0.3548 (0.3411)	loss 1.2598 (1.1620)	grad_norm 0.3766 (nan)	loss_scale 2048.0000 (3906.2670)	mem 17013MB
[2024-07-30 16:23:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:22 lr 0.000000	 wd 0.0500	time 0.3109 (0.3399)	loss 1.4266 (1.1620)	grad_norm 0.4041 (nan)	loss_scale 2048.0000 (3751.5404)	mem 17013MB
[2024-07-30 16:23:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:48 lr 0.000000	 wd 0.0500	time 0.3672 (0.3396)	loss 1.5044 (1.1632)	grad_norm 0.3761 (nan)	loss_scale 2048.0000 (3620.5995)	mem 17013MB
[2024-07-30 16:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:06:13 lr 0.000000	 wd 0.0500	time 0.3197 (0.3387)	loss 0.9510 (1.1627)	grad_norm 0.3737 (nan)	loss_scale 2048.0000 (3508.3512)	mem 17013MB
[2024-07-30 16:25:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:40 lr 0.000000	 wd 0.0500	time 0.2992 (0.3395)	loss 1.1818 (1.1611)	grad_norm 0.3692 (nan)	loss_scale 2048.0000 (3411.0593)	mem 17013MB
[2024-07-30 16:25:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:05:05 lr 0.000000	 wd 0.0500	time 0.3227 (0.3391)	loss 1.3929 (1.1638)	grad_norm 0.3984 (nan)	loss_scale 2048.0000 (3325.9213)	mem 17013MB
[2024-07-30 16:26:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:31 lr 0.000000	 wd 0.0500	time 0.3172 (0.3387)	loss 1.2362 (1.1633)	grad_norm 0.3857 (nan)	loss_scale 2048.0000 (3250.7937)	mem 17013MB
[2024-07-30 16:26:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:57 lr 0.000000	 wd 0.0500	time 0.3037 (0.3385)	loss 0.7909 (1.1629)	grad_norm 0.3716 (nan)	loss_scale 2048.0000 (3184.0089)	mem 17013MB
[2024-07-30 16:27:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:23 lr 0.000000	 wd 0.0500	time 0.3109 (0.3382)	loss 1.0965 (1.1605)	grad_norm 0.3715 (nan)	loss_scale 2048.0000 (3124.2504)	mem 17013MB
[2024-07-30 16:27:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:49 lr 0.000000	 wd 0.0500	time 0.3477 (0.3381)	loss 1.1057 (1.1606)	grad_norm 0.4071 (nan)	loss_scale 2048.0000 (3070.4648)	mem 17013MB
[2024-07-30 16:28:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:15 lr 0.000000	 wd 0.0500	time 0.3040 (0.3382)	loss 0.9729 (1.1605)	grad_norm 0.3889 (nan)	loss_scale 2048.0000 (3021.7991)	mem 17013MB
[2024-07-30 16:28:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:42 lr 0.000000	 wd 0.0500	time 0.3416 (0.3381)	loss 1.2664 (1.1595)	grad_norm 0.3789 (nan)	loss_scale 2048.0000 (2977.5557)	mem 17013MB
[2024-07-30 16:29:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:08 lr 0.000000	 wd 0.0500	time 0.3653 (0.3381)	loss 1.2366 (1.1595)	grad_norm 0.6405 (nan)	loss_scale 2048.0000 (2937.1578)	mem 17013MB
[2024-07-30 16:30:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3135 (0.3379)	loss 1.5045 (1.1582)	grad_norm 0.3588 (nan)	loss_scale 2048.0000 (2900.1249)	mem 17013MB
[2024-07-30 16:30:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2903 (0.3372)	loss 1.0790 (1.1574)	grad_norm 0.3529 (nan)	loss_scale 2048.0000 (2866.0536)	mem 17013MB
[2024-07-30 16:30:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 28 training takes 0:14:08
[2024-07-30 16:30:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.288 (12.288)	Loss 0.4993 (0.4993)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17013MB
[2024-07-30 16:31:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.126 Acc@5 97.872
[2024-07-30 16:31:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-30 16:31:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-07-30 16:31:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:25:39 lr 0.000000	 wd 0.0500	time 12.1262 (12.1262)	loss 1.2359 (1.2359)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:32:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:17:46 lr 0.000000	 wd 0.0500	time 0.3495 (0.4439)	loss 1.4052 (1.1688)	grad_norm 0.3858 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:32:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:14:52 lr 0.000000	 wd 0.0500	time 0.2932 (0.3878)	loss 0.8151 (1.1671)	grad_norm 0.3968 (0.3931)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:33:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:13:32 lr 0.000000	 wd 0.0500	time 0.3079 (0.3689)	loss 0.8710 (1.1638)	grad_norm 0.3799 (0.3902)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:33:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:12:35 lr 0.000000	 wd 0.0500	time 0.2989 (0.3595)	loss 1.1185 (1.1575)	grad_norm 0.3807 (0.3871)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:34:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:11:47 lr 0.000000	 wd 0.0500	time 0.3036 (0.3533)	loss 1.3001 (1.1530)	grad_norm 0.3832 (0.3919)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:34:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:11:04 lr 0.000000	 wd 0.0500	time 0.2944 (0.3493)	loss 0.7198 (1.1561)	grad_norm 0.3798 (0.3917)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:10:24 lr 0.000000	 wd 0.0500	time 0.3059 (0.3464)	loss 1.1267 (1.1569)	grad_norm 0.3961 (0.3923)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:35:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:46 lr 0.000000	 wd 0.0500	time 0.2972 (0.3448)	loss 0.7661 (1.1577)	grad_norm 0.3672 (0.3905)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:36:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:09:10 lr 0.000000	 wd 0.0500	time 0.3042 (0.3435)	loss 1.4919 (1.1590)	grad_norm 0.3817 (0.3922)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:36:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:33 lr 0.000000	 wd 0.0500	time 0.3078 (0.3420)	loss 1.4401 (1.1598)	grad_norm 0.3849 (0.3924)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:37:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:57 lr 0.000000	 wd 0.0500	time 0.3270 (0.3409)	loss 1.2368 (1.1597)	grad_norm 0.3687 (0.3917)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:38:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:22 lr 0.000000	 wd 0.0500	time 0.3097 (0.3401)	loss 1.0199 (1.1575)	grad_norm 0.3519 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:38:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:47 lr 0.000000	 wd 0.0500	time 0.3336 (0.3393)	loss 1.1309 (1.1609)	grad_norm 0.4050 (0.3905)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:39:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:06:13 lr 0.000000	 wd 0.0500	time 0.3449 (0.3389)	loss 1.1207 (1.1559)	grad_norm 0.4067 (0.3898)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:39:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:39 lr 0.000000	 wd 0.0500	time 0.2873 (0.3387)	loss 1.0205 (1.1562)	grad_norm 0.3695 (0.3892)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:40:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:05:05 lr 0.000000	 wd 0.0500	time 0.3044 (0.3383)	loss 0.8161 (1.1582)	grad_norm 0.3646 (0.3907)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:40:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:30 lr 0.000000	 wd 0.0500	time 0.3355 (0.3379)	loss 1.3537 (1.1609)	grad_norm 0.4230 (0.3901)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:41:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:57 lr 0.000000	 wd 0.0500	time 0.3494 (0.3378)	loss 1.1687 (1.1593)	grad_norm 0.3517 (0.3916)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:41:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:23 lr 0.000000	 wd 0.0500	time 0.3113 (0.3376)	loss 0.7525 (1.1596)	grad_norm 0.3773 (0.3911)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:42:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:49 lr 0.000000	 wd 0.0500	time 0.3104 (0.3374)	loss 0.9174 (1.1613)	grad_norm 0.3751 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:43:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:15 lr 0.000000	 wd 0.0500	time 0.3236 (0.3371)	loss 0.7514 (1.1622)	grad_norm 0.3536 (0.3915)	loss_scale 2048.0000 (2048.0000)	mem 17013MB
[2024-07-30 16:43:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:41 lr 0.000000	 wd 0.0500	time 0.2910 (0.3371)	loss 0.8357 (1.1628)	grad_norm 0.4130 (0.3911)	loss_scale 4096.0000 (2113.1340)	mem 17013MB
[2024-07-30 16:44:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:08 lr 0.000000	 wd 0.0500	time 0.3715 (0.3370)	loss 0.8546 (1.1599)	grad_norm 0.4041 (0.3928)	loss_scale 4096.0000 (2199.3081)	mem 17013MB
[2024-07-30 16:44:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3096 (0.3369)	loss 1.2569 (1.1600)	grad_norm 0.4267 (0.3926)	loss_scale 4096.0000 (2278.3040)	mem 17013MB
[2024-07-30 16:45:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2958 (0.3360)	loss 1.3145 (1.1593)	grad_norm 0.3680 (0.3934)	loss_scale 4096.0000 (2350.9828)	mem 17013MB
[2024-07-30 16:45:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 249): INFO EPOCH 29 training takes 0:14:05
[2024-07-30 16:45:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_29.pth saving......
[2024-07-30 16:45:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft/diffusion_ft_adapter_smt_l_sequence_cross1/ckpt_epoch_29.pth saved !!!
[2024-07-30 16:45:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.397 (11.397)	Loss 0.4888 (0.4888)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17013MB
[2024-07-30 16:45:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 296): INFO  * Acc@1 86.162 Acc@5 97.882
[2024-07-30 16:45:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-30 16:45:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-07-30 16:45:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross1-full-ft] (main.py 189): INFO Training time 7:18:11
