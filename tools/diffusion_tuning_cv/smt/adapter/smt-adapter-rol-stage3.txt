[2024-08-02 08:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/config.json
[2024-08-02 08:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_sequence_stage3
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-02 08:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_sequence_stage3.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_sequence_stage3", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-02 08:05:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft
[2024-08-02 08:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-02 08:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 113): INFO number of params: 4162792
[2024-08-02 08:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3, ignoring auto resume
[2024-08-02 08:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth for fine-tuning......
[2024-08-02 08:05:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-02 08:05:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth'
[2024-08-02 08:05:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 15.344 (15.344)	Loss 0.5010 (0.5010)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 2339MB
[2024-08-02 08:06:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.104 Acc@5 97.868
[2024-08-02 08:06:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 08:06:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 168): INFO Start training
[2024-08-02 08:06:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:29:31 lr 0.000000	 wd 0.0500	time 12.2189 (12.2189)	loss 1.5285 (1.5285)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17019MB
[2024-08-02 08:06:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:41 lr 0.000000	 wd 0.0500	time 0.3254 (0.4421)	loss 1.2662 (1.2015)	grad_norm 0.3668 (nan)	loss_scale 16384.0000 (19790.5743)	mem 17019MB
[2024-08-02 08:07:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:44 lr 0.000000	 wd 0.0500	time 0.2991 (0.3844)	loss 1.0134 (1.1860)	grad_norm 0.3620 (nan)	loss_scale 8192.0000 (15242.8259)	mem 17019MB
[2024-08-02 08:08:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:24 lr 0.000000	 wd 0.0500	time 0.3099 (0.3654)	loss 0.8971 (1.1495)	grad_norm 0.4199 (nan)	loss_scale 8192.0000 (12900.3588)	mem 17019MB
[2024-08-02 08:08:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:12:32 lr 0.000001	 wd 0.0500	time 0.3278 (0.3578)	loss 0.9674 (1.1570)	grad_norm 0.3852 (nan)	loss_scale 8192.0000 (11726.2045)	mem 17019MB
[2024-08-02 08:09:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:47 lr 0.000001	 wd 0.0500	time 0.3614 (0.3535)	loss 1.1356 (1.1618)	grad_norm 0.3381 (nan)	loss_scale 8192.0000 (11020.7745)	mem 17019MB
[2024-08-02 08:09:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:11:05 lr 0.000001	 wd 0.0500	time 0.3340 (0.3500)	loss 1.3230 (1.1617)	grad_norm 0.3670 (nan)	loss_scale 4096.0000 (9868.5657)	mem 17019MB
[2024-08-02 08:10:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:10:24 lr 0.000001	 wd 0.0500	time 0.3068 (0.3466)	loss 1.2231 (1.1637)	grad_norm 0.4300 (nan)	loss_scale 4096.0000 (9045.0899)	mem 17019MB
[2024-08-02 08:10:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:46 lr 0.000001	 wd 0.0500	time 0.3309 (0.3444)	loss 1.0492 (1.1631)	grad_norm 0.3482 (nan)	loss_scale 4096.0000 (8427.2260)	mem 17019MB
[2024-08-02 08:11:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:09:09 lr 0.000001	 wd 0.0500	time 0.3201 (0.3429)	loss 1.3504 (1.1628)	grad_norm 0.3747 (nan)	loss_scale 4096.0000 (7946.5128)	mem 17019MB
[2024-08-02 08:11:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:33 lr 0.000002	 wd 0.0500	time 0.2990 (0.3418)	loss 1.4270 (1.1606)	grad_norm 0.3630 (nan)	loss_scale 4096.0000 (7561.8462)	mem 17019MB
[2024-08-02 08:12:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:57 lr 0.000002	 wd 0.0500	time 0.3035 (0.3409)	loss 1.2200 (1.1614)	grad_norm 0.3732 (nan)	loss_scale 4096.0000 (7247.0554)	mem 17019MB
[2024-08-02 08:12:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:21 lr 0.000002	 wd 0.0500	time 0.3605 (0.3394)	loss 0.9239 (1.1611)	grad_norm 0.3544 (nan)	loss_scale 4096.0000 (6984.6861)	mem 17019MB
[2024-08-02 08:13:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:46 lr 0.000002	 wd 0.0500	time 0.3350 (0.3383)	loss 1.2335 (1.1628)	grad_norm 0.3583 (nan)	loss_scale 4096.0000 (6762.6503)	mem 17019MB
[2024-08-02 08:14:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:12 lr 0.000002	 wd 0.0500	time 0.3034 (0.3380)	loss 1.4331 (1.1646)	grad_norm 0.3537 (nan)	loss_scale 4096.0000 (6572.3112)	mem 17019MB
[2024-08-02 08:14:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:38 lr 0.000002	 wd 0.0500	time 0.3029 (0.3378)	loss 1.1713 (1.1685)	grad_norm 0.3656 (nan)	loss_scale 4096.0000 (6407.3338)	mem 17019MB
[2024-08-02 08:15:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:05:04 lr 0.000003	 wd 0.0500	time 0.3471 (0.3371)	loss 1.2125 (1.1657)	grad_norm 0.3595 (nan)	loss_scale 4096.0000 (6262.9656)	mem 17019MB
[2024-08-02 08:15:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:29 lr 0.000003	 wd 0.0500	time 0.3067 (0.3365)	loss 0.8830 (1.1651)	grad_norm 0.3841 (nan)	loss_scale 4096.0000 (6135.5720)	mem 17019MB
[2024-08-02 08:16:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:56 lr 0.000003	 wd 0.0500	time 0.3659 (0.3363)	loss 1.1171 (1.1652)	grad_norm 0.3531 (nan)	loss_scale 4096.0000 (6022.3254)	mem 17019MB
[2024-08-02 08:16:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:22 lr 0.000003	 wd 0.0500	time 0.2832 (0.3359)	loss 1.4071 (1.1647)	grad_norm 0.3546 (nan)	loss_scale 4096.0000 (5920.9932)	mem 17019MB
[2024-08-02 08:17:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:48 lr 0.000003	 wd 0.0500	time 0.3071 (0.3359)	loss 0.8397 (1.1637)	grad_norm 0.3744 (nan)	loss_scale 4096.0000 (5829.7891)	mem 17019MB
[2024-08-02 08:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:14 lr 0.000003	 wd 0.0500	time 0.3389 (0.3354)	loss 1.0618 (1.1626)	grad_norm 0.3694 (nan)	loss_scale 4096.0000 (5747.2670)	mem 17019MB
[2024-08-02 08:18:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:41 lr 0.000004	 wd 0.0500	time 0.3230 (0.3351)	loss 1.5315 (1.1621)	grad_norm 0.3785 (nan)	loss_scale 4096.0000 (5672.2435)	mem 17019MB
[2024-08-02 08:19:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:07 lr 0.000004	 wd 0.0500	time 0.3642 (0.3349)	loss 1.3247 (1.1623)	grad_norm 0.3767 (nan)	loss_scale 4096.0000 (5603.7410)	mem 17019MB
[2024-08-02 08:19:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:34 lr 0.000004	 wd 0.0500	time 0.3235 (0.3356)	loss 1.3039 (1.1622)	grad_norm 0.3551 (nan)	loss_scale 4096.0000 (5540.9446)	mem 17019MB
[2024-08-02 08:20:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2948 (0.3351)	loss 0.8723 (1.1604)	grad_norm 0.3691 (nan)	loss_scale 4096.0000 (5483.1699)	mem 17019MB
[2024-08-02 08:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 0 training takes 0:14:00
[2024-08-02 08:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_0.pth saving......
[2024-08-02 08:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_0.pth saved !!!
[2024-08-02 08:20:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.328 (11.328)	Loss 0.5166 (0.5166)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-02 08:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.102 Acc@5 97.860
[2024-08-02 08:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 08:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 08:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 08:20:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 08:20:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:30:41 lr 0.000004	 wd 0.0500	time 10.8080 (10.8080)	loss 1.1012 (1.1012)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:21:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:17:20 lr 0.000004	 wd 0.0500	time 0.3318 (0.4334)	loss 1.1644 (1.1418)	grad_norm 0.3628 (0.3732)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:21:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:14:33 lr 0.000004	 wd 0.0500	time 0.3055 (0.3796)	loss 0.7627 (1.1593)	grad_norm 0.3632 (0.3770)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:22:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:13:42 lr 0.000004	 wd 0.0500	time 0.3448 (0.3733)	loss 0.8743 (1.1589)	grad_norm 0.3632 (0.3731)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:51 lr 0.000005	 wd 0.0500	time 0.3194 (0.3668)	loss 0.9698 (1.1740)	grad_norm 0.3732 (0.3740)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:23:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:12:01 lr 0.000005	 wd 0.0500	time 0.3376 (0.3602)	loss 1.4443 (1.1713)	grad_norm 0.3599 (0.3759)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:24:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:11:16 lr 0.000005	 wd 0.0500	time 0.3439 (0.3554)	loss 1.4769 (1.1702)	grad_norm 0.3507 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:24:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:10:33 lr 0.000005	 wd 0.0500	time 0.3055 (0.3513)	loss 0.8659 (1.1671)	grad_norm 0.3496 (0.3770)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:25:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:53 lr 0.000005	 wd 0.0500	time 0.3735 (0.3487)	loss 1.4063 (1.1662)	grad_norm 0.3402 (0.3758)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:25:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:09:17 lr 0.000005	 wd 0.0500	time 0.3549 (0.3479)	loss 1.1673 (1.1651)	grad_norm 0.3834 (0.3829)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:26:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:40 lr 0.000006	 wd 0.0500	time 0.3122 (0.3462)	loss 1.3577 (1.1638)	grad_norm 0.3920 (0.3834)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:26:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:08:03 lr 0.000006	 wd 0.0500	time 0.3049 (0.3446)	loss 1.4276 (1.1623)	grad_norm 0.3585 (0.3828)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:27:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:26 lr 0.000006	 wd 0.0500	time 0.2978 (0.3429)	loss 0.9598 (1.1602)	grad_norm 0.3515 (0.3813)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:28:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:50 lr 0.000006	 wd 0.0500	time 0.3127 (0.3416)	loss 1.5314 (1.1613)	grad_norm 0.3740 (0.3822)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:28:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:06:16 lr 0.000006	 wd 0.0500	time 0.3101 (0.3414)	loss 0.9357 (1.1613)	grad_norm 0.3680 (0.3810)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:29:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:41 lr 0.000006	 wd 0.0500	time 0.3368 (0.3408)	loss 1.3912 (1.1620)	grad_norm 0.3548 (0.3802)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:29:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:05:06 lr 0.000007	 wd 0.0500	time 0.3092 (0.3399)	loss 1.2813 (1.1637)	grad_norm 0.3616 (0.3801)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:30:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:32 lr 0.000007	 wd 0.0500	time 0.2952 (0.3397)	loss 0.9865 (1.1640)	grad_norm 0.3745 (0.3795)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:30:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:58 lr 0.000007	 wd 0.0500	time 0.3262 (0.3391)	loss 1.6925 (1.1633)	grad_norm 0.3636 (0.3790)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:31:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:24 lr 0.000007	 wd 0.0500	time 0.3405 (0.3393)	loss 1.4420 (1.1645)	grad_norm 0.3848 (0.3787)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:31:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:50 lr 0.000007	 wd 0.0500	time 0.3248 (0.3395)	loss 1.4039 (1.1650)	grad_norm 0.4119 (0.3783)	loss_scale 8192.0000 (4100.0940)	mem 17019MB
[2024-08-02 08:32:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:16 lr 0.000007	 wd 0.0500	time 0.3372 (0.3392)	loss 1.4812 (1.1671)	grad_norm 0.3858 (0.3787)	loss_scale 8192.0000 (4294.8539)	mem 17019MB
[2024-08-02 08:33:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.3451 (0.3391)	loss 1.2238 (1.1676)	grad_norm 0.3990 (0.3786)	loss_scale 8192.0000 (4471.9164)	mem 17019MB
[2024-08-02 08:33:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.3181 (0.3389)	loss 0.9609 (1.1698)	grad_norm 0.3634 (0.3786)	loss_scale 8192.0000 (4633.5889)	mem 17019MB
[2024-08-02 08:34:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.3570 (0.3391)	loss 1.2032 (1.1699)	grad_norm 0.3856 (0.3785)	loss_scale 8192.0000 (4781.7943)	mem 17019MB
[2024-08-02 08:34:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.3124 (0.3383)	loss 0.9053 (1.1700)	grad_norm 0.3647 (nan)	loss_scale 4096.0000 (4836.2607)	mem 17019MB
[2024-08-02 08:34:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:14:08
[2024-08-02 08:34:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.756 (11.756)	Loss 0.5107 (0.5107)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 08:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.126 Acc@5 97.842
[2024-08-02 08:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 08:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-02 08:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 08:35:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 08:35:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:46:34 lr 0.000008	 wd 0.0500	time 11.1888 (11.1888)	loss 1.2366 (1.2366)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:36:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:17:28 lr 0.000008	 wd 0.0500	time 0.3433 (0.4364)	loss 1.4148 (1.1725)	grad_norm 0.3748 (0.3808)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:36:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:39 lr 0.000008	 wd 0.0500	time 0.3063 (0.3821)	loss 1.3876 (1.1639)	grad_norm 0.3602 (0.3742)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:37:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:13:27 lr 0.000008	 wd 0.0500	time 0.2955 (0.3666)	loss 1.1807 (1.1547)	grad_norm 0.3617 (0.3718)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:37:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:12:30 lr 0.000009	 wd 0.0500	time 0.3354 (0.3571)	loss 1.4212 (1.1579)	grad_norm 0.3520 (0.3730)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:38:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:11:46 lr 0.000009	 wd 0.0500	time 0.3015 (0.3527)	loss 1.4282 (1.1536)	grad_norm 0.3855 (0.3724)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:38:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:11:03 lr 0.000009	 wd 0.0500	time 0.2993 (0.3487)	loss 1.2729 (1.1549)	grad_norm 0.3565 (0.3722)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:39:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:10:24 lr 0.000009	 wd 0.0500	time 0.3132 (0.3467)	loss 1.3276 (1.1598)	grad_norm 0.3701 (0.3725)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:39:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:46 lr 0.000009	 wd 0.0500	time 0.3444 (0.3447)	loss 1.3776 (1.1627)	grad_norm 0.3553 (0.3718)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:40:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:09:09 lr 0.000009	 wd 0.0500	time 0.3223 (0.3433)	loss 0.9376 (1.1677)	grad_norm 0.3743 (0.3715)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:40:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:08:33 lr 0.000010	 wd 0.0500	time 0.3330 (0.3416)	loss 0.8646 (1.1649)	grad_norm 0.3777 (0.3705)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:58 lr 0.000010	 wd 0.0500	time 0.3443 (0.3412)	loss 1.3753 (1.1625)	grad_norm 0.4335 (0.3742)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:42:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:07:23 lr 0.000010	 wd 0.0500	time 0.3062 (0.3409)	loss 0.9006 (1.1627)	grad_norm 0.4035 (0.3740)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:42:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:48 lr 0.000010	 wd 0.0500	time 0.3310 (0.3398)	loss 1.3989 (1.1638)	grad_norm 0.3740 (0.3739)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 08:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:06:14 lr 0.000010	 wd 0.0500	time 0.2934 (0.3397)	loss 1.2187 (1.1654)	grad_norm 0.3612 (nan)	loss_scale 2048.0000 (4037.5275)	mem 17019MB
[2024-08-02 08:43:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:39 lr 0.000010	 wd 0.0500	time 0.3145 (0.3389)	loss 1.3188 (1.1658)	grad_norm 0.3954 (nan)	loss_scale 2048.0000 (3904.9807)	mem 17019MB
[2024-08-02 08:44:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:05:05 lr 0.000011	 wd 0.0500	time 0.3583 (0.3388)	loss 0.9698 (1.1664)	grad_norm 0.3519 (nan)	loss_scale 2048.0000 (3788.9919)	mem 17019MB
[2024-08-02 08:44:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:31 lr 0.000011	 wd 0.0500	time 0.3367 (0.3386)	loss 1.2818 (1.1668)	grad_norm 0.3616 (nan)	loss_scale 2048.0000 (3686.6408)	mem 17019MB
[2024-08-02 08:45:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:57 lr 0.000011	 wd 0.0500	time 0.2877 (0.3381)	loss 1.0659 (1.1673)	grad_norm 0.3655 (nan)	loss_scale 2048.0000 (3595.6557)	mem 17019MB
[2024-08-02 08:45:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:23 lr 0.000011	 wd 0.0500	time 0.3148 (0.3380)	loss 1.3500 (1.1670)	grad_norm 0.3927 (nan)	loss_scale 2048.0000 (3514.2430)	mem 17019MB
[2024-08-02 08:46:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:49 lr 0.000011	 wd 0.0500	time 0.3049 (0.3378)	loss 1.1336 (1.1671)	grad_norm 0.3607 (nan)	loss_scale 2048.0000 (3440.9675)	mem 17019MB
[2024-08-02 08:47:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:15 lr 0.000011	 wd 0.0500	time 0.3046 (0.3374)	loss 1.0023 (1.1655)	grad_norm 0.3551 (nan)	loss_scale 2048.0000 (3374.6673)	mem 17019MB
[2024-08-02 08:47:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:41 lr 0.000012	 wd 0.0500	time 0.3137 (0.3375)	loss 1.3859 (1.1653)	grad_norm 0.3773 (nan)	loss_scale 2048.0000 (3314.3916)	mem 17019MB
[2024-08-02 08:48:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:08 lr 0.000012	 wd 0.0500	time 0.3047 (0.3372)	loss 1.2063 (1.1655)	grad_norm 0.3609 (nan)	loss_scale 2048.0000 (3259.3551)	mem 17019MB
[2024-08-02 08:48:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:34 lr 0.000012	 wd 0.0500	time 0.3138 (0.3369)	loss 0.7778 (1.1662)	grad_norm 0.3746 (nan)	loss_scale 2048.0000 (3208.9030)	mem 17019MB
[2024-08-02 08:49:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2944 (0.3361)	loss 1.1880 (1.1666)	grad_norm 0.3576 (nan)	loss_scale 2048.0000 (3162.4854)	mem 17019MB
[2024-08-02 08:49:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:14:03
[2024-08-02 08:49:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.424 (11.424)	Loss 0.4934 (0.4934)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 08:49:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.094 Acc@5 97.874
[2024-08-02 08:49:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 08:49:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-02 08:50:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:16:29 lr 0.000012	 wd 0.0500	time 11.9064 (11.9064)	loss 0.7514 (0.7514)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:50:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:17:37 lr 0.000012	 wd 0.0500	time 0.3173 (0.4404)	loss 1.2352 (1.2026)	grad_norm 0.3644 (0.3872)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:51:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:14:51 lr 0.000012	 wd 0.0500	time 0.3123 (0.3872)	loss 1.5083 (1.1766)	grad_norm 0.3754 (0.4381)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:51:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:13:28 lr 0.000012	 wd 0.0500	time 0.3290 (0.3672)	loss 1.4336 (1.1736)	grad_norm 0.3545 (0.4157)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:52:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:12:29 lr 0.000013	 wd 0.0500	time 0.3105 (0.3567)	loss 1.4101 (1.1657)	grad_norm 0.3630 (0.4036)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:52:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:44 lr 0.000013	 wd 0.0500	time 0.3663 (0.3518)	loss 0.9999 (1.1659)	grad_norm 0.3615 (0.3973)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:53:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:11:01 lr 0.000013	 wd 0.0500	time 0.3041 (0.3476)	loss 1.0591 (1.1608)	grad_norm 0.3787 (0.3949)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:53:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:10:20 lr 0.000013	 wd 0.0500	time 0.3044 (0.3444)	loss 1.5043 (1.1585)	grad_norm 0.3956 (0.3933)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:54:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:43 lr 0.000013	 wd 0.0500	time 0.3447 (0.3426)	loss 0.7721 (1.1543)	grad_norm 0.4028 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:54:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:09:08 lr 0.000013	 wd 0.0500	time 0.3241 (0.3425)	loss 1.5568 (1.1565)	grad_norm 0.3716 (0.3883)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:55:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:32 lr 0.000014	 wd 0.0500	time 0.3028 (0.3414)	loss 1.2902 (1.1571)	grad_norm 0.3359 (0.3876)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:56:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:57 lr 0.000014	 wd 0.0500	time 0.3292 (0.3404)	loss 0.9947 (1.1578)	grad_norm 0.3862 (0.3861)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:56:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:07:22 lr 0.000014	 wd 0.0500	time 0.3381 (0.3400)	loss 1.1508 (1.1575)	grad_norm 0.3645 (0.3852)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:57:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:47 lr 0.000014	 wd 0.0500	time 0.3301 (0.3391)	loss 1.3134 (1.1568)	grad_norm 0.3484 (0.3852)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:57:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:06:12 lr 0.000014	 wd 0.0500	time 0.2941 (0.3383)	loss 1.1251 (1.1578)	grad_norm 0.3476 (0.3842)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:58:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:38 lr 0.000014	 wd 0.0500	time 0.3040 (0.3379)	loss 1.3902 (1.1563)	grad_norm 0.3674 (0.3836)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:58:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:05:04 lr 0.000015	 wd 0.0500	time 0.3039 (0.3379)	loss 0.7651 (1.1548)	grad_norm 0.3632 (0.3833)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:59:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:30 lr 0.000015	 wd 0.0500	time 0.3333 (0.3374)	loss 0.9810 (1.1571)	grad_norm 0.4640 (0.3852)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 08:59:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:56 lr 0.000015	 wd 0.0500	time 0.3352 (0.3370)	loss 1.1419 (1.1583)	grad_norm 0.3767 (0.3845)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:00:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:22 lr 0.000015	 wd 0.0500	time 0.3078 (0.3370)	loss 1.2395 (1.1574)	grad_norm 0.3580 (0.3846)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:01:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:49 lr 0.000015	 wd 0.0500	time 0.3078 (0.3369)	loss 1.3155 (1.1579)	grad_norm 0.3625 (0.3847)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:01:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:15 lr 0.000015	 wd 0.0500	time 0.3315 (0.3382)	loss 0.9689 (1.1583)	grad_norm 0.3747 (0.3840)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:02:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:42 lr 0.000016	 wd 0.0500	time 0.3311 (0.3380)	loss 0.8015 (1.1575)	grad_norm 0.3561 (0.3835)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:02:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:08 lr 0.000016	 wd 0.0500	time 0.3410 (0.3383)	loss 0.8994 (1.1576)	grad_norm 0.4030 (0.3831)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:03:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:34 lr 0.000016	 wd 0.0500	time 0.3214 (0.3381)	loss 1.4012 (1.1593)	grad_norm 0.3788 (0.3826)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:03:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2954 (0.3372)	loss 1.2962 (1.1591)	grad_norm 0.3752 (0.3826)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:03:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:14:06
[2024-08-02 09:04:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.907 (11.907)	Loss 0.5107 (0.5107)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 09:04:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.124 Acc@5 97.854
[2024-08-02 09:04:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 09:04:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.13%
[2024-08-02 09:04:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 8:14:20 lr 0.000016	 wd 0.0500	time 11.8546 (11.8546)	loss 1.2650 (1.2650)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:05:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:17:28 lr 0.000016	 wd 0.0500	time 0.3573 (0.4365)	loss 0.9391 (1.1740)	grad_norm 0.3685 (0.3767)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:05:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:14:28 lr 0.000016	 wd 0.0500	time 0.3160 (0.3772)	loss 0.8648 (1.1614)	grad_norm 0.3724 (0.3768)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:06:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:13:12 lr 0.000016	 wd 0.0500	time 0.3181 (0.3599)	loss 0.7941 (1.1586)	grad_norm 0.3710 (0.3765)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 09:06:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:12:22 lr 0.000017	 wd 0.0500	time 0.3347 (0.3534)	loss 1.2951 (1.1533)	grad_norm 0.3661 (0.3765)	loss_scale 4096.0000 (2272.7182)	mem 17019MB
[2024-08-02 09:07:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:11:35 lr 0.000017	 wd 0.0500	time 0.3056 (0.3476)	loss 1.2476 (1.1541)	grad_norm 0.3576 (0.3758)	loss_scale 4096.0000 (2636.6467)	mem 17019MB
[2024-08-02 09:07:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:54 lr 0.000017	 wd 0.0500	time 0.3096 (0.3441)	loss 1.1746 (1.1588)	grad_norm 0.3802 (0.3761)	loss_scale 4096.0000 (2879.4676)	mem 17019MB
[2024-08-02 09:08:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:10:15 lr 0.000017	 wd 0.0500	time 0.3069 (0.3414)	loss 0.8322 (1.1591)	grad_norm 0.3703 (0.3750)	loss_scale 4096.0000 (3053.0100)	mem 17019MB
[2024-08-02 09:08:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:40 lr 0.000017	 wd 0.0500	time 0.2844 (0.3408)	loss 0.7745 (1.1617)	grad_norm 0.3784 (0.3750)	loss_scale 4096.0000 (3183.2210)	mem 17019MB
[2024-08-02 09:09:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:09:03 lr 0.000017	 wd 0.0500	time 0.3459 (0.3394)	loss 0.8278 (1.1628)	grad_norm 0.3636 (0.3747)	loss_scale 4096.0000 (3284.5283)	mem 17019MB
[2024-08-02 09:10:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:08:27 lr 0.000018	 wd 0.0500	time 0.3367 (0.3379)	loss 1.5544 (1.1628)	grad_norm 0.3512 (0.3766)	loss_scale 4096.0000 (3365.5944)	mem 17019MB
[2024-08-02 09:10:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:52 lr 0.000018	 wd 0.0500	time 0.2962 (0.3370)	loss 1.4481 (1.1637)	grad_norm 0.3660 (0.3765)	loss_scale 4096.0000 (3431.9346)	mem 17019MB
[2024-08-02 09:11:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:18 lr 0.000018	 wd 0.0500	time 0.3160 (0.3365)	loss 1.2744 (1.1626)	grad_norm 0.3695 (0.3760)	loss_scale 4096.0000 (3487.2273)	mem 17019MB
[2024-08-02 09:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:43 lr 0.000018	 wd 0.0500	time 0.3398 (0.3361)	loss 1.0232 (1.1633)	grad_norm 0.3811 (0.3754)	loss_scale 4096.0000 (3534.0200)	mem 17019MB
[2024-08-02 09:12:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:06:10 lr 0.000018	 wd 0.0500	time 0.3203 (0.3359)	loss 1.3989 (1.1632)	grad_norm 0.4003 (0.3757)	loss_scale 4096.0000 (3574.1328)	mem 17019MB
[2024-08-02 09:12:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:36 lr 0.000018	 wd 0.0500	time 0.3327 (0.3359)	loss 1.2135 (1.1636)	grad_norm 0.3608 (0.3756)	loss_scale 4096.0000 (3608.9007)	mem 17019MB
[2024-08-02 09:13:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:05:02 lr 0.000019	 wd 0.0500	time 0.3345 (0.3356)	loss 1.4486 (1.1622)	grad_norm 0.3733 (0.3754)	loss_scale 4096.0000 (3639.3254)	mem 17019MB
[2024-08-02 09:13:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:28 lr 0.000019	 wd 0.0500	time 0.3302 (0.3354)	loss 1.1662 (1.1628)	grad_norm 0.3789 (0.3753)	loss_scale 4096.0000 (3666.1728)	mem 17019MB
[2024-08-02 09:14:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:55 lr 0.000019	 wd 0.0500	time 0.3188 (0.3351)	loss 1.5053 (1.1648)	grad_norm 0.3718 (0.3753)	loss_scale 4096.0000 (3690.0389)	mem 17019MB
[2024-08-02 09:14:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:21 lr 0.000019	 wd 0.0500	time 0.3321 (0.3351)	loss 1.3779 (1.1652)	grad_norm 0.3795 (0.3754)	loss_scale 4096.0000 (3711.3940)	mem 17019MB
[2024-08-02 09:15:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:48 lr 0.000019	 wd 0.0500	time 0.3457 (0.3350)	loss 0.8648 (1.1630)	grad_norm 0.3745 (0.3752)	loss_scale 4096.0000 (3730.6147)	mem 17019MB
[2024-08-02 09:16:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:14 lr 0.000019	 wd 0.0500	time 0.2997 (0.3350)	loss 1.0007 (1.1617)	grad_norm 0.3661 (0.3758)	loss_scale 4096.0000 (3748.0057)	mem 17019MB
[2024-08-02 09:16:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:41 lr 0.000020	 wd 0.0500	time 0.3470 (0.3351)	loss 0.8819 (1.1613)	grad_norm 0.3460 (0.3760)	loss_scale 4096.0000 (3763.8164)	mem 17019MB
[2024-08-02 09:17:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:07 lr 0.000020	 wd 0.0500	time 0.3466 (0.3350)	loss 0.7896 (1.1607)	grad_norm 0.3864 (0.3760)	loss_scale 4096.0000 (3778.2529)	mem 17019MB
[2024-08-02 09:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3456 (0.3348)	loss 0.7743 (1.1606)	grad_norm 0.3666 (0.3759)	loss_scale 4096.0000 (3791.4869)	mem 17019MB
[2024-08-02 09:18:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2941 (0.3343)	loss 1.2333 (1.1612)	grad_norm 0.3802 (0.3759)	loss_scale 4096.0000 (3803.6625)	mem 17019MB
[2024-08-02 09:18:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:13:58
[2024-08-02 09:18:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.901 (11.901)	Loss 0.4985 (0.4985)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-02 09:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.152 Acc@5 97.876
[2024-08-02 09:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 09:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-08-02 09:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 09:18:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 09:19:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:27:56 lr 0.000020	 wd 0.0500	time 10.7421 (10.7421)	loss 1.3820 (1.3820)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:19:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:17:21 lr 0.000020	 wd 0.0500	time 0.3016 (0.4335)	loss 0.9852 (1.1899)	grad_norm 0.3853 (0.3735)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:20:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:14:36 lr 0.000020	 wd 0.0500	time 0.2962 (0.3809)	loss 1.3303 (1.1614)	grad_norm 0.3866 (0.3820)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:20:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:13:20 lr 0.000020	 wd 0.0500	time 0.2923 (0.3634)	loss 1.0096 (1.1507)	grad_norm 0.3654 (0.3813)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:21:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:12:25 lr 0.000020	 wd 0.0500	time 0.3067 (0.3544)	loss 1.2576 (1.1595)	grad_norm 0.3764 (0.3804)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:21:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:39 lr 0.000020	 wd 0.0500	time 0.3185 (0.3496)	loss 0.9990 (1.1583)	grad_norm 0.3720 (0.3823)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:22:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:59 lr 0.000020	 wd 0.0500	time 0.3036 (0.3467)	loss 1.0599 (1.1595)	grad_norm 0.3784 (0.3810)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:22:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:10:21 lr 0.000020	 wd 0.0500	time 0.3554 (0.3448)	loss 0.9227 (1.1582)	grad_norm 0.3507 (0.3818)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:23:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:44 lr 0.000020	 wd 0.0500	time 0.3558 (0.3434)	loss 1.5097 (1.1572)	grad_norm 0.5936 (0.3869)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:23:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:09:08 lr 0.000020	 wd 0.0500	time 0.3600 (0.3421)	loss 1.0605 (1.1560)	grad_norm 0.3633 (0.3867)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:24:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:33 lr 0.000020	 wd 0.0500	time 0.3406 (0.3416)	loss 1.2528 (1.1543)	grad_norm 0.3474 (0.3847)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:25:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:57 lr 0.000020	 wd 0.0500	time 0.3064 (0.3403)	loss 1.1740 (1.1553)	grad_norm 0.3788 (0.3835)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:25:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:07:22 lr 0.000020	 wd 0.0500	time 0.3082 (0.3396)	loss 1.4061 (1.1539)	grad_norm 0.4044 (0.3824)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:26:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:47 lr 0.000020	 wd 0.0500	time 0.2945 (0.3390)	loss 0.9970 (1.1547)	grad_norm 0.8020 (0.3825)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:26:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:06:13 lr 0.000020	 wd 0.0500	time 0.3373 (0.3389)	loss 1.4254 (1.1583)	grad_norm 0.4411 (0.3820)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:27:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:39 lr 0.000020	 wd 0.0500	time 0.3408 (0.3389)	loss 0.8358 (1.1594)	grad_norm 0.4339 (0.3818)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:27:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:05:05 lr 0.000020	 wd 0.0500	time 0.3081 (0.3384)	loss 1.4016 (1.1620)	grad_norm 0.3768 (0.3818)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:28:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:31 lr 0.000020	 wd 0.0500	time 0.3433 (0.3382)	loss 0.9548 (1.1622)	grad_norm 0.3539 (0.3813)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:28:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:57 lr 0.000020	 wd 0.0500	time 0.3497 (0.3379)	loss 0.7682 (1.1601)	grad_norm 0.3779 (0.3809)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:29:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:23 lr 0.000020	 wd 0.0500	time 0.3213 (0.3376)	loss 1.1320 (1.1596)	grad_norm 0.5184 (0.3816)	loss_scale 8192.0000 (4195.1142)	mem 17019MB
[2024-08-02 09:30:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.3505 (0.3375)	loss 1.5596 (1.1611)	grad_norm 0.3541 (0.3816)	loss_scale 8192.0000 (4394.8586)	mem 17019MB
[2024-08-02 09:30:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:15 lr 0.000020	 wd 0.0500	time 0.3514 (0.3373)	loss 1.4711 (1.1606)	grad_norm 0.3867 (0.3817)	loss_scale 8192.0000 (4575.5888)	mem 17019MB
[2024-08-02 09:31:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:41 lr 0.000020	 wd 0.0500	time 0.3255 (0.3371)	loss 1.2119 (1.1618)	grad_norm 0.3612 (0.3812)	loss_scale 8192.0000 (4739.8964)	mem 17019MB
[2024-08-02 09:31:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3225 (0.3370)	loss 1.1275 (1.1603)	grad_norm 0.3790 (nan)	loss_scale 4096.0000 (4743.9548)	mem 17019MB
[2024-08-02 09:32:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3117 (0.3369)	loss 0.7849 (1.1598)	grad_norm 0.3807 (nan)	loss_scale 4096.0000 (4716.9679)	mem 17019MB
[2024-08-02 09:32:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2951 (0.3363)	loss 1.2927 (1.1610)	grad_norm 0.3643 (nan)	loss_scale 4096.0000 (4692.1391)	mem 17019MB
[2024-08-02 09:32:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:14:03
[2024-08-02 09:33:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.881 (11.881)	Loss 0.4944 (0.4944)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-02 09:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.152 Acc@5 97.882
[2024-08-02 09:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 09:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.15%
[2024-08-02 09:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 09:33:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 09:33:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:53:18 lr 0.000020	 wd 0.0500	time 11.3502 (11.3502)	loss 1.2209 (1.2209)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:34:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:17:27 lr 0.000020	 wd 0.0500	time 0.2857 (0.4361)	loss 0.8938 (1.1833)	grad_norm 0.3796 (0.3792)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:34:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:14:40 lr 0.000020	 wd 0.0500	time 0.3532 (0.3823)	loss 0.8565 (1.1731)	grad_norm 0.3939 (0.3786)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:35:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:13:23 lr 0.000020	 wd 0.0500	time 0.3807 (0.3648)	loss 0.9465 (1.1647)	grad_norm 0.3810 (0.3787)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:35:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:37 lr 0.000020	 wd 0.0500	time 0.3369 (0.3601)	loss 0.7796 (1.1665)	grad_norm 0.3911 (0.3791)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:36:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:48 lr 0.000020	 wd 0.0500	time 0.3222 (0.3539)	loss 1.1930 (1.1668)	grad_norm 0.3582 (0.3784)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:36:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:11:03 lr 0.000020	 wd 0.0500	time 0.3126 (0.3488)	loss 0.8533 (1.1582)	grad_norm 0.3807 (0.3801)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:37:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:10:25 lr 0.000020	 wd 0.0500	time 0.3083 (0.3469)	loss 1.4763 (1.1607)	grad_norm 0.3397 (0.3781)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:37:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:46 lr 0.000020	 wd 0.0500	time 0.2976 (0.3445)	loss 0.7033 (1.1651)	grad_norm 0.3494 (0.3798)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:38:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:09:09 lr 0.000020	 wd 0.0500	time 0.3380 (0.3429)	loss 1.4816 (1.1616)	grad_norm 0.3474 (0.3789)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:39:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:33 lr 0.000020	 wd 0.0500	time 0.3335 (0.3418)	loss 1.0356 (1.1614)	grad_norm 0.3558 (0.3800)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:59 lr 0.000020	 wd 0.0500	time 0.3090 (0.3422)	loss 0.8578 (1.1580)	grad_norm 0.3690 (0.3813)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:40:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:24 lr 0.000020	 wd 0.0500	time 0.3053 (0.3413)	loss 1.2288 (1.1551)	grad_norm 0.3783 (0.3806)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:40:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:49 lr 0.000020	 wd 0.0500	time 0.3071 (0.3404)	loss 1.0952 (1.1539)	grad_norm 0.3632 (0.3820)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:41:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:06:14 lr 0.000020	 wd 0.0500	time 0.3061 (0.3398)	loss 0.9404 (1.1540)	grad_norm 0.3680 (0.3814)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:41:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:40 lr 0.000020	 wd 0.0500	time 0.3236 (0.3395)	loss 0.7341 (1.1550)	grad_norm 0.3598 (0.3807)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:42:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:05:05 lr 0.000020	 wd 0.0500	time 0.3076 (0.3390)	loss 1.4516 (1.1525)	grad_norm 0.3579 (0.3809)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:43:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:31 lr 0.000020	 wd 0.0500	time 0.3000 (0.3387)	loss 1.3779 (1.1519)	grad_norm 0.3590 (0.3805)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:57 lr 0.000020	 wd 0.0500	time 0.3531 (0.3385)	loss 1.2009 (1.1524)	grad_norm 0.3598 (0.3816)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:44:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:23 lr 0.000020	 wd 0.0500	time 0.2869 (0.3384)	loss 1.3361 (1.1535)	grad_norm 0.3614 (0.3820)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:44:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.3042 (0.3383)	loss 0.9784 (1.1543)	grad_norm 0.3684 (0.3817)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:45:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:15 lr 0.000020	 wd 0.0500	time 0.3182 (0.3381)	loss 1.0050 (1.1545)	grad_norm 0.3715 (0.3812)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:45:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:42 lr 0.000020	 wd 0.0500	time 0.3197 (0.3381)	loss 0.8097 (1.1538)	grad_norm 0.3977 (0.3815)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:46:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:08 lr 0.000020	 wd 0.0500	time 0.3259 (0.3381)	loss 1.0339 (1.1563)	grad_norm 0.3827 (0.3824)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:46:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.3072 (0.3380)	loss 1.3821 (1.1556)	grad_norm 0.3882 (0.3825)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:47:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2953 (0.3373)	loss 1.3384 (1.1552)	grad_norm 0.3622 (0.3824)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:47:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:14:06
[2024-08-02 09:47:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.271 (12.271)	Loss 0.5068 (0.5068)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 09:47:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.174 Acc@5 97.878
[2024-08-02 09:47:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 09:47:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-08-02 09:47:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 09:47:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 09:48:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:45:55 lr 0.000020	 wd 0.0500	time 11.1733 (11.1733)	loss 0.9164 (0.9164)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:48:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:17:27 lr 0.000020	 wd 0.0500	time 0.3039 (0.4361)	loss 1.1929 (1.1407)	grad_norm 0.3650 (0.3743)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:49:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:39 lr 0.000020	 wd 0.0500	time 0.3237 (0.3822)	loss 1.1029 (1.1619)	grad_norm 0.3719 (0.3806)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:49:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:21 lr 0.000020	 wd 0.0500	time 0.3486 (0.3639)	loss 1.2843 (1.1566)	grad_norm 0.3699 (0.3849)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:50:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:25 lr 0.000020	 wd 0.0500	time 0.2985 (0.3548)	loss 0.8191 (1.1618)	grad_norm 0.3807 (0.3908)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:50:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:41 lr 0.000020	 wd 0.0500	time 0.3784 (0.3506)	loss 1.3303 (1.1586)	grad_norm 0.3599 (0.3906)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:51:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:11:00 lr 0.000020	 wd 0.0500	time 0.3583 (0.3473)	loss 1.2933 (1.1628)	grad_norm 0.4884 (0.3890)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:52:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:10:22 lr 0.000020	 wd 0.0500	time 0.3710 (0.3453)	loss 0.9659 (1.1574)	grad_norm 0.3794 (0.3880)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:52:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:45 lr 0.000020	 wd 0.0500	time 0.3443 (0.3438)	loss 0.8903 (1.1550)	grad_norm 0.3704 (0.3866)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:53:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:09:08 lr 0.000020	 wd 0.0500	time 0.3474 (0.3426)	loss 1.4233 (1.1566)	grad_norm 0.3670 (0.3873)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:53:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:33 lr 0.000020	 wd 0.0500	time 0.3274 (0.3416)	loss 0.8065 (1.1566)	grad_norm 0.3859 (0.3874)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:54:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:57 lr 0.000020	 wd 0.0500	time 0.3223 (0.3409)	loss 1.3530 (1.1559)	grad_norm 0.4335 (0.3870)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:54:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:23 lr 0.000020	 wd 0.0500	time 0.3418 (0.3407)	loss 1.3746 (1.1527)	grad_norm 0.3820 (0.3860)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 09:55:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:49 lr 0.000020	 wd 0.0500	time 0.3454 (0.3405)	loss 1.3611 (1.1538)	grad_norm 0.3456 (0.3858)	loss_scale 8192.0000 (4366.7579)	mem 17019MB
[2024-08-02 09:55:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:06:14 lr 0.000019	 wd 0.0500	time 0.3091 (0.3399)	loss 1.2693 (1.1536)	grad_norm 0.3672 (0.3852)	loss_scale 8192.0000 (4639.7944)	mem 17019MB
[2024-08-02 09:56:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:40 lr 0.000019	 wd 0.0500	time 0.3450 (0.3396)	loss 0.8292 (1.1541)	grad_norm 0.3584 (0.3849)	loss_scale 8192.0000 (4876.4504)	mem 17019MB
[2024-08-02 09:57:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:05:06 lr 0.000019	 wd 0.0500	time 0.3589 (0.3394)	loss 1.1028 (1.1546)	grad_norm 0.3830 (0.3844)	loss_scale 8192.0000 (5083.5428)	mem 17019MB
[2024-08-02 09:57:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:31 lr 0.000019	 wd 0.0500	time 0.3030 (0.3391)	loss 1.3201 (1.1548)	grad_norm 0.3526 (0.3846)	loss_scale 8192.0000 (5266.2857)	mem 17019MB
[2024-08-02 09:58:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:58 lr 0.000019	 wd 0.0500	time 0.3096 (0.3390)	loss 0.7504 (1.1571)	grad_norm 0.3641 (0.3840)	loss_scale 8192.0000 (5428.7351)	mem 17019MB
[2024-08-02 09:58:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:23 lr 0.000019	 wd 0.0500	time 0.2998 (0.3387)	loss 1.1062 (1.1575)	grad_norm 0.3709 (0.3834)	loss_scale 8192.0000 (5574.0936)	mem 17019MB
[2024-08-02 09:59:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:49 lr 0.000019	 wd 0.0500	time 0.3366 (0.3386)	loss 1.1445 (1.1582)	grad_norm 0.3727 (0.3831)	loss_scale 8192.0000 (5704.9235)	mem 17019MB
[2024-08-02 09:59:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:15 lr 0.000019	 wd 0.0500	time 0.2981 (0.3382)	loss 1.4392 (1.1608)	grad_norm 0.3745 (0.3837)	loss_scale 8192.0000 (5823.2994)	mem 17019MB
[2024-08-02 10:00:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:42 lr 0.000019	 wd 0.0500	time 0.3005 (0.3380)	loss 1.2221 (1.1589)	grad_norm 0.4106 (0.3832)	loss_scale 8192.0000 (5930.9187)	mem 17019MB
[2024-08-02 10:00:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:08 lr 0.000019	 wd 0.0500	time 0.3153 (0.3379)	loss 1.3729 (1.1580)	grad_norm 0.3849 (0.3831)	loss_scale 8192.0000 (6029.1838)	mem 17019MB
[2024-08-02 10:01:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3022 (0.3380)	loss 1.2341 (1.1584)	grad_norm 0.3608 (0.3829)	loss_scale 8192.0000 (6119.2636)	mem 17019MB
[2024-08-02 10:02:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2951 (0.3373)	loss 1.5633 (1.1586)	grad_norm 0.3446 (0.3829)	loss_scale 8192.0000 (6202.1399)	mem 17019MB
[2024-08-02 10:02:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:14:06
[2024-08-02 10:02:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.631 (11.631)	Loss 0.4851 (0.4851)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-02 10:02:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.140 Acc@5 97.900
[2024-08-02 10:02:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 10:02:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.17%
[2024-08-02 10:02:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:43:37 lr 0.000019	 wd 0.0500	time 11.1183 (11.1183)	loss 1.2314 (1.2314)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-02 10:03:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:17:21 lr 0.000019	 wd 0.0500	time 0.3072 (0.4338)	loss 0.9669 (1.2003)	grad_norm 0.3500 (0.3735)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-02 10:03:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:14:36 lr 0.000019	 wd 0.0500	time 0.3221 (0.3808)	loss 0.8083 (1.1853)	grad_norm 0.3682 (0.3825)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-02 10:04:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:13:22 lr 0.000019	 wd 0.0500	time 0.3145 (0.3646)	loss 1.3827 (1.1769)	grad_norm 0.3798 (0.3823)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-02 10:04:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:24 lr 0.000019	 wd 0.0500	time 0.3255 (0.3544)	loss 0.8365 (1.1757)	grad_norm 0.3714 (0.3856)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-02 10:05:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:11:40 lr 0.000019	 wd 0.0500	time 0.2978 (0.3499)	loss 0.7967 (1.1649)	grad_norm 0.3670 (0.3841)	loss_scale 8192.0000 (8192.0000)	mem 17019MB
[2024-08-02 10:06:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:11:03 lr 0.000019	 wd 0.0500	time 0.3493 (0.3487)	loss 0.9121 (1.1618)	grad_norm 0.3840 (nan)	loss_scale 4096.0000 (7592.2529)	mem 17019MB
[2024-08-02 10:06:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:22 lr 0.000019	 wd 0.0500	time 0.2961 (0.3455)	loss 1.3329 (1.1624)	grad_norm 0.3820 (nan)	loss_scale 4096.0000 (7093.5007)	mem 17019MB
[2024-08-02 10:07:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:43 lr 0.000019	 wd 0.0500	time 0.3068 (0.3427)	loss 1.4529 (1.1642)	grad_norm 0.3734 (nan)	loss_scale 4096.0000 (6719.2809)	mem 17019MB
[2024-08-02 10:07:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:09:06 lr 0.000019	 wd 0.0500	time 0.3052 (0.3409)	loss 0.9441 (1.1593)	grad_norm 0.3738 (nan)	loss_scale 4096.0000 (6428.1287)	mem 17019MB
[2024-08-02 10:08:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:31 lr 0.000019	 wd 0.0500	time 0.3317 (0.3406)	loss 1.3372 (1.1628)	grad_norm 0.3664 (nan)	loss_scale 4096.0000 (6195.1489)	mem 17019MB
[2024-08-02 10:08:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:56 lr 0.000019	 wd 0.0500	time 0.3050 (0.3401)	loss 1.1891 (1.1610)	grad_norm 0.3589 (nan)	loss_scale 4096.0000 (6004.4905)	mem 17019MB
[2024-08-02 10:09:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:21 lr 0.000019	 wd 0.0500	time 0.3576 (0.3393)	loss 1.1176 (1.1605)	grad_norm 0.3799 (nan)	loss_scale 4096.0000 (5845.5820)	mem 17019MB
[2024-08-02 10:09:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:46 lr 0.000019	 wd 0.0500	time 0.2979 (0.3384)	loss 0.8982 (1.1600)	grad_norm 0.3788 (nan)	loss_scale 4096.0000 (5711.1022)	mem 17019MB
[2024-08-02 10:10:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:06:12 lr 0.000019	 wd 0.0500	time 0.3340 (0.3377)	loss 1.2575 (1.1608)	grad_norm 0.3773 (nan)	loss_scale 4096.0000 (5595.8201)	mem 17019MB
[2024-08-02 10:11:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:38 lr 0.000019	 wd 0.0500	time 0.3410 (0.3375)	loss 1.1586 (1.1610)	grad_norm 0.3811 (nan)	loss_scale 4096.0000 (5495.8987)	mem 17019MB
[2024-08-02 10:11:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:05:04 lr 0.000019	 wd 0.0500	time 0.3379 (0.3380)	loss 1.3662 (1.1637)	grad_norm 0.3776 (nan)	loss_scale 4096.0000 (5408.4597)	mem 17019MB
[2024-08-02 10:12:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:31 lr 0.000019	 wd 0.0500	time 0.3459 (0.3384)	loss 0.7609 (1.1661)	grad_norm 0.3579 (nan)	loss_scale 4096.0000 (5331.3016)	mem 17019MB
[2024-08-02 10:12:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:57 lr 0.000019	 wd 0.0500	time 0.2909 (0.3380)	loss 0.7826 (1.1648)	grad_norm 0.3764 (nan)	loss_scale 4096.0000 (5262.7118)	mem 17019MB
[2024-08-02 10:13:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:23 lr 0.000019	 wd 0.0500	time 0.3089 (0.3377)	loss 1.1730 (1.1665)	grad_norm 0.4572 (nan)	loss_scale 4096.0000 (5201.3382)	mem 17019MB
[2024-08-02 10:13:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:49 lr 0.000019	 wd 0.0500	time 0.3040 (0.3374)	loss 0.8072 (1.1654)	grad_norm 0.3694 (nan)	loss_scale 4096.0000 (5146.0990)	mem 17019MB
[2024-08-02 10:14:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:15 lr 0.000019	 wd 0.0500	time 0.2946 (0.3370)	loss 0.8698 (1.1658)	grad_norm 0.3839 (nan)	loss_scale 4096.0000 (5096.1180)	mem 17019MB
[2024-08-02 10:14:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:42 lr 0.000019	 wd 0.0500	time 0.3099 (0.3378)	loss 1.1053 (1.1641)	grad_norm 0.3937 (nan)	loss_scale 4096.0000 (5050.6788)	mem 17019MB
[2024-08-02 10:15:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:08 lr 0.000019	 wd 0.0500	time 0.3484 (0.3376)	loss 1.3325 (1.1628)	grad_norm 0.3594 (nan)	loss_scale 4096.0000 (5009.1890)	mem 17019MB
[2024-08-02 10:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3361 (0.3375)	loss 1.1577 (1.1627)	grad_norm 0.3917 (nan)	loss_scale 4096.0000 (4971.1554)	mem 17019MB
[2024-08-02 10:16:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2959 (0.3366)	loss 0.8969 (1.1625)	grad_norm 0.3576 (nan)	loss_scale 4096.0000 (4936.1631)	mem 17019MB
[2024-08-02 10:16:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:14:04
[2024-08-02 10:16:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.065 (12.065)	Loss 0.4863 (0.4863)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-02 10:17:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.184 Acc@5 97.892
[2024-08-02 10:17:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 10:17:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.18%
[2024-08-02 10:17:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 10:17:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 10:17:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:41:04 lr 0.000019	 wd 0.0500	time 11.0570 (11.0570)	loss 1.3121 (1.3121)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:17:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:17:13 lr 0.000019	 wd 0.0500	time 0.3067 (0.4303)	loss 1.2401 (1.1386)	grad_norm 0.3797 (0.3796)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:18:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:31 lr 0.000019	 wd 0.0500	time 0.3306 (0.3787)	loss 0.8891 (1.1626)	grad_norm 0.3691 (0.3775)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:18:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:13:16 lr 0.000019	 wd 0.0500	time 0.3173 (0.3618)	loss 1.4993 (1.1714)	grad_norm 0.3626 (0.3782)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:19:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:12:23 lr 0.000019	 wd 0.0500	time 0.3075 (0.3537)	loss 0.8384 (1.1549)	grad_norm 0.3801 (0.3819)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:20:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:37 lr 0.000019	 wd 0.0500	time 0.3027 (0.3483)	loss 0.7441 (1.1502)	grad_norm 0.3845 (0.3810)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:20:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:56 lr 0.000019	 wd 0.0500	time 0.3341 (0.3449)	loss 0.8359 (1.1547)	grad_norm 0.3629 (0.3833)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:21:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:10:17 lr 0.000019	 wd 0.0500	time 0.3111 (0.3428)	loss 1.3785 (1.1584)	grad_norm 0.3605 (0.3842)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:21:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:40 lr 0.000019	 wd 0.0500	time 0.3464 (0.3411)	loss 1.3812 (1.1596)	grad_norm 0.3585 (0.3837)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:22:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:09:04 lr 0.000019	 wd 0.0500	time 0.3335 (0.3399)	loss 1.3407 (1.1592)	grad_norm 0.3825 (0.3830)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:22:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:08:29 lr 0.000019	 wd 0.0500	time 0.3471 (0.3391)	loss 1.1541 (1.1599)	grad_norm 0.3721 (0.3833)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:23:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:54 lr 0.000018	 wd 0.0500	time 0.3571 (0.3385)	loss 1.3227 (1.1560)	grad_norm 0.3697 (0.3827)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:23:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:20 lr 0.000018	 wd 0.0500	time 0.3367 (0.3384)	loss 0.8383 (1.1550)	grad_norm 0.3826 (0.3824)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:24:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:46 lr 0.000018	 wd 0.0500	time 0.2955 (0.3379)	loss 1.2643 (1.1559)	grad_norm 0.3680 (0.3831)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 10:25:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:06:12 lr 0.000018	 wd 0.0500	time 0.3297 (0.3377)	loss 1.0686 (1.1551)	grad_norm 0.3676 (nan)	loss_scale 2048.0000 (3970.2841)	mem 17019MB
[2024-08-02 10:25:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:37 lr 0.000018	 wd 0.0500	time 0.2950 (0.3371)	loss 0.8452 (1.1584)	grad_norm 0.3775 (nan)	loss_scale 2048.0000 (3842.2172)	mem 17019MB
[2024-08-02 10:26:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:05:03 lr 0.000018	 wd 0.0500	time 0.3209 (0.3368)	loss 0.9042 (1.1566)	grad_norm 0.3627 (nan)	loss_scale 2048.0000 (3730.1487)	mem 17019MB
[2024-08-02 10:26:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:29 lr 0.000018	 wd 0.0500	time 0.3118 (0.3366)	loss 1.4194 (1.1571)	grad_norm 0.3757 (nan)	loss_scale 2048.0000 (3631.2569)	mem 17019MB
[2024-08-02 10:27:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:56 lr 0.000018	 wd 0.0500	time 0.3313 (0.3366)	loss 1.2431 (1.1559)	grad_norm 0.4007 (nan)	loss_scale 2048.0000 (3543.3470)	mem 17019MB
[2024-08-02 10:27:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:22 lr 0.000018	 wd 0.0500	time 0.3168 (0.3363)	loss 1.3830 (1.1553)	grad_norm 0.4402 (nan)	loss_scale 2048.0000 (3464.6860)	mem 17019MB
[2024-08-02 10:28:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:48 lr 0.000018	 wd 0.0500	time 0.3401 (0.3364)	loss 1.3853 (1.1558)	grad_norm 0.3508 (nan)	loss_scale 2048.0000 (3393.8871)	mem 17019MB
[2024-08-02 10:28:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:15 lr 0.000018	 wd 0.0500	time 0.3056 (0.3363)	loss 1.4015 (1.1574)	grad_norm 0.3859 (nan)	loss_scale 2048.0000 (3329.8277)	mem 17019MB
[2024-08-02 10:29:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:41 lr 0.000018	 wd 0.0500	time 0.3314 (0.3360)	loss 1.1570 (1.1585)	grad_norm 0.3737 (nan)	loss_scale 2048.0000 (3271.5893)	mem 17019MB
[2024-08-02 10:30:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:07 lr 0.000018	 wd 0.0500	time 0.3470 (0.3359)	loss 0.7850 (1.1585)	grad_norm 0.3830 (nan)	loss_scale 2048.0000 (3218.4129)	mem 17019MB
[2024-08-02 10:30:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:34 lr 0.000018	 wd 0.0500	time 0.3180 (0.3359)	loss 1.0699 (1.1588)	grad_norm 0.3984 (nan)	loss_scale 2048.0000 (3169.6660)	mem 17019MB
[2024-08-02 10:31:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2955 (0.3353)	loss 1.2559 (1.1583)	grad_norm 0.3761 (nan)	loss_scale 2048.0000 (3124.8173)	mem 17019MB
[2024-08-02 10:31:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 9 training takes 0:14:01
[2024-08-02 10:31:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.520 (12.520)	Loss 0.4995 (0.4995)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 10:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.214 Acc@5 97.902
[2024-08-02 10:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 10:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-02 10:31:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 10:31:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 10:31:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:40:03 lr 0.000018	 wd 0.0500	time 11.0327 (11.0327)	loss 1.6163 (1.6163)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 10:32:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:17:35 lr 0.000018	 wd 0.0500	time 0.3001 (0.4395)	loss 0.8843 (1.1690)	grad_norm 0.4062 (nan)	loss_scale 1024.0000 (1926.3366)	mem 17019MB
[2024-08-02 10:32:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:39 lr 0.000018	 wd 0.0500	time 0.3140 (0.3819)	loss 1.4266 (1.1644)	grad_norm 0.3592 (nan)	loss_scale 1024.0000 (1477.4129)	mem 17019MB
[2024-08-02 10:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:13:30 lr 0.000018	 wd 0.0500	time 0.3400 (0.3679)	loss 1.3171 (1.1593)	grad_norm 0.3883 (nan)	loss_scale 1024.0000 (1326.7774)	mem 17019MB
[2024-08-02 10:34:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:12:29 lr 0.000018	 wd 0.0500	time 0.3336 (0.3568)	loss 0.8757 (1.1628)	grad_norm 0.3847 (nan)	loss_scale 1024.0000 (1251.2718)	mem 17019MB
[2024-08-02 10:34:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:43 lr 0.000018	 wd 0.0500	time 0.3126 (0.3514)	loss 0.6973 (1.1583)	grad_norm 0.3632 (nan)	loss_scale 1024.0000 (1205.9082)	mem 17019MB
[2024-08-02 10:35:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:11:04 lr 0.000018	 wd 0.0500	time 0.3165 (0.3493)	loss 1.1610 (1.1660)	grad_norm 0.3677 (nan)	loss_scale 1024.0000 (1175.6406)	mem 17019MB
[2024-08-02 10:35:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:10:24 lr 0.000018	 wd 0.0500	time 0.3363 (0.3467)	loss 1.2860 (1.1643)	grad_norm 0.3410 (nan)	loss_scale 1024.0000 (1154.0086)	mem 17019MB
[2024-08-02 10:36:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:46 lr 0.000018	 wd 0.0500	time 0.3133 (0.3443)	loss 1.2648 (1.1669)	grad_norm 0.3638 (nan)	loss_scale 1024.0000 (1137.7778)	mem 17019MB
[2024-08-02 10:36:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:09:08 lr 0.000018	 wd 0.0500	time 0.3488 (0.3426)	loss 1.4507 (1.1646)	grad_norm 0.3595 (nan)	loss_scale 1024.0000 (1125.1498)	mem 17019MB
[2024-08-02 10:37:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:32 lr 0.000018	 wd 0.0500	time 0.3486 (0.3414)	loss 1.2894 (1.1656)	grad_norm 0.3757 (nan)	loss_scale 1024.0000 (1115.0450)	mem 17019MB
[2024-08-02 10:37:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:58 lr 0.000018	 wd 0.0500	time 0.3288 (0.3414)	loss 0.9911 (1.1714)	grad_norm 0.4117 (nan)	loss_scale 1024.0000 (1106.7757)	mem 17019MB
[2024-08-02 10:38:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:07:23 lr 0.000018	 wd 0.0500	time 0.3415 (0.3406)	loss 1.3912 (1.1689)	grad_norm 0.3731 (nan)	loss_scale 1024.0000 (1099.8834)	mem 17019MB
[2024-08-02 10:39:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:48 lr 0.000018	 wd 0.0500	time 0.3674 (0.3396)	loss 1.3549 (1.1665)	grad_norm 0.3871 (nan)	loss_scale 1024.0000 (1094.0507)	mem 17019MB
[2024-08-02 10:39:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:13 lr 0.000018	 wd 0.0500	time 0.3231 (0.3389)	loss 1.3465 (1.1644)	grad_norm 0.3693 (nan)	loss_scale 1024.0000 (1089.0507)	mem 17019MB
[2024-08-02 10:40:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:39 lr 0.000018	 wd 0.0500	time 0.3362 (0.3391)	loss 1.1537 (1.1619)	grad_norm 0.3940 (nan)	loss_scale 1024.0000 (1084.7169)	mem 17019MB
[2024-08-02 10:40:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:05:05 lr 0.000018	 wd 0.0500	time 0.3035 (0.3391)	loss 1.1071 (1.1606)	grad_norm 0.3727 (nan)	loss_scale 1024.0000 (1080.9244)	mem 17019MB
[2024-08-02 10:41:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:31 lr 0.000018	 wd 0.0500	time 0.3425 (0.3386)	loss 1.2317 (1.1626)	grad_norm 0.3884 (nan)	loss_scale 1024.0000 (1077.5779)	mem 17019MB
[2024-08-02 10:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:57 lr 0.000018	 wd 0.0500	time 0.2987 (0.3382)	loss 1.1329 (1.1617)	grad_norm 0.3952 (nan)	loss_scale 1024.0000 (1074.6030)	mem 17019MB
[2024-08-02 10:42:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:23 lr 0.000018	 wd 0.0500	time 0.3097 (0.3380)	loss 1.0813 (1.1607)	grad_norm 0.3727 (nan)	loss_scale 1024.0000 (1071.9411)	mem 17019MB
[2024-08-02 10:42:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:49 lr 0.000017	 wd 0.0500	time 0.3083 (0.3381)	loss 1.3187 (1.1642)	grad_norm 0.3818 (nan)	loss_scale 1024.0000 (1069.5452)	mem 17019MB
[2024-08-02 10:43:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:15 lr 0.000017	 wd 0.0500	time 0.3269 (0.3381)	loss 1.1962 (1.1655)	grad_norm 0.3663 (nan)	loss_scale 1024.0000 (1067.3774)	mem 17019MB
[2024-08-02 10:44:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:42 lr 0.000017	 wd 0.0500	time 0.3001 (0.3384)	loss 1.3204 (1.1666)	grad_norm 0.3681 (nan)	loss_scale 1024.0000 (1065.4066)	mem 17019MB
[2024-08-02 10:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:08 lr 0.000017	 wd 0.0500	time 0.3324 (0.3380)	loss 1.2865 (1.1645)	grad_norm 0.3757 (nan)	loss_scale 1024.0000 (1063.6071)	mem 17019MB
[2024-08-02 10:45:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:34 lr 0.000017	 wd 0.0500	time 0.3072 (0.3376)	loss 1.3514 (1.1635)	grad_norm 0.3522 (nan)	loss_scale 1024.0000 (1061.9575)	mem 17019MB
[2024-08-02 10:45:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.3015 (0.3368)	loss 0.8917 (1.1631)	grad_norm 0.3684 (nan)	loss_scale 1024.0000 (1060.4398)	mem 17019MB
[2024-08-02 10:45:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 10 training takes 0:14:05
[2024-08-02 10:45:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_10.pth saving......
[2024-08-02 10:45:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_10.pth saved !!!
[2024-08-02 10:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.446 (11.446)	Loss 0.4912 (0.4912)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-02 10:46:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.196 Acc@5 97.906
[2024-08-02 10:46:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 10:46:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-02 10:46:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:21:02 lr 0.000017	 wd 0.0500	time 12.0155 (12.0155)	loss 0.9062 (0.9062)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:46:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:17:35 lr 0.000017	 wd 0.0500	time 0.3000 (0.4395)	loss 1.3889 (1.1485)	grad_norm 0.3591 (0.3839)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:47:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:40 lr 0.000017	 wd 0.0500	time 0.3318 (0.3826)	loss 1.3205 (1.1691)	grad_norm 0.3819 (0.3810)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:48:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:13:20 lr 0.000017	 wd 0.0500	time 0.3300 (0.3634)	loss 0.8440 (1.1592)	grad_norm 0.3940 (0.3811)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:48:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:12:25 lr 0.000017	 wd 0.0500	time 0.3322 (0.3547)	loss 1.3913 (1.1574)	grad_norm 0.3807 (0.3821)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:47 lr 0.000017	 wd 0.0500	time 0.3490 (0.3536)	loss 0.7275 (1.1569)	grad_norm 0.4030 (0.3822)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:49:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:11:07 lr 0.000017	 wd 0.0500	time 0.3369 (0.3507)	loss 1.4530 (1.1576)	grad_norm 0.4247 (0.3810)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:50:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:10:25 lr 0.000017	 wd 0.0500	time 0.3063 (0.3472)	loss 1.0078 (1.1530)	grad_norm 0.4082 (0.3840)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:50:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:48 lr 0.000017	 wd 0.0500	time 0.3360 (0.3460)	loss 1.4779 (1.1511)	grad_norm 0.3626 (0.3847)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:51:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:09:11 lr 0.000017	 wd 0.0500	time 0.3016 (0.3441)	loss 0.9927 (1.1539)	grad_norm 0.3958 (0.3845)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:51:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:08:34 lr 0.000017	 wd 0.0500	time 0.3019 (0.3424)	loss 1.1639 (1.1539)	grad_norm 0.4094 (0.3856)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:52:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:59 lr 0.000017	 wd 0.0500	time 0.3219 (0.3418)	loss 0.9046 (1.1524)	grad_norm 0.3785 (0.3850)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:53:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:23 lr 0.000017	 wd 0.0500	time 0.3119 (0.3406)	loss 0.9234 (1.1494)	grad_norm 0.3764 (0.3849)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:53:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:47 lr 0.000017	 wd 0.0500	time 0.3257 (0.3394)	loss 1.3893 (1.1498)	grad_norm 0.3829 (0.3933)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:54:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:06:13 lr 0.000017	 wd 0.0500	time 0.3500 (0.3388)	loss 0.8041 (1.1501)	grad_norm 0.4095 (0.3924)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:54:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:39 lr 0.000017	 wd 0.0500	time 0.3147 (0.3387)	loss 1.3915 (1.1509)	grad_norm 0.3811 (0.3913)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 10:55:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:05:04 lr 0.000017	 wd 0.0500	time 0.3275 (0.3380)	loss 1.2667 (1.1522)	grad_norm 0.3726 (0.3905)	loss_scale 2048.0000 (1032.9544)	mem 17019MB
[2024-08-02 10:55:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:30 lr 0.000017	 wd 0.0500	time 0.3063 (0.3374)	loss 1.0761 (1.1514)	grad_norm 0.3656 (0.3895)	loss_scale 2048.0000 (1092.6279)	mem 17019MB
[2024-08-02 10:56:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:56 lr 0.000017	 wd 0.0500	time 0.3545 (0.3374)	loss 0.8276 (1.1513)	grad_norm 0.3620 (0.3890)	loss_scale 2048.0000 (1145.6746)	mem 17019MB
[2024-08-02 10:56:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:23 lr 0.000017	 wd 0.0500	time 0.3024 (0.3376)	loss 1.1352 (1.1505)	grad_norm 0.3780 (0.3885)	loss_scale 2048.0000 (1193.1405)	mem 17019MB
[2024-08-02 10:57:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:49 lr 0.000017	 wd 0.0500	time 0.3046 (0.3372)	loss 1.0581 (1.1507)	grad_norm 0.3507 (0.3886)	loss_scale 2048.0000 (1235.8621)	mem 17019MB
[2024-08-02 10:58:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:15 lr 0.000017	 wd 0.0500	time 0.3028 (0.3370)	loss 1.3521 (1.1499)	grad_norm 0.3876 (0.3880)	loss_scale 2048.0000 (1274.5169)	mem 17019MB
[2024-08-02 10:58:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:41 lr 0.000017	 wd 0.0500	time 0.3539 (0.3369)	loss 0.9849 (1.1518)	grad_norm 0.3873 (0.3879)	loss_scale 2048.0000 (1309.6592)	mem 17019MB
[2024-08-02 10:59:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:08 lr 0.000016	 wd 0.0500	time 0.3271 (0.3370)	loss 1.1505 (1.1500)	grad_norm 0.3849 (0.3876)	loss_scale 2048.0000 (1341.7471)	mem 17019MB
[2024-08-02 10:59:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:34 lr 0.000016	 wd 0.0500	time 0.3055 (0.3368)	loss 1.3838 (1.1512)	grad_norm 0.3780 (0.3876)	loss_scale 2048.0000 (1371.1620)	mem 17019MB
[2024-08-02 11:00:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2931 (0.3358)	loss 0.8371 (1.1495)	grad_norm 0.3442 (0.3874)	loss_scale 2048.0000 (1398.2247)	mem 17019MB
[2024-08-02 11:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 11 training takes 0:14:02
[2024-08-02 11:00:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.880 (11.880)	Loss 0.4915 (0.4915)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 11:00:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.160 Acc@5 97.898
[2024-08-02 11:00:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 11:00:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-02 11:00:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:09:43 lr 0.000016	 wd 0.0500	time 11.7441 (11.7441)	loss 0.9480 (0.9480)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:01:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:18:01 lr 0.000016	 wd 0.0500	time 0.3315 (0.4504)	loss 1.3072 (1.1535)	grad_norm 0.3614 (0.3808)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:02:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:14:52 lr 0.000016	 wd 0.0500	time 0.3088 (0.3877)	loss 1.2876 (1.1507)	grad_norm 0.3653 (0.3805)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:02:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:13:29 lr 0.000016	 wd 0.0500	time 0.3393 (0.3676)	loss 0.7445 (1.1629)	grad_norm 0.3905 (0.3838)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:03:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:30 lr 0.000016	 wd 0.0500	time 0.3063 (0.3569)	loss 1.2178 (1.1547)	grad_norm 0.3610 (0.3837)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:03:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:45 lr 0.000016	 wd 0.0500	time 0.3044 (0.3526)	loss 1.4197 (1.1548)	grad_norm 0.3635 (0.3843)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:04:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:11:04 lr 0.000016	 wd 0.0500	time 0.3075 (0.3494)	loss 1.4052 (1.1531)	grad_norm 0.3889 (0.3836)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:04:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:10:25 lr 0.000016	 wd 0.0500	time 0.3120 (0.3472)	loss 1.2002 (1.1511)	grad_norm 0.3755 (0.3828)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:05:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:47 lr 0.000016	 wd 0.0500	time 0.3086 (0.3451)	loss 1.3607 (1.1522)	grad_norm 0.3741 (0.3852)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:05:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:09:09 lr 0.000016	 wd 0.0500	time 0.3281 (0.3432)	loss 1.5229 (1.1496)	grad_norm 0.3669 (0.3865)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:06:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:35 lr 0.000016	 wd 0.0500	time 0.3190 (0.3431)	loss 1.3798 (1.1516)	grad_norm 0.3834 (0.3862)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:08:00 lr 0.000016	 wd 0.0500	time 0.3493 (0.3424)	loss 0.7983 (1.1531)	grad_norm 0.3736 (0.3854)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:07:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:24 lr 0.000016	 wd 0.0500	time 0.3244 (0.3411)	loss 1.4231 (1.1513)	grad_norm 0.3800 (0.3852)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:08:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:48 lr 0.000016	 wd 0.0500	time 0.3830 (0.3399)	loss 1.0418 (1.1522)	grad_norm 0.3565 (0.3865)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:08:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:06:13 lr 0.000016	 wd 0.0500	time 0.3088 (0.3390)	loss 1.3047 (1.1519)	grad_norm 0.3600 (0.3889)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:09:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:39 lr 0.000016	 wd 0.0500	time 0.3321 (0.3389)	loss 0.8060 (1.1492)	grad_norm 0.3878 (0.3898)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:09:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:05:05 lr 0.000016	 wd 0.0500	time 0.3505 (0.3388)	loss 1.3593 (1.1459)	grad_norm 0.3724 (0.3890)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:31 lr 0.000016	 wd 0.0500	time 0.3535 (0.3385)	loss 1.0562 (1.1481)	grad_norm 0.3780 (0.3892)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:10:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:57 lr 0.000016	 wd 0.0500	time 0.3159 (0.3380)	loss 0.8126 (1.1467)	grad_norm 0.3742 (0.3889)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:11:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:23 lr 0.000016	 wd 0.0500	time 0.3034 (0.3379)	loss 1.2960 (1.1474)	grad_norm 0.3801 (0.3886)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:11:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:49 lr 0.000016	 wd 0.0500	time 0.3129 (0.3379)	loss 1.0461 (1.1482)	grad_norm 0.3827 (0.3887)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:16 lr 0.000016	 wd 0.0500	time 0.2991 (0.3383)	loss 0.8001 (1.1475)	grad_norm 0.3357 (0.3885)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:13:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:42 lr 0.000016	 wd 0.0500	time 0.3253 (0.3382)	loss 0.8110 (1.1490)	grad_norm 0.3875 (0.3878)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:13:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:08 lr 0.000015	 wd 0.0500	time 0.3248 (0.3383)	loss 1.1836 (1.1494)	grad_norm 0.3737 (0.3893)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:14:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:34 lr 0.000015	 wd 0.0500	time 0.3316 (0.3379)	loss 1.3573 (1.1497)	grad_norm 0.3641 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:14:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.3170 (0.3370)	loss 0.9293 (1.1491)	grad_norm 0.3710 (0.3907)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:14:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 12 training takes 0:14:05
[2024-08-02 11:15:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.821 (11.821)	Loss 0.5034 (0.5034)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 11:15:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.172 Acc@5 97.894
[2024-08-02 11:15:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 11:15:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-02 11:15:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:21:19 lr 0.000015	 wd 0.0500	time 10.5833 (10.5833)	loss 1.3204 (1.3204)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:16:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:17:35 lr 0.000015	 wd 0.0500	time 0.3077 (0.4394)	loss 1.3257 (1.2138)	grad_norm 0.3733 (0.3841)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:16:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:39 lr 0.000015	 wd 0.0500	time 0.2981 (0.3819)	loss 1.3772 (1.2013)	grad_norm 0.3824 (0.3817)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:17:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:13:19 lr 0.000015	 wd 0.0500	time 0.2985 (0.3630)	loss 1.3790 (1.2012)	grad_norm 0.3671 (0.3921)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:17:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:12:23 lr 0.000015	 wd 0.0500	time 0.3312 (0.3535)	loss 1.4506 (1.1819)	grad_norm 0.3745 (0.3887)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:18:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:11:40 lr 0.000015	 wd 0.0500	time 0.3349 (0.3499)	loss 1.2757 (1.1752)	grad_norm 0.5037 (0.3870)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:18:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:11:00 lr 0.000015	 wd 0.0500	time 0.3288 (0.3473)	loss 1.1797 (1.1692)	grad_norm 0.3619 (0.3880)	loss_scale 4096.0000 (2109.3378)	mem 17019MB
[2024-08-02 11:19:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:10:21 lr 0.000015	 wd 0.0500	time 0.3302 (0.3447)	loss 1.1794 (1.1603)	grad_norm 0.3919 (0.3875)	loss_scale 4096.0000 (2392.7418)	mem 17019MB
[2024-08-02 11:19:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:43 lr 0.000015	 wd 0.0500	time 0.3110 (0.3428)	loss 1.2304 (1.1594)	grad_norm 0.3933 (0.3891)	loss_scale 4096.0000 (2605.3833)	mem 17019MB
[2024-08-02 11:20:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:09:06 lr 0.000015	 wd 0.0500	time 0.3443 (0.3412)	loss 1.3301 (1.1601)	grad_norm 0.3803 (0.3879)	loss_scale 4096.0000 (2770.8235)	mem 17019MB
[2024-08-02 11:20:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:30 lr 0.000015	 wd 0.0500	time 0.3545 (0.3398)	loss 1.3805 (1.1591)	grad_norm 0.3896 (0.3875)	loss_scale 4096.0000 (2903.2088)	mem 17019MB
[2024-08-02 11:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:56 lr 0.000015	 wd 0.0500	time 0.3274 (0.3398)	loss 1.3938 (1.1550)	grad_norm 0.3782 (0.3868)	loss_scale 4096.0000 (3011.5459)	mem 17019MB
[2024-08-02 11:22:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:22 lr 0.000015	 wd 0.0500	time 0.3379 (0.3396)	loss 1.1731 (1.1590)	grad_norm 0.4012 (nan)	loss_scale 2048.0000 (2944.9592)	mem 17019MB
[2024-08-02 11:22:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:47 lr 0.000015	 wd 0.0500	time 0.3048 (0.3387)	loss 0.7253 (1.1574)	grad_norm 0.3733 (nan)	loss_scale 2048.0000 (2876.0154)	mem 17019MB
[2024-08-02 11:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:06:12 lr 0.000015	 wd 0.0500	time 0.3194 (0.3379)	loss 1.3377 (1.1560)	grad_norm 0.3780 (nan)	loss_scale 2048.0000 (2816.9136)	mem 17019MB
[2024-08-02 11:23:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:38 lr 0.000015	 wd 0.0500	time 0.3286 (0.3374)	loss 1.2007 (1.1566)	grad_norm 0.3927 (nan)	loss_scale 2048.0000 (2765.6869)	mem 17019MB
[2024-08-02 11:24:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:05:04 lr 0.000015	 wd 0.0500	time 0.3308 (0.3375)	loss 0.9203 (1.1555)	grad_norm 0.3755 (nan)	loss_scale 2048.0000 (2720.8595)	mem 17019MB
[2024-08-02 11:24:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:30 lr 0.000015	 wd 0.0500	time 0.3172 (0.3378)	loss 1.0247 (1.1567)	grad_norm 0.3869 (nan)	loss_scale 2048.0000 (2681.3028)	mem 17019MB
[2024-08-02 11:25:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:56 lr 0.000015	 wd 0.0500	time 0.3043 (0.3375)	loss 1.2199 (1.1571)	grad_norm 0.3899 (nan)	loss_scale 2048.0000 (2646.1388)	mem 17019MB
[2024-08-02 11:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:23 lr 0.000015	 wd 0.0500	time 0.2985 (0.3372)	loss 1.0816 (1.1573)	grad_norm 0.3734 (nan)	loss_scale 2048.0000 (2614.6744)	mem 17019MB
[2024-08-02 11:26:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:49 lr 0.000015	 wd 0.0500	time 0.3243 (0.3369)	loss 1.3799 (1.1572)	grad_norm 0.3570 (nan)	loss_scale 2048.0000 (2586.3548)	mem 17019MB
[2024-08-02 11:27:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:15 lr 0.000014	 wd 0.0500	time 0.3201 (0.3366)	loss 1.3408 (1.1571)	grad_norm 0.3745 (nan)	loss_scale 2048.0000 (2560.7311)	mem 17019MB
[2024-08-02 11:27:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:41 lr 0.000014	 wd 0.0500	time 0.3044 (0.3366)	loss 1.1920 (1.1558)	grad_norm 0.4230 (nan)	loss_scale 2048.0000 (2537.4357)	mem 17019MB
[2024-08-02 11:28:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:08 lr 0.000014	 wd 0.0500	time 0.3537 (0.3368)	loss 1.3316 (1.1548)	grad_norm 0.3910 (nan)	loss_scale 2048.0000 (2516.1651)	mem 17019MB
[2024-08-02 11:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:34 lr 0.000014	 wd 0.0500	time 0.3494 (0.3367)	loss 0.9278 (1.1574)	grad_norm 0.3541 (nan)	loss_scale 2048.0000 (2496.6664)	mem 17019MB
[2024-08-02 11:29:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.3022 (0.3359)	loss 0.7858 (1.1562)	grad_norm 0.3751 (nan)	loss_scale 2048.0000 (2478.7269)	mem 17019MB
[2024-08-02 11:29:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 13 training takes 0:14:02
[2024-08-02 11:29:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.546 (11.546)	Loss 0.4766 (0.4766)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 11:29:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.184 Acc@5 97.914
[2024-08-02 11:29:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 11:29:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-02 11:29:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:42:31 lr 0.000014	 wd 0.0500	time 11.0916 (11.0916)	loss 1.4123 (1.4123)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:30:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:17:30 lr 0.000014	 wd 0.0500	time 0.3088 (0.4374)	loss 1.1741 (1.1722)	grad_norm 0.3717 (0.3805)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:31:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:46 lr 0.000014	 wd 0.0500	time 0.3309 (0.3850)	loss 1.4434 (1.1706)	grad_norm 0.3617 (0.3846)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:31:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:13:26 lr 0.000014	 wd 0.0500	time 0.3011 (0.3662)	loss 0.9171 (1.1739)	grad_norm 0.3805 (0.3838)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:32:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:12:33 lr 0.000014	 wd 0.0500	time 0.3709 (0.3585)	loss 1.5764 (1.1736)	grad_norm 0.3674 (0.3836)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:32:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:46 lr 0.000014	 wd 0.0500	time 0.2974 (0.3531)	loss 0.9628 (1.1693)	grad_norm 0.3860 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:33:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:11:04 lr 0.000014	 wd 0.0500	time 0.3553 (0.3492)	loss 0.9700 (1.1683)	grad_norm 0.3887 (0.3939)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:33:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:10:25 lr 0.000014	 wd 0.0500	time 0.3109 (0.3472)	loss 1.1246 (1.1691)	grad_norm 0.3860 (0.3925)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:34:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:47 lr 0.000014	 wd 0.0500	time 0.3393 (0.3452)	loss 0.7919 (1.1632)	grad_norm 0.3551 (0.3916)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:34:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:09:09 lr 0.000014	 wd 0.0500	time 0.3244 (0.3432)	loss 0.8487 (1.1621)	grad_norm 0.3812 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:35:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:34 lr 0.000014	 wd 0.0500	time 0.3916 (0.3423)	loss 0.9915 (1.1595)	grad_norm 0.4257 (0.3896)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:36:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:58 lr 0.000014	 wd 0.0500	time 0.3426 (0.3416)	loss 1.3708 (1.1589)	grad_norm 0.3953 (0.3887)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:36:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:23 lr 0.000014	 wd 0.0500	time 0.3336 (0.3403)	loss 1.4606 (1.1584)	grad_norm 0.3890 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:37:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:48 lr 0.000014	 wd 0.0500	time 0.3456 (0.3395)	loss 1.2335 (1.1617)	grad_norm 0.3772 (0.3919)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:37:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:13 lr 0.000014	 wd 0.0500	time 0.3068 (0.3391)	loss 1.2337 (1.1612)	grad_norm 0.3831 (0.3919)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:38:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:39 lr 0.000014	 wd 0.0500	time 0.3072 (0.3384)	loss 1.3796 (1.1603)	grad_norm 0.3689 (0.3916)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:38:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:05:04 lr 0.000014	 wd 0.0500	time 0.2887 (0.3377)	loss 0.7605 (1.1592)	grad_norm 0.3752 (0.3909)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:39:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:30 lr 0.000014	 wd 0.0500	time 0.3391 (0.3379)	loss 0.9227 (1.1581)	grad_norm 0.3572 (0.3901)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:39:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:58 lr 0.000013	 wd 0.0500	time 0.3259 (0.3391)	loss 0.8975 (1.1588)	grad_norm 0.4066 (0.3904)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:40:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:23 lr 0.000013	 wd 0.0500	time 0.3248 (0.3388)	loss 1.1428 (1.1591)	grad_norm 0.3949 (0.3917)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:49 lr 0.000013	 wd 0.0500	time 0.3232 (0.3384)	loss 1.2122 (1.1585)	grad_norm 0.3708 (0.3914)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:41:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:16 lr 0.000013	 wd 0.0500	time 0.2935 (0.3386)	loss 1.3321 (1.1578)	grad_norm 0.3713 (0.3909)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:42:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:42 lr 0.000013	 wd 0.0500	time 0.3143 (0.3383)	loss 1.3036 (1.1570)	grad_norm 0.3799 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:42:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:08 lr 0.000013	 wd 0.0500	time 0.3672 (0.3379)	loss 0.8508 (1.1559)	grad_norm 0.3754 (0.3903)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:43:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:34 lr 0.000013	 wd 0.0500	time 0.3137 (0.3378)	loss 0.8545 (1.1551)	grad_norm 0.4857 (0.3902)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:43:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2930 (0.3371)	loss 1.2331 (1.1540)	grad_norm 0.3783 (0.3900)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:43:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 14 training takes 0:14:05
[2024-08-02 11:44:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.602 (11.602)	Loss 0.4688 (0.4688)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-02 11:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.236 Acc@5 97.938
[2024-08-02 11:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 11:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.24%
[2024-08-02 11:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 11:44:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 11:44:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][0/2502]	eta 6:58:13 lr 0.000013	 wd 0.0500	time 10.0292 (10.0292)	loss 1.2520 (1.2520)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:45:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:17:15 lr 0.000013	 wd 0.0500	time 0.3413 (0.4312)	loss 0.7721 (1.1601)	grad_norm 0.3760 (0.3949)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:45:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:38 lr 0.000013	 wd 0.0500	time 0.3428 (0.3816)	loss 1.4308 (1.1458)	grad_norm 0.3920 (0.3894)	loss_scale 4096.0000 (3026.1493)	mem 17019MB
[2024-08-02 11:46:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:13:26 lr 0.000013	 wd 0.0500	time 0.2958 (0.3663)	loss 0.9227 (1.1455)	grad_norm 0.3762 (0.3852)	loss_scale 4096.0000 (3381.5814)	mem 17019MB
[2024-08-02 11:46:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:12:29 lr 0.000013	 wd 0.0500	time 0.3031 (0.3567)	loss 0.7945 (1.1443)	grad_norm 0.3680 (0.3874)	loss_scale 4096.0000 (3559.7406)	mem 17019MB
[2024-08-02 11:47:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:11:42 lr 0.000013	 wd 0.0500	time 0.3489 (0.3508)	loss 0.9466 (1.1555)	grad_norm 0.4008 (0.3865)	loss_scale 4096.0000 (3666.7784)	mem 17019MB
[2024-08-02 11:47:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:11:00 lr 0.000013	 wd 0.0500	time 0.3381 (0.3474)	loss 1.3740 (1.1582)	grad_norm 0.3783 (0.3869)	loss_scale 4096.0000 (3738.1963)	mem 17019MB
[2024-08-02 11:48:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:10:23 lr 0.000013	 wd 0.0500	time 0.2952 (0.3458)	loss 1.2394 (1.1582)	grad_norm 0.3729 (0.3865)	loss_scale 4096.0000 (3789.2382)	mem 17019MB
[2024-08-02 11:48:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:45 lr 0.000013	 wd 0.0500	time 0.2997 (0.3440)	loss 1.4016 (1.1600)	grad_norm 0.3630 (0.3860)	loss_scale 4096.0000 (3827.5356)	mem 17019MB
[2024-08-02 11:49:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:09:08 lr 0.000013	 wd 0.0500	time 0.3459 (0.3423)	loss 1.2496 (1.1597)	grad_norm 0.3830 (0.3896)	loss_scale 4096.0000 (3857.3319)	mem 17019MB
[2024-08-02 11:50:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:08:31 lr 0.000013	 wd 0.0500	time 0.3093 (0.3406)	loss 0.8484 (1.1596)	grad_norm 0.3739 (0.3888)	loss_scale 4096.0000 (3881.1748)	mem 17019MB
[2024-08-02 11:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:58 lr 0.000013	 wd 0.0500	time 0.3298 (0.3414)	loss 0.8620 (1.1604)	grad_norm 0.3715 (0.3899)	loss_scale 4096.0000 (3900.6866)	mem 17019MB
[2024-08-02 11:51:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:07:23 lr 0.000013	 wd 0.0500	time 0.3085 (0.3404)	loss 0.8334 (1.1604)	grad_norm 0.3884 (0.3898)	loss_scale 4096.0000 (3916.9492)	mem 17019MB
[2024-08-02 11:51:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:47 lr 0.000013	 wd 0.0500	time 0.3140 (0.3394)	loss 0.8556 (1.1615)	grad_norm 0.3800 (0.3894)	loss_scale 4096.0000 (3930.7118)	mem 17019MB
[2024-08-02 11:52:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:06:13 lr 0.000012	 wd 0.0500	time 0.2943 (0.3386)	loss 0.9417 (1.1616)	grad_norm 0.3738 (0.3889)	loss_scale 4096.0000 (3942.5096)	mem 17019MB
[2024-08-02 11:52:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:39 lr 0.000012	 wd 0.0500	time 0.3030 (0.3385)	loss 1.0111 (1.1578)	grad_norm 0.3756 (0.3889)	loss_scale 4096.0000 (3952.7355)	mem 17019MB
[2024-08-02 11:53:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:05:05 lr 0.000012	 wd 0.0500	time 0.3096 (0.3382)	loss 1.0174 (1.1586)	grad_norm 0.3869 (0.3905)	loss_scale 4096.0000 (3961.6839)	mem 17019MB
[2024-08-02 11:53:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:30 lr 0.000012	 wd 0.0500	time 0.3030 (0.3376)	loss 0.8173 (1.1586)	grad_norm 0.3815 (0.3904)	loss_scale 4096.0000 (3969.5802)	mem 17019MB
[2024-08-02 11:54:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:56 lr 0.000012	 wd 0.0500	time 0.3355 (0.3371)	loss 1.6677 (1.1566)	grad_norm 0.4078 (0.3899)	loss_scale 4096.0000 (3976.5997)	mem 17019MB
[2024-08-02 11:55:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:22 lr 0.000012	 wd 0.0500	time 0.3339 (0.3368)	loss 0.9581 (1.1549)	grad_norm 0.3966 (0.3895)	loss_scale 4096.0000 (3982.8806)	mem 17019MB
[2024-08-02 11:55:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:49 lr 0.000012	 wd 0.0500	time 0.3012 (0.3385)	loss 0.8362 (1.1562)	grad_norm 0.3767 (0.3891)	loss_scale 4096.0000 (3988.5337)	mem 17019MB
[2024-08-02 11:56:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:15 lr 0.000012	 wd 0.0500	time 0.3188 (0.3381)	loss 0.7824 (1.1550)	grad_norm 0.3725 (0.3886)	loss_scale 4096.0000 (3993.6487)	mem 17019MB
[2024-08-02 11:56:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:42 lr 0.000012	 wd 0.0500	time 0.3296 (0.3378)	loss 1.0704 (1.1545)	grad_norm 0.3638 (0.3884)	loss_scale 4096.0000 (3998.2990)	mem 17019MB
[2024-08-02 11:57:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:08 lr 0.000012	 wd 0.0500	time 0.2876 (0.3375)	loss 1.0101 (1.1548)	grad_norm 0.3864 (0.3883)	loss_scale 4096.0000 (4002.5450)	mem 17019MB
[2024-08-02 11:57:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:34 lr 0.000012	 wd 0.0500	time 0.3144 (0.3376)	loss 1.4598 (1.1552)	grad_norm 0.3800 (nan)	loss_scale 2048.0000 (4003.0254)	mem 17019MB
[2024-08-02 11:58:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2958 (0.3369)	loss 1.4976 (1.1558)	grad_norm 0.3938 (nan)	loss_scale 2048.0000 (3924.8557)	mem 17019MB
[2024-08-02 11:58:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 15 training takes 0:14:05
[2024-08-02 11:58:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.044 (12.044)	Loss 0.5200 (0.5200)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 11:58:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.176 Acc@5 97.910
[2024-08-02 11:58:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 11:58:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.24%
[2024-08-02 11:59:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:09:12 lr 0.000012	 wd 0.0500	time 10.2928 (10.2928)	loss 1.1990 (1.1990)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 11:59:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:17:21 lr 0.000012	 wd 0.0500	time 0.3438 (0.4334)	loss 1.0309 (1.1654)	grad_norm 0.3848 (0.4093)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:00:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:14:42 lr 0.000012	 wd 0.0500	time 0.3347 (0.3832)	loss 1.2471 (1.1557)	grad_norm 0.3729 (0.3968)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:00:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:13:37 lr 0.000012	 wd 0.0500	time 0.3090 (0.3714)	loss 1.0103 (1.1486)	grad_norm 0.3833 (0.3940)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:01:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:12:36 lr 0.000012	 wd 0.0500	time 0.3370 (0.3597)	loss 0.9249 (1.1374)	grad_norm 0.3748 (0.4028)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:01:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:45 lr 0.000012	 wd 0.0500	time 0.3138 (0.3526)	loss 1.5169 (1.1403)	grad_norm 0.3966 (0.4105)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:02:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:11:02 lr 0.000012	 wd 0.0500	time 0.3503 (0.3484)	loss 1.4338 (1.1411)	grad_norm 0.3512 (0.4082)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:02:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:10:24 lr 0.000012	 wd 0.0500	time 0.3017 (0.3464)	loss 1.0637 (1.1438)	grad_norm 0.4126 (0.4089)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:03:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:46 lr 0.000012	 wd 0.0500	time 0.3163 (0.3445)	loss 1.3101 (1.1453)	grad_norm 0.4118 (0.4060)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:04:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:09:09 lr 0.000012	 wd 0.0500	time 0.3090 (0.3431)	loss 1.1293 (1.1464)	grad_norm 0.3783 (0.4041)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:04:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:32 lr 0.000011	 wd 0.0500	time 0.2909 (0.3415)	loss 1.3491 (1.1463)	grad_norm 0.3705 (0.4035)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:05:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:58 lr 0.000011	 wd 0.0500	time 0.2960 (0.3411)	loss 0.8380 (1.1503)	grad_norm 0.3794 (0.4022)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:05:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:22 lr 0.000011	 wd 0.0500	time 0.3357 (0.3401)	loss 0.8664 (1.1481)	grad_norm 0.3729 (0.4012)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:48 lr 0.000011	 wd 0.0500	time 0.3151 (0.3396)	loss 1.2877 (1.1510)	grad_norm 0.3734 (0.3995)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:06:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:06:13 lr 0.000011	 wd 0.0500	time 0.3162 (0.3393)	loss 1.2286 (1.1510)	grad_norm 0.3898 (0.3980)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:07:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:40 lr 0.000011	 wd 0.0500	time 0.3023 (0.3394)	loss 1.2819 (1.1532)	grad_norm 0.3634 (0.3973)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:08:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:05:06 lr 0.000011	 wd 0.0500	time 0.3241 (0.3393)	loss 1.1097 (1.1538)	grad_norm 0.3930 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:08:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:31 lr 0.000011	 wd 0.0500	time 0.3163 (0.3387)	loss 1.0676 (1.1529)	grad_norm 0.5688 (0.3966)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:09:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:57 lr 0.000011	 wd 0.0500	time 0.3125 (0.3385)	loss 0.8690 (1.1519)	grad_norm 0.3493 (0.3972)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:09:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:24 lr 0.000011	 wd 0.0500	time 0.2939 (0.3394)	loss 1.0195 (1.1526)	grad_norm 0.4573 (0.3975)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:10:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:50 lr 0.000011	 wd 0.0500	time 0.3044 (0.3391)	loss 1.3457 (1.1533)	grad_norm 0.3638 (0.3968)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:10:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:16 lr 0.000011	 wd 0.0500	time 0.3528 (0.3390)	loss 1.3523 (1.1541)	grad_norm 0.3627 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:11:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:42 lr 0.000011	 wd 0.0500	time 0.3792 (0.3388)	loss 1.0104 (1.1538)	grad_norm 0.3844 (0.3956)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:11:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:08 lr 0.000011	 wd 0.0500	time 0.3333 (0.3385)	loss 0.8878 (1.1520)	grad_norm 0.4001 (0.3951)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:34 lr 0.000011	 wd 0.0500	time 0.3031 (0.3385)	loss 1.2068 (1.1510)	grad_norm 0.3635 (0.3947)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:13:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2923 (0.3377)	loss 0.8402 (1.1513)	grad_norm 0.3837 (0.3945)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:13:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 16 training takes 0:14:07
[2024-08-02 12:13:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.890 (11.890)	Loss 0.4902 (0.4902)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 12:13:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.202 Acc@5 97.882
[2024-08-02 12:13:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 12:13:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.24%
[2024-08-02 12:13:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:58:04 lr 0.000011	 wd 0.0500	time 11.4645 (11.4645)	loss 1.3826 (1.3826)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:14:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:17:45 lr 0.000011	 wd 0.0500	time 0.3153 (0.4434)	loss 1.2214 (1.1350)	grad_norm 0.3592 (0.3898)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:14:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:46 lr 0.000011	 wd 0.0500	time 0.2962 (0.3853)	loss 1.2705 (1.1346)	grad_norm 0.3739 (0.3839)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:15:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:13:26 lr 0.000011	 wd 0.0500	time 0.3230 (0.3661)	loss 1.2237 (1.1501)	grad_norm 0.3822 (0.3922)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:15:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:31 lr 0.000011	 wd 0.0500	time 0.3385 (0.3573)	loss 1.4044 (1.1479)	grad_norm 0.3793 (0.3907)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:16:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:43 lr 0.000010	 wd 0.0500	time 0.3015 (0.3513)	loss 1.3597 (1.1523)	grad_norm 0.3852 (0.3906)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:17:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:59 lr 0.000010	 wd 0.0500	time 0.3179 (0.3467)	loss 0.8095 (1.1535)	grad_norm 0.3845 (0.3898)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:10:22 lr 0.000010	 wd 0.0500	time 0.3448 (0.3452)	loss 1.2767 (1.1512)	grad_norm 0.3872 (0.3919)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:18:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:44 lr 0.000010	 wd 0.0500	time 0.3257 (0.3436)	loss 1.1789 (1.1495)	grad_norm 0.3846 (0.3928)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:18:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:09:07 lr 0.000010	 wd 0.0500	time 0.3021 (0.3420)	loss 1.2888 (1.1511)	grad_norm 0.3836 (0.3938)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:19:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:31 lr 0.000010	 wd 0.0500	time 0.3026 (0.3407)	loss 1.0282 (1.1506)	grad_norm 0.3885 (0.3933)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:19:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:57 lr 0.000010	 wd 0.0500	time 0.3233 (0.3404)	loss 1.0187 (1.1550)	grad_norm 0.3685 (0.3937)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:20:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:22 lr 0.000010	 wd 0.0500	time 0.2995 (0.3396)	loss 0.7581 (1.1550)	grad_norm 0.4080 (0.3938)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:47 lr 0.000010	 wd 0.0500	time 0.3307 (0.3386)	loss 1.4101 (1.1521)	grad_norm 0.3908 (0.3927)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:21:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:06:12 lr 0.000010	 wd 0.0500	time 0.3503 (0.3382)	loss 1.3219 (1.1522)	grad_norm 0.3534 (0.3923)	loss_scale 4096.0000 (2059.6945)	mem 17019MB
[2024-08-02 12:21:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:38 lr 0.000010	 wd 0.0500	time 0.3214 (0.3381)	loss 1.4558 (1.1527)	grad_norm 0.3810 (0.3919)	loss_scale 4096.0000 (2195.3578)	mem 17019MB
[2024-08-02 12:22:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:05:04 lr 0.000010	 wd 0.0500	time 0.3197 (0.3375)	loss 1.3808 (1.1526)	grad_norm 0.3671 (0.3915)	loss_scale 4096.0000 (2314.0737)	mem 17019MB
[2024-08-02 12:23:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:30 lr 0.000010	 wd 0.0500	time 0.3310 (0.3370)	loss 1.5150 (1.1543)	grad_norm 0.3825 (0.3914)	loss_scale 4096.0000 (2418.8313)	mem 17019MB
[2024-08-02 12:23:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:56 lr 0.000010	 wd 0.0500	time 0.3299 (0.3370)	loss 0.9726 (1.1538)	grad_norm 0.3897 (0.3924)	loss_scale 4096.0000 (2511.9556)	mem 17019MB
[2024-08-02 12:24:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:22 lr 0.000010	 wd 0.0500	time 0.3486 (0.3368)	loss 1.2376 (1.1528)	grad_norm 0.3727 (0.3921)	loss_scale 4096.0000 (2595.2825)	mem 17019MB
[2024-08-02 12:24:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:48 lr 0.000010	 wd 0.0500	time 0.3250 (0.3365)	loss 1.2039 (1.1531)	grad_norm 0.3780 (0.3934)	loss_scale 4096.0000 (2670.2809)	mem 17019MB
[2024-08-02 12:25:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:15 lr 0.000010	 wd 0.0500	time 0.3253 (0.3365)	loss 1.4645 (1.1530)	grad_norm 0.4054 (0.3936)	loss_scale 4096.0000 (2738.1399)	mem 17019MB
[2024-08-02 12:25:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:41 lr 0.000010	 wd 0.0500	time 0.3102 (0.3366)	loss 1.3591 (1.1528)	grad_norm 0.3739 (0.3930)	loss_scale 4096.0000 (2799.8328)	mem 17019MB
[2024-08-02 12:26:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:07 lr 0.000010	 wd 0.0500	time 0.3365 (0.3364)	loss 0.7920 (1.1523)	grad_norm 0.3918 (0.3933)	loss_scale 4096.0000 (2856.1634)	mem 17019MB
[2024-08-02 12:26:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:34 lr 0.000010	 wd 0.0500	time 0.3305 (0.3361)	loss 0.9278 (1.1510)	grad_norm 0.4465 (nan)	loss_scale 2048.0000 (2883.9184)	mem 17019MB
[2024-08-02 12:27:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.3313 (0.3353)	loss 1.4503 (1.1507)	grad_norm 0.3738 (nan)	loss_scale 2048.0000 (2850.4950)	mem 17019MB
[2024-08-02 12:27:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 17 training takes 0:14:01
[2024-08-02 12:27:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.432 (11.432)	Loss 0.4822 (0.4822)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 12:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.266 Acc@5 97.908
[2024-08-02 12:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-08-02 12:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 12:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 12:28:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 12:28:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:26:10 lr 0.000009	 wd 0.0500	time 10.6998 (10.6998)	loss 1.4283 (1.4283)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:17:04 lr 0.000009	 wd 0.0500	time 0.3014 (0.4265)	loss 1.4780 (1.1845)	grad_norm 0.3826 (0.3854)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:29:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:25 lr 0.000009	 wd 0.0500	time 0.3343 (0.3760)	loss 1.0278 (1.1766)	grad_norm 0.4120 (0.3917)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:29:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:13:17 lr 0.000009	 wd 0.0500	time 0.3230 (0.3624)	loss 1.4994 (1.1755)	grad_norm 0.5059 (0.3915)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:30:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:12:21 lr 0.000009	 wd 0.0500	time 0.3332 (0.3528)	loss 0.7437 (1.1743)	grad_norm 0.3810 (0.3978)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:30:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:11:41 lr 0.000009	 wd 0.0500	time 0.3334 (0.3502)	loss 1.2857 (1.1657)	grad_norm 0.3600 (0.3950)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:31:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:11:00 lr 0.000009	 wd 0.0500	time 0.3497 (0.3473)	loss 1.4818 (1.1584)	grad_norm 0.3889 (0.3935)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:32:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:10:23 lr 0.000009	 wd 0.0500	time 0.3229 (0.3459)	loss 1.4580 (1.1634)	grad_norm 0.3707 (0.3947)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:32:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:44 lr 0.000009	 wd 0.0500	time 0.3051 (0.3436)	loss 1.3156 (1.1664)	grad_norm 0.3670 (0.3936)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:33:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:09:07 lr 0.000009	 wd 0.0500	time 0.3384 (0.3420)	loss 0.9145 (1.1662)	grad_norm 0.3894 (0.3929)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:33:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:33 lr 0.000009	 wd 0.0500	time 0.3603 (0.3420)	loss 1.4206 (1.1619)	grad_norm 0.3784 (0.3947)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:34:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:58 lr 0.000009	 wd 0.0500	time 0.3237 (0.3411)	loss 0.9903 (1.1639)	grad_norm 0.3757 (0.3936)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:34:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:22 lr 0.000009	 wd 0.0500	time 0.3303 (0.3401)	loss 1.0402 (1.1570)	grad_norm 0.3827 (0.3927)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:35:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:47 lr 0.000009	 wd 0.0500	time 0.3364 (0.3392)	loss 1.4627 (1.1572)	grad_norm 0.3861 (0.3953)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:35:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:06:14 lr 0.000009	 wd 0.0500	time 0.3463 (0.3394)	loss 1.1525 (1.1574)	grad_norm 0.3807 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:36:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:39 lr 0.000009	 wd 0.0500	time 0.3214 (0.3390)	loss 1.3069 (1.1571)	grad_norm 0.3694 (0.3948)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:37:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:05:05 lr 0.000009	 wd 0.0500	time 0.3010 (0.3384)	loss 1.4526 (1.1591)	grad_norm 0.3979 (0.3945)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:37:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:30 lr 0.000009	 wd 0.0500	time 0.3187 (0.3379)	loss 1.3215 (1.1609)	grad_norm 0.3809 (0.3940)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:38:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:57 lr 0.000009	 wd 0.0500	time 0.2923 (0.3383)	loss 0.7203 (1.1612)	grad_norm 0.3746 (0.3934)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:38:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:23 lr 0.000009	 wd 0.0500	time 0.3537 (0.3380)	loss 1.4180 (1.1608)	grad_norm 0.3907 (0.3935)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:39:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.3517 (0.3378)	loss 1.0566 (1.1605)	grad_norm 0.3729 (0.3959)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:39:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:15 lr 0.000008	 wd 0.0500	time 0.3313 (0.3377)	loss 1.3250 (1.1594)	grad_norm 0.3737 (0.3959)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:40:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.3137 (0.3380)	loss 1.0189 (1.1596)	grad_norm 0.4760 (0.3982)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:41:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.3393 (0.3378)	loss 1.3064 (1.1602)	grad_norm 0.4086 (0.3982)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:41:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.3686 (0.3375)	loss 1.4224 (1.1609)	grad_norm 0.3974 (0.3978)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:42:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.3163 (0.3366)	loss 1.2455 (1.1612)	grad_norm 0.3710 (0.3972)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:42:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 18 training takes 0:14:04
[2024-08-02 12:42:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.797 (11.797)	Loss 0.4941 (0.4941)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 12:42:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.180 Acc@5 97.890
[2024-08-02 12:42:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 12:42:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 12:42:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:25:34 lr 0.000008	 wd 0.0500	time 12.1240 (12.1240)	loss 0.7944 (0.7944)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:43:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:29 lr 0.000008	 wd 0.0500	time 0.3010 (0.4369)	loss 1.5862 (1.2074)	grad_norm 0.3625 (0.3879)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:43:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:37 lr 0.000008	 wd 0.0500	time 0.3002 (0.3811)	loss 1.1745 (1.1769)	grad_norm 0.3914 (0.3941)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:44:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:13:26 lr 0.000008	 wd 0.0500	time 0.2985 (0.3660)	loss 1.4036 (1.1633)	grad_norm 0.3858 (0.3896)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:44:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:12:28 lr 0.000008	 wd 0.0500	time 0.3171 (0.3562)	loss 0.9575 (1.1568)	grad_norm 0.3711 (0.3911)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:45:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:42 lr 0.000008	 wd 0.0500	time 0.3105 (0.3507)	loss 1.4378 (1.1612)	grad_norm 0.3787 (0.3923)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:46:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:59 lr 0.000008	 wd 0.0500	time 0.3040 (0.3465)	loss 0.8427 (1.1508)	grad_norm 0.3846 (0.3916)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:46:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:10:21 lr 0.000008	 wd 0.0500	time 0.3134 (0.3451)	loss 1.2454 (1.1488)	grad_norm 0.3653 (0.3954)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:47:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:44 lr 0.000008	 wd 0.0500	time 0.3168 (0.3433)	loss 1.5176 (1.1505)	grad_norm 0.3845 (0.3948)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:47:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:09:07 lr 0.000008	 wd 0.0500	time 0.3179 (0.3416)	loss 1.2699 (1.1510)	grad_norm 0.3975 (0.3942)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:48:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:31 lr 0.000008	 wd 0.0500	time 0.3263 (0.3407)	loss 1.4438 (1.1526)	grad_norm 0.3776 (0.4009)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:58 lr 0.000008	 wd 0.0500	time 0.3182 (0.3410)	loss 1.1682 (1.1515)	grad_norm 0.3861 (0.3996)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:49:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:22 lr 0.000008	 wd 0.0500	time 0.3313 (0.3401)	loss 1.3105 (1.1512)	grad_norm 0.3815 (0.3985)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:47 lr 0.000008	 wd 0.0500	time 0.3393 (0.3392)	loss 0.7787 (1.1502)	grad_norm 0.3893 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 12:50:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:06:13 lr 0.000008	 wd 0.0500	time 0.3169 (0.3392)	loss 0.8041 (1.1494)	grad_norm 0.3853 (0.3965)	loss_scale 4096.0000 (2094.7780)	mem 17019MB
[2024-08-02 12:51:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:39 lr 0.000008	 wd 0.0500	time 0.3201 (0.3392)	loss 1.3583 (1.1506)	grad_norm 0.3768 (0.3961)	loss_scale 4096.0000 (2228.1039)	mem 17019MB
[2024-08-02 12:51:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:05:05 lr 0.000007	 wd 0.0500	time 0.3469 (0.3388)	loss 1.4055 (1.1523)	grad_norm 0.3744 (0.3961)	loss_scale 4096.0000 (2344.7745)	mem 17019MB
[2024-08-02 12:52:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:31 lr 0.000007	 wd 0.0500	time 0.3230 (0.3381)	loss 0.8982 (1.1528)	grad_norm 0.3996 (0.3955)	loss_scale 4096.0000 (2447.7272)	mem 17019MB
[2024-08-02 12:52:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:57 lr 0.000007	 wd 0.0500	time 0.3217 (0.3380)	loss 1.3205 (1.1552)	grad_norm 0.3796 (0.3948)	loss_scale 4096.0000 (2539.2471)	mem 17019MB
[2024-08-02 12:53:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:23 lr 0.000007	 wd 0.0500	time 0.3317 (0.3381)	loss 1.0969 (1.1542)	grad_norm 0.3891 (0.3973)	loss_scale 4096.0000 (2621.1383)	mem 17019MB
[2024-08-02 12:53:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:49 lr 0.000007	 wd 0.0500	time 0.2985 (0.3377)	loss 1.4697 (1.1553)	grad_norm 0.3872 (0.3968)	loss_scale 4096.0000 (2694.8446)	mem 17019MB
[2024-08-02 12:54:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:15 lr 0.000007	 wd 0.0500	time 0.3097 (0.3380)	loss 1.5684 (1.1567)	grad_norm 0.3657 (0.3965)	loss_scale 4096.0000 (2761.5345)	mem 17019MB
[2024-08-02 12:55:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:42 lr 0.000007	 wd 0.0500	time 0.3427 (0.3382)	loss 1.1351 (1.1558)	grad_norm 0.4677 (0.3966)	loss_scale 4096.0000 (2822.1645)	mem 17019MB
[2024-08-02 12:55:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:08 lr 0.000007	 wd 0.0500	time 0.3393 (0.3381)	loss 1.0097 (1.1560)	grad_norm 0.3706 (0.3962)	loss_scale 4096.0000 (2877.5246)	mem 17019MB
[2024-08-02 12:56:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:34 lr 0.000007	 wd 0.0500	time 0.3362 (0.3379)	loss 1.1001 (1.1551)	grad_norm 0.3895 (0.3960)	loss_scale 4096.0000 (2928.2732)	mem 17019MB
[2024-08-02 12:56:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.3313 (0.3371)	loss 0.8038 (1.1554)	grad_norm 0.5530 (0.3960)	loss_scale 4096.0000 (2974.9636)	mem 17019MB
[2024-08-02 12:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 19 training takes 0:14:05
[2024-08-02 12:56:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.443 (12.443)	Loss 0.4775 (0.4775)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 12:57:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.232 Acc@5 97.918
[2024-08-02 12:57:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 12:57:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 12:57:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][0/2502]	eta 6:30:47 lr 0.000007	 wd 0.0500	time 9.3716 (9.3716)	loss 1.5171 (1.5171)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 12:57:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:17:22 lr 0.000007	 wd 0.0500	time 0.3128 (0.4340)	loss 1.0660 (1.1650)	grad_norm 0.3878 (0.3921)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 12:58:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:35 lr 0.000007	 wd 0.0500	time 0.3508 (0.3802)	loss 1.3023 (1.1486)	grad_norm 0.4031 (0.3873)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 12:58:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:13:15 lr 0.000007	 wd 0.0500	time 0.2937 (0.3612)	loss 0.6955 (1.1606)	grad_norm 0.4101 (0.3874)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 12:59:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:12:21 lr 0.000007	 wd 0.0500	time 0.3130 (0.3527)	loss 1.2996 (1.1600)	grad_norm 0.3762 (0.3882)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:00:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:11:35 lr 0.000007	 wd 0.0500	time 0.3046 (0.3475)	loss 1.2738 (1.1580)	grad_norm 0.3841 (0.3884)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:00:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:10:54 lr 0.000007	 wd 0.0500	time 0.3065 (0.3440)	loss 1.2562 (1.1628)	grad_norm 0.4060 (0.3884)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:01:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:10:16 lr 0.000007	 wd 0.0500	time 0.3362 (0.3419)	loss 1.2533 (1.1612)	grad_norm 0.3806 (0.3888)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:01:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:40 lr 0.000007	 wd 0.0500	time 0.3435 (0.3412)	loss 1.1585 (1.1614)	grad_norm 0.3736 (0.3885)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:02:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:09:05 lr 0.000007	 wd 0.0500	time 0.3449 (0.3404)	loss 1.2486 (1.1608)	grad_norm 0.3837 (0.3886)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:02:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:08:32 lr 0.000007	 wd 0.0500	time 0.3038 (0.3411)	loss 1.3752 (1.1627)	grad_norm 0.3758 (0.3889)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:03:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:57 lr 0.000007	 wd 0.0500	time 0.3080 (0.3406)	loss 1.0635 (1.1605)	grad_norm 0.4013 (0.3884)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:03:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:22 lr 0.000006	 wd 0.0500	time 0.3179 (0.3401)	loss 1.2222 (1.1567)	grad_norm 0.3657 (0.3879)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:04:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:48 lr 0.000006	 wd 0.0500	time 0.3392 (0.3397)	loss 1.3418 (1.1563)	grad_norm 0.3811 (0.3877)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:05:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:06:13 lr 0.000006	 wd 0.0500	time 0.2898 (0.3391)	loss 1.0535 (1.1577)	grad_norm 0.3980 (0.3877)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:05:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:39 lr 0.000006	 wd 0.0500	time 0.3557 (0.3388)	loss 1.3178 (1.1604)	grad_norm 0.4836 (0.3882)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:06:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:05:05 lr 0.000006	 wd 0.0500	time 0.3483 (0.3392)	loss 1.2866 (1.1625)	grad_norm 0.3781 (0.3882)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:06:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:32 lr 0.000006	 wd 0.0500	time 0.3204 (0.3395)	loss 1.6042 (1.1637)	grad_norm 0.3742 (0.3881)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:07:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:58 lr 0.000006	 wd 0.0500	time 0.3411 (0.3397)	loss 0.7609 (1.1614)	grad_norm 0.3772 (0.3878)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:07:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:24 lr 0.000006	 wd 0.0500	time 0.3173 (0.3391)	loss 0.8318 (1.1627)	grad_norm 0.3758 (0.3879)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:08:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:50 lr 0.000006	 wd 0.0500	time 0.3395 (0.3388)	loss 1.0996 (1.1618)	grad_norm 0.4004 (0.3880)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:09:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:16 lr 0.000006	 wd 0.0500	time 0.3224 (0.3385)	loss 1.2267 (1.1618)	grad_norm 0.3590 (0.3881)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:09:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:42 lr 0.000006	 wd 0.0500	time 0.3283 (0.3381)	loss 1.3917 (1.1629)	grad_norm 0.4072 (0.3884)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:10:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:08 lr 0.000006	 wd 0.0500	time 0.3170 (0.3381)	loss 0.8636 (1.1641)	grad_norm 0.3773 (0.3889)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:10:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:34 lr 0.000006	 wd 0.0500	time 0.3093 (0.3380)	loss 0.7750 (1.1635)	grad_norm 0.3884 (0.3895)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:11:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2919 (0.3371)	loss 1.0429 (1.1653)	grad_norm 0.3767 (0.3898)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:11:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 20 training takes 0:14:05
[2024-08-02 13:11:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_20.pth saving......
[2024-08-02 13:11:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_20.pth saved !!!
[2024-08-02 13:11:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.014 (11.014)	Loss 0.4873 (0.4873)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-02 13:11:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.258 Acc@5 97.934
[2024-08-02 13:11:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-08-02 13:11:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 13:11:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:20:11 lr 0.000006	 wd 0.0500	time 11.9952 (11.9952)	loss 0.9947 (0.9947)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:17:40 lr 0.000006	 wd 0.0500	time 0.2969 (0.4415)	loss 1.3641 (1.1857)	grad_norm 0.3986 (0.3836)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:13:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:14:40 lr 0.000006	 wd 0.0500	time 0.3332 (0.3825)	loss 1.3177 (1.1800)	grad_norm 0.3840 (0.3850)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:13:26 lr 0.000006	 wd 0.0500	time 0.3349 (0.3662)	loss 1.1838 (1.1698)	grad_norm 0.3968 (0.3875)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:14:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:12:30 lr 0.000006	 wd 0.0500	time 0.3461 (0.3569)	loss 1.3602 (1.1619)	grad_norm 0.3757 (0.3910)	loss_scale 8192.0000 (4463.7207)	mem 17019MB
[2024-08-02 13:14:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:42 lr 0.000006	 wd 0.0500	time 0.2930 (0.3508)	loss 0.8990 (1.1569)	grad_norm 0.3749 (nan)	loss_scale 4096.0000 (5093.4291)	mem 17019MB
[2024-08-02 13:15:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:59 lr 0.000006	 wd 0.0500	time 0.3534 (0.3466)	loss 1.2936 (1.1522)	grad_norm 0.3885 (nan)	loss_scale 2048.0000 (4695.7471)	mem 17019MB
[2024-08-02 13:15:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:10:19 lr 0.000006	 wd 0.0500	time 0.2944 (0.3440)	loss 1.2277 (1.1498)	grad_norm 0.4331 (nan)	loss_scale 2048.0000 (4318.0371)	mem 17019MB
[2024-08-02 13:16:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:46 lr 0.000006	 wd 0.0500	time 0.3402 (0.3447)	loss 1.2058 (1.1484)	grad_norm 0.3882 (nan)	loss_scale 2048.0000 (4034.6367)	mem 17019MB
[2024-08-02 13:16:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:09:09 lr 0.000005	 wd 0.0500	time 0.3396 (0.3433)	loss 1.4299 (1.1514)	grad_norm 0.3999 (nan)	loss_scale 2048.0000 (3814.1443)	mem 17019MB
[2024-08-02 13:17:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:33 lr 0.000005	 wd 0.0500	time 0.3031 (0.3417)	loss 1.4889 (1.1548)	grad_norm 0.3733 (nan)	loss_scale 2048.0000 (3637.7063)	mem 17019MB
[2024-08-02 13:18:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:57 lr 0.000005	 wd 0.0500	time 0.3368 (0.3407)	loss 0.7983 (1.1579)	grad_norm 0.3751 (nan)	loss_scale 2048.0000 (3493.3188)	mem 17019MB
[2024-08-02 13:18:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:22 lr 0.000005	 wd 0.0500	time 0.3078 (0.3396)	loss 1.3882 (1.1548)	grad_norm 0.3849 (nan)	loss_scale 2048.0000 (3372.9759)	mem 17019MB
[2024-08-02 13:19:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:48 lr 0.000005	 wd 0.0500	time 0.3351 (0.3395)	loss 1.1920 (1.1552)	grad_norm 0.3720 (nan)	loss_scale 2048.0000 (3271.1330)	mem 17019MB
[2024-08-02 13:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:06:13 lr 0.000005	 wd 0.0500	time 0.3519 (0.3391)	loss 0.8288 (1.1525)	grad_norm 0.4002 (nan)	loss_scale 2048.0000 (3183.8287)	mem 17019MB
[2024-08-02 13:20:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:39 lr 0.000005	 wd 0.0500	time 0.3048 (0.3386)	loss 0.8467 (1.1542)	grad_norm 0.3782 (nan)	loss_scale 2048.0000 (3108.1572)	mem 17019MB
[2024-08-02 13:20:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:05:04 lr 0.000005	 wd 0.0500	time 0.3844 (0.3378)	loss 1.0471 (1.1508)	grad_norm 0.4021 (nan)	loss_scale 2048.0000 (3041.9388)	mem 17019MB
[2024-08-02 13:21:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:30 lr 0.000005	 wd 0.0500	time 0.3133 (0.3373)	loss 1.3500 (1.1516)	grad_norm 0.3605 (nan)	loss_scale 2048.0000 (2983.5062)	mem 17019MB
[2024-08-02 13:21:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:57 lr 0.000005	 wd 0.0500	time 0.3389 (0.3385)	loss 0.7692 (1.1521)	grad_norm 0.3936 (nan)	loss_scale 2048.0000 (2931.5625)	mem 17019MB
[2024-08-02 13:22:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:23 lr 0.000005	 wd 0.0500	time 0.2970 (0.3387)	loss 0.8439 (1.1528)	grad_norm 0.3728 (nan)	loss_scale 2048.0000 (2885.0836)	mem 17019MB
[2024-08-02 13:23:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:49 lr 0.000005	 wd 0.0500	time 0.2892 (0.3385)	loss 1.0635 (1.1519)	grad_norm 0.4132 (nan)	loss_scale 2048.0000 (2843.2504)	mem 17019MB
[2024-08-02 13:23:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:15 lr 0.000005	 wd 0.0500	time 0.3043 (0.3382)	loss 1.1229 (1.1516)	grad_norm 0.3785 (nan)	loss_scale 2048.0000 (2805.3993)	mem 17019MB
[2024-08-02 13:24:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:42 lr 0.000005	 wd 0.0500	time 0.3369 (0.3381)	loss 1.4378 (1.1502)	grad_norm 0.3892 (nan)	loss_scale 2048.0000 (2770.9877)	mem 17019MB
[2024-08-02 13:24:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:08 lr 0.000005	 wd 0.0500	time 0.3173 (0.3385)	loss 1.5312 (1.1519)	grad_norm 0.4047 (nan)	loss_scale 2048.0000 (2739.5671)	mem 17019MB
[2024-08-02 13:25:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:34 lr 0.000005	 wd 0.0500	time 0.3528 (0.3389)	loss 1.2238 (1.1537)	grad_norm 0.4529 (nan)	loss_scale 2048.0000 (2710.7638)	mem 17019MB
[2024-08-02 13:25:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2934 (0.3380)	loss 1.4758 (1.1541)	grad_norm 0.3851 (nan)	loss_scale 2048.0000 (2684.2639)	mem 17019MB
[2024-08-02 13:25:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 21 training takes 0:14:08
[2024-08-02 13:26:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.808 (11.808)	Loss 0.5063 (0.5063)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 13:26:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.240 Acc@5 97.916
[2024-08-02 13:26:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 13:26:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 13:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:01:24 lr 0.000005	 wd 0.0500	time 11.5447 (11.5447)	loss 0.9913 (0.9913)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:27:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:17:48 lr 0.000005	 wd 0.0500	time 0.3163 (0.4447)	loss 1.4701 (1.1711)	grad_norm 0.4310 (0.4491)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:27:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:58 lr 0.000005	 wd 0.0500	time 0.2933 (0.3902)	loss 0.7405 (1.1633)	grad_norm 0.3663 (0.4305)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:28:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:13:35 lr 0.000005	 wd 0.0500	time 0.3156 (0.3703)	loss 1.3709 (1.1688)	grad_norm 0.3774 (0.4168)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:28:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:12:35 lr 0.000005	 wd 0.0500	time 0.3081 (0.3594)	loss 0.7476 (1.1749)	grad_norm 0.3752 (0.4130)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:29:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:49 lr 0.000005	 wd 0.0500	time 0.3309 (0.3543)	loss 1.1890 (1.1725)	grad_norm 0.3649 (0.4067)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:29:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:11:10 lr 0.000005	 wd 0.0500	time 0.3323 (0.3527)	loss 0.8737 (1.1741)	grad_norm 0.3667 (0.4045)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:30:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:10:29 lr 0.000005	 wd 0.0500	time 0.2962 (0.3495)	loss 1.3474 (1.1686)	grad_norm 0.4016 (0.4015)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:31:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:50 lr 0.000004	 wd 0.0500	time 0.3315 (0.3471)	loss 0.9323 (1.1673)	grad_norm 0.4262 (0.3997)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:31:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:09:13 lr 0.000004	 wd 0.0500	time 0.3515 (0.3458)	loss 1.1015 (1.1661)	grad_norm 0.3931 (0.4006)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:32:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:37 lr 0.000004	 wd 0.0500	time 0.3051 (0.3444)	loss 1.2736 (1.1664)	grad_norm 0.3808 (0.4015)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:32:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:08:01 lr 0.000004	 wd 0.0500	time 0.3198 (0.3431)	loss 0.8052 (1.1635)	grad_norm 0.3847 (0.4030)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:33:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:26 lr 0.000004	 wd 0.0500	time 0.2965 (0.3426)	loss 1.3723 (1.1645)	grad_norm 0.3710 (0.4040)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:33:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:50 lr 0.000004	 wd 0.0500	time 0.3439 (0.3419)	loss 0.8522 (1.1583)	grad_norm 0.3601 (0.4027)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:34:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:06:15 lr 0.000004	 wd 0.0500	time 0.3044 (0.3409)	loss 0.9997 (1.1528)	grad_norm 0.3777 (0.4021)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:34:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:41 lr 0.000004	 wd 0.0500	time 0.3194 (0.3405)	loss 1.1964 (1.1525)	grad_norm 0.3771 (0.4011)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:35:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:05:06 lr 0.000004	 wd 0.0500	time 0.2989 (0.3402)	loss 1.5134 (1.1526)	grad_norm 0.3706 (0.4004)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:36:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:32 lr 0.000004	 wd 0.0500	time 0.3405 (0.3397)	loss 0.8946 (1.1508)	grad_norm 0.3879 (0.3998)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:36:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:58 lr 0.000004	 wd 0.0500	time 0.3583 (0.3398)	loss 1.3871 (1.1494)	grad_norm 0.3582 (0.4005)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:37:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:24 lr 0.000004	 wd 0.0500	time 0.2985 (0.3400)	loss 0.9336 (1.1503)	grad_norm 0.3717 (0.4003)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:37:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:50 lr 0.000004	 wd 0.0500	time 0.3126 (0.3396)	loss 0.9254 (1.1514)	grad_norm 0.3671 (0.3997)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:38:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:16 lr 0.000004	 wd 0.0500	time 0.3382 (0.3392)	loss 1.3055 (1.1528)	grad_norm 0.3696 (0.3993)	loss_scale 4096.0000 (2116.2342)	mem 17019MB
[2024-08-02 13:38:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:42 lr 0.000004	 wd 0.0500	time 0.3075 (0.3392)	loss 1.0817 (1.1526)	grad_norm 0.3880 (0.3993)	loss_scale 4096.0000 (2206.1826)	mem 17019MB
[2024-08-02 13:39:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:08 lr 0.000004	 wd 0.0500	time 0.3289 (0.3391)	loss 1.1423 (1.1527)	grad_norm 0.3653 (0.3989)	loss_scale 4096.0000 (2288.3129)	mem 17019MB
[2024-08-02 13:39:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:34 lr 0.000004	 wd 0.0500	time 0.3428 (0.3388)	loss 1.3400 (1.1537)	grad_norm 0.3979 (0.3990)	loss_scale 4096.0000 (2363.6018)	mem 17019MB
[2024-08-02 13:40:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2916 (0.3378)	loss 1.1043 (1.1533)	grad_norm 0.3738 (0.3985)	loss_scale 4096.0000 (2432.8701)	mem 17019MB
[2024-08-02 13:40:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 22 training takes 0:14:09
[2024-08-02 13:40:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.975 (11.975)	Loss 0.4900 (0.4900)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 13:41:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.198 Acc@5 97.922
[2024-08-02 13:41:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 13:41:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 13:41:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:38:06 lr 0.000004	 wd 0.0500	time 10.9858 (10.9858)	loss 0.7642 (0.7642)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:41:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:17:29 lr 0.000004	 wd 0.0500	time 0.3254 (0.4367)	loss 1.4726 (1.1430)	grad_norm 0.4009 (0.3857)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:42:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:14:30 lr 0.000004	 wd 0.0500	time 0.3447 (0.3781)	loss 0.8471 (1.1474)	grad_norm 0.3840 (0.3848)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:42:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:13:19 lr 0.000004	 wd 0.0500	time 0.3063 (0.3629)	loss 1.4250 (1.1384)	grad_norm 0.3894 (0.3909)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 13:43:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:12:25 lr 0.000004	 wd 0.0500	time 0.2986 (0.3549)	loss 0.8948 (1.1440)	grad_norm 0.3815 (nan)	loss_scale 2048.0000 (3595.4913)	mem 17019MB
[2024-08-02 13:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:39 lr 0.000004	 wd 0.0500	time 0.3173 (0.3493)	loss 0.8078 (1.1471)	grad_norm 0.3638 (nan)	loss_scale 2048.0000 (3286.6108)	mem 17019MB
[2024-08-02 13:44:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:10:58 lr 0.000004	 wd 0.0500	time 0.3134 (0.3461)	loss 1.4279 (1.1463)	grad_norm 0.4109 (nan)	loss_scale 2048.0000 (3080.5191)	mem 17019MB
[2024-08-02 13:45:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:10:21 lr 0.000004	 wd 0.0500	time 0.3004 (0.3451)	loss 1.1316 (1.1496)	grad_norm 0.3931 (nan)	loss_scale 2048.0000 (2933.2268)	mem 17019MB
[2024-08-02 13:45:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:43 lr 0.000003	 wd 0.0500	time 0.3075 (0.3430)	loss 1.4405 (1.1513)	grad_norm 0.3716 (nan)	loss_scale 2048.0000 (2822.7116)	mem 17019MB
[2024-08-02 13:46:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:09:07 lr 0.000003	 wd 0.0500	time 0.3439 (0.3415)	loss 1.1391 (1.1520)	grad_norm 0.3719 (nan)	loss_scale 2048.0000 (2736.7281)	mem 17019MB
[2024-08-02 13:46:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:31 lr 0.000003	 wd 0.0500	time 0.2961 (0.3403)	loss 1.1245 (1.1477)	grad_norm 0.3485 (nan)	loss_scale 2048.0000 (2667.9241)	mem 17019MB
[2024-08-02 13:47:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:58 lr 0.000003	 wd 0.0500	time 0.3325 (0.3410)	loss 1.0348 (1.1505)	grad_norm 0.3895 (nan)	loss_scale 2048.0000 (2611.6185)	mem 17019MB
[2024-08-02 13:47:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:22 lr 0.000003	 wd 0.0500	time 0.2998 (0.3400)	loss 1.3905 (1.1535)	grad_norm 0.4666 (nan)	loss_scale 2048.0000 (2564.6894)	mem 17019MB
[2024-08-02 13:48:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:47 lr 0.000003	 wd 0.0500	time 0.3179 (0.3392)	loss 1.2413 (1.1542)	grad_norm 1.8012 (nan)	loss_scale 2048.0000 (2524.9746)	mem 17019MB
[2024-08-02 13:48:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:06:12 lr 0.000003	 wd 0.0500	time 0.3237 (0.3384)	loss 1.2698 (1.1543)	grad_norm 0.4158 (nan)	loss_scale 2048.0000 (2490.9293)	mem 17019MB
[2024-08-02 13:49:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:39 lr 0.000003	 wd 0.0500	time 0.3468 (0.3392)	loss 1.2841 (1.1528)	grad_norm 0.3893 (nan)	loss_scale 2048.0000 (2461.4204)	mem 17019MB
[2024-08-02 13:50:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:05:05 lr 0.000003	 wd 0.0500	time 0.3495 (0.3390)	loss 1.1378 (1.1561)	grad_norm 0.3642 (nan)	loss_scale 2048.0000 (2435.5978)	mem 17019MB
[2024-08-02 13:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:31 lr 0.000003	 wd 0.0500	time 0.3068 (0.3384)	loss 0.7121 (1.1534)	grad_norm 0.3838 (nan)	loss_scale 2048.0000 (2412.8113)	mem 17019MB
[2024-08-02 13:51:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:57 lr 0.000003	 wd 0.0500	time 0.3202 (0.3382)	loss 1.3074 (1.1526)	grad_norm 0.4076 (nan)	loss_scale 2048.0000 (2392.5552)	mem 17019MB
[2024-08-02 13:51:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:23 lr 0.000003	 wd 0.0500	time 0.3363 (0.3385)	loss 1.2912 (1.1520)	grad_norm 0.3691 (nan)	loss_scale 2048.0000 (2374.4303)	mem 17019MB
[2024-08-02 13:52:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:49 lr 0.000003	 wd 0.0500	time 0.3189 (0.3383)	loss 0.9332 (1.1517)	grad_norm 1.0845 (nan)	loss_scale 2048.0000 (2358.1169)	mem 17019MB
[2024-08-02 13:52:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:15 lr 0.000003	 wd 0.0500	time 0.3227 (0.3382)	loss 1.3920 (1.1520)	grad_norm 0.3974 (nan)	loss_scale 2048.0000 (2343.3565)	mem 17019MB
[2024-08-02 13:53:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:42 lr 0.000003	 wd 0.0500	time 0.3040 (0.3378)	loss 1.3057 (1.1505)	grad_norm 0.3998 (nan)	loss_scale 2048.0000 (2329.9373)	mem 17019MB
[2024-08-02 13:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:08 lr 0.000003	 wd 0.0500	time 0.3286 (0.3381)	loss 1.3641 (1.1506)	grad_norm 0.3671 (nan)	loss_scale 2048.0000 (2317.6845)	mem 17019MB
[2024-08-02 13:54:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:34 lr 0.000003	 wd 0.0500	time 0.3154 (0.3379)	loss 0.8097 (1.1505)	grad_norm 0.3854 (nan)	loss_scale 2048.0000 (2306.4523)	mem 17019MB
[2024-08-02 13:55:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2947 (0.3370)	loss 0.8678 (1.1497)	grad_norm 0.3765 (nan)	loss_scale 2048.0000 (2296.1184)	mem 17019MB
[2024-08-02 13:55:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 23 training takes 0:14:07
[2024-08-02 13:55:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.052 (12.052)	Loss 0.5068 (0.5068)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-02 13:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.212 Acc@5 97.898
[2024-08-02 13:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 13:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 13:55:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:15:57 lr 0.000003	 wd 0.0500	time 11.8936 (11.8936)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:56:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:18:05 lr 0.000003	 wd 0.0500	time 0.2979 (0.4520)	loss 0.9404 (1.1566)	grad_norm 0.3638 (0.3855)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:57:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:14:55 lr 0.000003	 wd 0.0500	time 0.3573 (0.3890)	loss 1.2286 (1.1462)	grad_norm 0.3635 (0.3850)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:57:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:13:32 lr 0.000003	 wd 0.0500	time 0.3395 (0.3690)	loss 1.3569 (1.1604)	grad_norm 0.3796 (0.3861)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:58:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:12:37 lr 0.000003	 wd 0.0500	time 0.3205 (0.3602)	loss 0.7178 (1.1601)	grad_norm 0.3903 (0.3951)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:58:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:11:53 lr 0.000003	 wd 0.0500	time 0.2974 (0.3563)	loss 1.4376 (1.1603)	grad_norm 0.4037 (0.3993)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:59:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:11:09 lr 0.000003	 wd 0.0500	time 0.3221 (0.3520)	loss 0.6807 (1.1555)	grad_norm 0.3755 (0.3996)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 13:59:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:10:27 lr 0.000003	 wd 0.0500	time 0.3072 (0.3484)	loss 1.1949 (1.1541)	grad_norm 0.3770 (0.3980)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:00:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:48 lr 0.000003	 wd 0.0500	time 0.3263 (0.3460)	loss 1.2105 (1.1562)	grad_norm 0.3858 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:00:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:09:14 lr 0.000003	 wd 0.0500	time 0.3450 (0.3461)	loss 0.9333 (1.1600)	grad_norm 0.3647 (0.3957)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:01:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:37 lr 0.000003	 wd 0.0500	time 0.3036 (0.3446)	loss 0.7996 (1.1569)	grad_norm 0.4007 (0.3962)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:02:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:08:02 lr 0.000003	 wd 0.0500	time 0.3031 (0.3443)	loss 0.8692 (1.1563)	grad_norm 0.4393 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:26 lr 0.000002	 wd 0.0500	time 0.3376 (0.3431)	loss 0.8915 (1.1544)	grad_norm 0.3802 (0.3971)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:03:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:52 lr 0.000002	 wd 0.0500	time 0.3008 (0.3435)	loss 0.7670 (1.1514)	grad_norm 0.4055 (0.3960)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:03:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:06:17 lr 0.000002	 wd 0.0500	time 0.3153 (0.3426)	loss 1.4511 (1.1525)	grad_norm 0.3741 (0.3963)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:04:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:42 lr 0.000002	 wd 0.0500	time 0.3472 (0.3419)	loss 1.2400 (1.1486)	grad_norm 0.3962 (0.3961)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:04:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:05:07 lr 0.000002	 wd 0.0500	time 0.3098 (0.3414)	loss 1.2884 (1.1476)	grad_norm 0.3872 (0.3979)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:05:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:33 lr 0.000002	 wd 0.0500	time 0.3322 (0.3414)	loss 1.5016 (1.1490)	grad_norm 0.4766 (0.3987)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:05:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:59 lr 0.000002	 wd 0.0500	time 0.3476 (0.3410)	loss 1.0844 (1.1490)	grad_norm 0.3829 (0.4006)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 14:06:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:24 lr 0.000002	 wd 0.0500	time 0.2955 (0.3405)	loss 0.9111 (1.1503)	grad_norm 0.3682 (0.4005)	loss_scale 4096.0000 (2155.7328)	mem 17019MB
[2024-08-02 14:07:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:50 lr 0.000002	 wd 0.0500	time 0.3015 (0.3400)	loss 1.2791 (1.1502)	grad_norm 0.3928 (0.4003)	loss_scale 4096.0000 (2252.6977)	mem 17019MB
[2024-08-02 14:07:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:16 lr 0.000002	 wd 0.0500	time 0.3263 (0.3401)	loss 1.0006 (1.1505)	grad_norm 0.3928 (0.4003)	loss_scale 4096.0000 (2340.4322)	mem 17019MB
[2024-08-02 14:08:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:42 lr 0.000002	 wd 0.0500	time 0.3487 (0.3396)	loss 1.0135 (1.1521)	grad_norm 0.4028 (0.4001)	loss_scale 4096.0000 (2420.1945)	mem 17019MB
[2024-08-02 14:08:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:08 lr 0.000002	 wd 0.0500	time 0.3398 (0.3391)	loss 1.3820 (1.1545)	grad_norm 0.4100 (0.3997)	loss_scale 4096.0000 (2493.0239)	mem 17019MB
[2024-08-02 14:09:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:34 lr 0.000002	 wd 0.0500	time 0.3264 (0.3387)	loss 1.2176 (1.1524)	grad_norm 0.3615 (0.3997)	loss_scale 4096.0000 (2559.7868)	mem 17019MB
[2024-08-02 14:09:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2930 (0.3378)	loss 0.9274 (1.1529)	grad_norm 0.3597 (0.4000)	loss_scale 4096.0000 (2621.2107)	mem 17019MB
[2024-08-02 14:09:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 24 training takes 0:14:10
[2024-08-02 14:10:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.293 (12.293)	Loss 0.4885 (0.4885)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-02 14:10:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.206 Acc@5 97.900
[2024-08-02 14:10:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 14:10:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 14:10:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:17:11 lr 0.000002	 wd 0.0500	time 11.9231 (11.9231)	loss 1.2640 (1.2640)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:11:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:17:35 lr 0.000002	 wd 0.0500	time 0.3327 (0.4393)	loss 1.1895 (1.2099)	grad_norm 0.3845 (0.3871)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:11:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:14:38 lr 0.000002	 wd 0.0500	time 0.3351 (0.3818)	loss 1.4296 (1.1799)	grad_norm 0.3810 (0.3865)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:12:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:13:28 lr 0.000002	 wd 0.0500	time 0.3256 (0.3671)	loss 1.1274 (1.1634)	grad_norm 0.3800 (0.3891)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:12:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:12:30 lr 0.000002	 wd 0.0500	time 0.2993 (0.3569)	loss 1.2955 (1.1643)	grad_norm 0.3695 (0.3897)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:13:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:11:41 lr 0.000002	 wd 0.0500	time 0.2999 (0.3502)	loss 1.4165 (1.1624)	grad_norm 0.3875 (0.3890)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:13:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:10:59 lr 0.000002	 wd 0.0500	time 0.3111 (0.3466)	loss 0.8212 (1.1524)	grad_norm 0.3790 (0.3895)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:14:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:10:23 lr 0.000002	 wd 0.0500	time 0.3257 (0.3458)	loss 0.7967 (1.1541)	grad_norm 0.3938 (0.3894)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:15:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:45 lr 0.000002	 wd 0.0500	time 0.3039 (0.3440)	loss 1.3495 (1.1532)	grad_norm 0.4062 (0.3901)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:15:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:09:07 lr 0.000002	 wd 0.0500	time 0.2986 (0.3419)	loss 1.4308 (1.1531)	grad_norm 0.3846 (0.3902)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:16:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:31 lr 0.000002	 wd 0.0500	time 0.3037 (0.3407)	loss 1.5066 (1.1544)	grad_norm 0.3914 (0.3907)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:16:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:07:57 lr 0.000002	 wd 0.0500	time 0.3172 (0.3405)	loss 1.5414 (1.1553)	grad_norm 0.3905 (0.3905)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:17:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:22 lr 0.000002	 wd 0.0500	time 0.2967 (0.3401)	loss 0.9664 (1.1518)	grad_norm 0.3680 (0.3941)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:17:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:47 lr 0.000002	 wd 0.0500	time 0.2968 (0.3391)	loss 0.8245 (1.1507)	grad_norm 0.3984 (0.3947)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:18:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:06:12 lr 0.000002	 wd 0.0500	time 0.3098 (0.3383)	loss 0.8005 (1.1502)	grad_norm 0.3967 (0.3956)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:18:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:38 lr 0.000002	 wd 0.0500	time 0.3473 (0.3382)	loss 0.8595 (1.1504)	grad_norm 0.3892 (0.3956)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:05:04 lr 0.000002	 wd 0.0500	time 0.3053 (0.3381)	loss 0.8066 (1.1496)	grad_norm 0.3690 (0.3975)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:20:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:30 lr 0.000002	 wd 0.0500	time 0.3189 (0.3377)	loss 0.7497 (1.1473)	grad_norm 0.3816 (0.3989)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:20:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:56 lr 0.000002	 wd 0.0500	time 0.3287 (0.3372)	loss 0.8755 (1.1492)	grad_norm 0.3873 (0.3988)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:21:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:22 lr 0.000002	 wd 0.0500	time 0.3394 (0.3372)	loss 1.6444 (1.1515)	grad_norm 0.3941 (0.3982)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:21:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:49 lr 0.000002	 wd 0.0500	time 0.3087 (0.3373)	loss 0.9848 (1.1520)	grad_norm 0.3778 (0.3992)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:22:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:15 lr 0.000002	 wd 0.0500	time 0.3046 (0.3370)	loss 1.2752 (1.1532)	grad_norm 0.3715 (0.4006)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:22:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:41 lr 0.000001	 wd 0.0500	time 0.3279 (0.3367)	loss 1.1137 (1.1536)	grad_norm 0.3758 (0.4002)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:23:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3186 (0.3368)	loss 1.3366 (1.1530)	grad_norm 0.3771 (0.3999)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:23:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3640 (0.3372)	loss 1.3488 (1.1521)	grad_norm 0.3459 (0.3996)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:24:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2923 (0.3363)	loss 0.7381 (1.1509)	grad_norm 0.4127 (0.3995)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:24:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 25 training takes 0:14:06
[2024-08-02 14:24:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.952 (11.952)	Loss 0.5117 (0.5117)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17019MB
[2024-08-02 14:25:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.222 Acc@5 97.896
[2024-08-02 14:25:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 14:25:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 14:25:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:31:32 lr 0.000001	 wd 0.0500	time 10.8283 (10.8283)	loss 1.1602 (1.1602)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:25:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:17:43 lr 0.000001	 wd 0.0500	time 0.3048 (0.4428)	loss 1.2992 (1.1641)	grad_norm 0.4087 (0.3873)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:26:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:14:47 lr 0.000001	 wd 0.0500	time 0.3186 (0.3855)	loss 1.0820 (1.1633)	grad_norm 0.3907 (0.3893)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:26:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:13:24 lr 0.000001	 wd 0.0500	time 0.3114 (0.3652)	loss 0.9118 (1.1453)	grad_norm 0.3712 (0.3925)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:27:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:12:26 lr 0.000001	 wd 0.0500	time 0.3053 (0.3552)	loss 1.3415 (1.1437)	grad_norm 0.4056 (0.4003)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:27:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:39 lr 0.000001	 wd 0.0500	time 0.3172 (0.3494)	loss 1.3115 (1.1526)	grad_norm 0.3905 (0.4012)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:28:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:11:00 lr 0.000001	 wd 0.0500	time 0.2986 (0.3475)	loss 0.7840 (1.1566)	grad_norm 0.4341 (0.3989)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:29:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:10:21 lr 0.000001	 wd 0.0500	time 0.3012 (0.3449)	loss 1.3887 (1.1525)	grad_norm 0.3646 (0.3978)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:29:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:42 lr 0.000001	 wd 0.0500	time 0.3165 (0.3421)	loss 1.2325 (1.1565)	grad_norm 0.3928 (0.3975)	loss_scale 8192.0000 (4116.4544)	mem 17019MB
[2024-08-02 14:30:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:09:05 lr 0.000001	 wd 0.0500	time 0.3095 (0.3408)	loss 1.2780 (1.1557)	grad_norm 0.4735 (0.3989)	loss_scale 8192.0000 (4568.7902)	mem 17019MB
[2024-08-02 14:30:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:31 lr 0.000001	 wd 0.0500	time 0.3372 (0.3404)	loss 1.1220 (1.1546)	grad_norm 0.3801 (nan)	loss_scale 4096.0000 (4881.6464)	mem 17019MB
[2024-08-02 14:31:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:56 lr 0.000001	 wd 0.0500	time 0.3028 (0.3397)	loss 0.9256 (1.1545)	grad_norm 0.3850 (nan)	loss_scale 4096.0000 (4810.2888)	mem 17019MB
[2024-08-02 14:31:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:20 lr 0.000001	 wd 0.0500	time 0.3109 (0.3385)	loss 1.0740 (1.1561)	grad_norm 0.3802 (nan)	loss_scale 4096.0000 (4750.8143)	mem 17019MB
[2024-08-02 14:32:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:45 lr 0.000001	 wd 0.0500	time 0.3484 (0.3377)	loss 1.2722 (1.1576)	grad_norm 0.3943 (nan)	loss_scale 4096.0000 (4700.4827)	mem 17019MB
[2024-08-02 14:32:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:06:12 lr 0.000001	 wd 0.0500	time 0.3117 (0.3380)	loss 1.3764 (1.1585)	grad_norm 0.3903 (nan)	loss_scale 4096.0000 (4657.3362)	mem 17019MB
[2024-08-02 14:33:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:38 lr 0.000001	 wd 0.0500	time 0.3387 (0.3375)	loss 1.3997 (1.1601)	grad_norm 0.3695 (nan)	loss_scale 4096.0000 (4619.9387)	mem 17019MB
[2024-08-02 14:34:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:05:03 lr 0.000001	 wd 0.0500	time 0.3127 (0.3370)	loss 1.2051 (1.1575)	grad_norm 0.3726 (nan)	loss_scale 4096.0000 (4587.2130)	mem 17019MB
[2024-08-02 14:34:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:29 lr 0.000001	 wd 0.0500	time 0.3331 (0.3365)	loss 1.2865 (1.1557)	grad_norm 0.3745 (nan)	loss_scale 4096.0000 (4558.3351)	mem 17019MB
[2024-08-02 14:35:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:56 lr 0.000001	 wd 0.0500	time 0.3134 (0.3370)	loss 0.8766 (1.1556)	grad_norm 0.3982 (nan)	loss_scale 4096.0000 (4532.6641)	mem 17019MB
[2024-08-02 14:35:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:22 lr 0.000001	 wd 0.0500	time 0.2993 (0.3371)	loss 1.2909 (1.1567)	grad_norm 0.4004 (nan)	loss_scale 4096.0000 (4509.6938)	mem 17019MB
[2024-08-02 14:36:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.3076 (0.3368)	loss 1.1832 (1.1552)	grad_norm 0.3842 (nan)	loss_scale 4096.0000 (4489.0195)	mem 17019MB
[2024-08-02 14:36:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:15 lr 0.000001	 wd 0.0500	time 0.3097 (0.3365)	loss 1.3671 (1.1541)	grad_norm 0.4096 (nan)	loss_scale 4096.0000 (4470.3132)	mem 17019MB
[2024-08-02 14:37:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:41 lr 0.000001	 wd 0.0500	time 0.6654 (0.3375)	loss 1.2782 (1.1525)	grad_norm 0.3828 (nan)	loss_scale 4096.0000 (4453.3067)	mem 17019MB
[2024-08-02 14:38:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3104 (0.3381)	loss 1.2871 (1.1528)	grad_norm 0.3563 (nan)	loss_scale 4096.0000 (4437.7784)	mem 17019MB
[2024-08-02 14:38:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3058 (0.3379)	loss 0.7133 (1.1532)	grad_norm 0.3791 (nan)	loss_scale 4096.0000 (4423.5435)	mem 17019MB
[2024-08-02 14:39:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2933 (0.3370)	loss 0.9359 (1.1537)	grad_norm 0.4163 (nan)	loss_scale 4096.0000 (4410.4470)	mem 17019MB
[2024-08-02 14:39:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 26 training takes 0:14:06
[2024-08-02 14:39:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.820 (11.820)	Loss 0.4949 (0.4949)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 17019MB
[2024-08-02 14:39:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.200 Acc@5 97.880
[2024-08-02 14:39:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 14:39:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 14:39:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:27:58 lr 0.000001	 wd 0.0500	time 12.1817 (12.1817)	loss 1.3905 (1.3905)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:40:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:17:36 lr 0.000001	 wd 0.0500	time 0.2905 (0.4398)	loss 1.2711 (1.1442)	grad_norm 0.3751 (0.3883)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:41:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:14:41 lr 0.000001	 wd 0.0500	time 0.3093 (0.3828)	loss 1.3710 (1.1566)	grad_norm 0.3905 (0.3890)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:13:19 lr 0.000001	 wd 0.0500	time 0.2956 (0.3629)	loss 1.3371 (1.1560)	grad_norm 0.3736 (0.3882)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:42:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:12:29 lr 0.000001	 wd 0.0500	time 0.3242 (0.3565)	loss 0.9924 (1.1524)	grad_norm 0.3839 (0.3880)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:42:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:46 lr 0.000001	 wd 0.0500	time 0.3351 (0.3528)	loss 0.6681 (1.1537)	grad_norm 0.3987 (0.3877)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:43:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:11:02 lr 0.000001	 wd 0.0500	time 0.3040 (0.3482)	loss 1.3135 (1.1536)	grad_norm 0.3921 (0.3876)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:43:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:10:21 lr 0.000001	 wd 0.0500	time 0.3399 (0.3448)	loss 0.9785 (1.1552)	grad_norm 0.4126 (0.3882)	loss_scale 4096.0000 (4096.0000)	mem 17019MB
[2024-08-02 14:44:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:45 lr 0.000001	 wd 0.0500	time 0.3016 (0.3440)	loss 0.9250 (1.1568)	grad_norm 0.3870 (nan)	loss_scale 2048.0000 (3860.7740)	mem 17019MB
[2024-08-02 14:44:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:09:08 lr 0.000001	 wd 0.0500	time 0.3285 (0.3426)	loss 1.2260 (1.1548)	grad_norm 0.3700 (nan)	loss_scale 2048.0000 (3659.5782)	mem 17019MB
[2024-08-02 14:45:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:32 lr 0.000001	 wd 0.0500	time 0.3223 (0.3410)	loss 1.1138 (1.1516)	grad_norm 0.3467 (nan)	loss_scale 1024.0000 (3451.5245)	mem 17019MB
[2024-08-02 14:45:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:58 lr 0.000001	 wd 0.0500	time 0.3024 (0.3412)	loss 1.1285 (1.1537)	grad_norm 0.3724 (nan)	loss_scale 1024.0000 (3231.0409)	mem 17019MB
[2024-08-02 14:46:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:23 lr 0.000001	 wd 0.0500	time 0.3311 (0.3407)	loss 0.7154 (1.1544)	grad_norm 0.3722 (nan)	loss_scale 1024.0000 (3047.2739)	mem 17019MB
[2024-08-02 14:47:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:48 lr 0.000001	 wd 0.0500	time 0.3023 (0.3401)	loss 0.8090 (1.1512)	grad_norm 0.3984 (nan)	loss_scale 1024.0000 (2891.7571)	mem 17019MB
[2024-08-02 14:47:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:06:13 lr 0.000001	 wd 0.0500	time 0.3081 (0.3390)	loss 0.8271 (1.1525)	grad_norm 0.3654 (nan)	loss_scale 1024.0000 (2758.4411)	mem 17019MB
[2024-08-02 14:48:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:38 lr 0.000001	 wd 0.0500	time 0.2981 (0.3382)	loss 1.4369 (1.1535)	grad_norm 0.3708 (nan)	loss_scale 1024.0000 (2642.8887)	mem 17019MB
[2024-08-02 14:48:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:05:05 lr 0.000001	 wd 0.0500	time 0.3234 (0.3384)	loss 1.3143 (1.1551)	grad_norm 0.3797 (nan)	loss_scale 1024.0000 (2541.7714)	mem 17019MB
[2024-08-02 14:49:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:32 lr 0.000001	 wd 0.0500	time 0.3211 (0.3395)	loss 1.3435 (1.1568)	grad_norm 0.3979 (nan)	loss_scale 1024.0000 (2452.5432)	mem 17019MB
[2024-08-02 14:49:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:58 lr 0.000001	 wd 0.0500	time 0.3370 (0.3392)	loss 1.3962 (1.1565)	grad_norm 0.3730 (nan)	loss_scale 1024.0000 (2373.2238)	mem 17019MB
[2024-08-02 14:50:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:24 lr 0.000001	 wd 0.0500	time 0.3534 (0.3389)	loss 1.3712 (1.1572)	grad_norm 0.3790 (nan)	loss_scale 1024.0000 (2302.2493)	mem 17019MB
[2024-08-02 14:51:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:50 lr 0.000001	 wd 0.0500	time 0.3502 (0.3392)	loss 1.3944 (1.1580)	grad_norm 0.3935 (nan)	loss_scale 1024.0000 (2238.3688)	mem 17019MB
[2024-08-02 14:51:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:16 lr 0.000001	 wd 0.0500	time 0.3137 (0.3394)	loss 1.4919 (1.1570)	grad_norm 0.3962 (nan)	loss_scale 1024.0000 (2180.5693)	mem 17019MB
[2024-08-02 14:52:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:42 lr 0.000001	 wd 0.0500	time 0.3019 (0.3390)	loss 1.1822 (1.1583)	grad_norm 0.3832 (nan)	loss_scale 1024.0000 (2128.0218)	mem 17019MB
[2024-08-02 14:52:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:08 lr 0.000001	 wd 0.0500	time 0.3279 (0.3385)	loss 1.2859 (1.1580)	grad_norm 0.3719 (nan)	loss_scale 1024.0000 (2080.0417)	mem 17019MB
[2024-08-02 14:53:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3490 (0.3384)	loss 1.3480 (1.1597)	grad_norm 0.4130 (nan)	loss_scale 1024.0000 (2036.0583)	mem 17019MB
[2024-08-02 14:53:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2930 (0.3377)	loss 1.2491 (1.1588)	grad_norm 0.3916 (nan)	loss_scale 1024.0000 (1995.5922)	mem 17019MB
[2024-08-02 14:53:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 27 training takes 0:14:09
[2024-08-02 14:54:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.717 (11.717)	Loss 0.4829 (0.4829)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 14:54:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.198 Acc@5 97.900
[2024-08-02 14:54:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 14:54:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 14:54:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:19:14 lr 0.000001	 wd 0.0500	time 10.5333 (10.5333)	loss 0.9605 (0.9605)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:55:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:17:28 lr 0.000000	 wd 0.0500	time 0.2963 (0.4367)	loss 0.9006 (1.1946)	grad_norm 0.3920 (0.3858)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:55:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:14:46 lr 0.000000	 wd 0.0500	time 0.3402 (0.3851)	loss 1.5430 (1.1812)	grad_norm 0.3708 (0.3842)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:13:31 lr 0.000000	 wd 0.0500	time 0.3173 (0.3687)	loss 1.4279 (1.1750)	grad_norm 0.4044 (0.3865)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:56:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:12:32 lr 0.000000	 wd 0.0500	time 0.3164 (0.3581)	loss 0.7276 (1.1663)	grad_norm 0.3622 (0.3868)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:57:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:45 lr 0.000000	 wd 0.0500	time 0.3414 (0.3523)	loss 0.7885 (1.1712)	grad_norm 0.4203 (0.3892)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:57:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:11:06 lr 0.000000	 wd 0.0500	time 0.3154 (0.3504)	loss 0.8802 (1.1637)	grad_norm 0.4051 (0.3883)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:58:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:10:26 lr 0.000000	 wd 0.0500	time 0.3506 (0.3477)	loss 1.2725 (1.1606)	grad_norm 0.3984 (0.3879)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:59:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:47 lr 0.000000	 wd 0.0500	time 0.2983 (0.3450)	loss 0.8322 (1.1611)	grad_norm 0.3999 (0.3877)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 14:59:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:09:11 lr 0.000000	 wd 0.0500	time 0.3552 (0.3440)	loss 0.8483 (1.1575)	grad_norm 0.3873 (0.3885)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:00:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:35 lr 0.000000	 wd 0.0500	time 0.3425 (0.3429)	loss 1.0042 (1.1592)	grad_norm 0.3756 (0.3890)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:00:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:59 lr 0.000000	 wd 0.0500	time 0.2963 (0.3420)	loss 1.2514 (1.1593)	grad_norm 0.3940 (0.3890)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:01:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:24 lr 0.000000	 wd 0.0500	time 0.3232 (0.3410)	loss 1.4192 (1.1592)	grad_norm 0.4260 (0.3893)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:01:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:50 lr 0.000000	 wd 0.0500	time 0.3176 (0.3412)	loss 1.4994 (1.1604)	grad_norm 0.3887 (0.3904)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:02:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:06:15 lr 0.000000	 wd 0.0500	time 0.3397 (0.3407)	loss 0.9480 (1.1598)	grad_norm 0.3788 (0.3911)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:40 lr 0.000000	 wd 0.0500	time 0.2953 (0.3397)	loss 1.1795 (1.1583)	grad_norm 0.3809 (0.3931)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:03:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:05:05 lr 0.000000	 wd 0.0500	time 0.3261 (0.3392)	loss 1.3841 (1.1611)	grad_norm 0.4326 (0.3941)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:04:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:32 lr 0.000000	 wd 0.0500	time 0.3255 (0.3392)	loss 1.2316 (1.1606)	grad_norm 0.3912 (0.4001)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:04:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:57 lr 0.000000	 wd 0.0500	time 0.3076 (0.3388)	loss 0.7888 (1.1601)	grad_norm 0.3778 (0.3997)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:05:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:23 lr 0.000000	 wd 0.0500	time 0.3067 (0.3383)	loss 1.0963 (1.1578)	grad_norm 0.3763 (0.4005)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:05:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:49 lr 0.000000	 wd 0.0500	time 0.3809 (0.3385)	loss 1.1043 (1.1578)	grad_norm 0.8648 (0.4006)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:06:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:16 lr 0.000000	 wd 0.0500	time 0.3103 (0.3383)	loss 0.9717 (1.1577)	grad_norm 0.3973 (0.4002)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:06:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:42 lr 0.000000	 wd 0.0500	time 0.3497 (0.3380)	loss 1.2631 (1.1567)	grad_norm 0.3948 (0.4003)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:07:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:08 lr 0.000000	 wd 0.0500	time 0.3379 (0.3377)	loss 1.2339 (1.1567)	grad_norm 0.3942 (0.3998)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:07:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3524 (0.3378)	loss 1.5001 (1.1554)	grad_norm 0.3659 (0.3993)	loss_scale 1024.0000 (1024.0000)	mem 17019MB
[2024-08-02 15:08:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.3689 (0.3371)	loss 1.0710 (1.1546)	grad_norm 0.3635 (0.3989)	loss_scale 2048.0000 (1043.6529)	mem 17019MB
[2024-08-02 15:08:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 28 training takes 0:14:09
[2024-08-02 15:08:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.149 (12.149)	Loss 0.4902 (0.4902)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 15:09:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.210 Acc@5 97.928
[2024-08-02 15:09:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 15:09:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 15:09:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:52:08 lr 0.000000	 wd 0.0500	time 11.3224 (11.3224)	loss 1.2406 (1.2406)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:09:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:17:39 lr 0.000000	 wd 0.0500	time 0.3073 (0.4413)	loss 1.4070 (1.1661)	grad_norm 0.3863 (0.3978)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:10:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:14:50 lr 0.000000	 wd 0.0500	time 0.3520 (0.3868)	loss 0.8154 (1.1643)	grad_norm 0.4086 (0.4159)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:10:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:13:28 lr 0.000000	 wd 0.0500	time 0.3200 (0.3671)	loss 0.8694 (1.1610)	grad_norm 0.3878 (0.4156)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:11:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:12:30 lr 0.000000	 wd 0.0500	time 0.3321 (0.3569)	loss 1.1198 (1.1548)	grad_norm 0.3916 (0.4121)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:12:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:11:45 lr 0.000000	 wd 0.0500	time 0.3150 (0.3523)	loss 1.2967 (1.1503)	grad_norm 0.4246 (0.4097)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:12:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:11:04 lr 0.000000	 wd 0.0500	time 0.3033 (0.3494)	loss 0.7186 (1.1533)	grad_norm 0.3814 (0.4084)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:10:24 lr 0.000000	 wd 0.0500	time 0.2894 (0.3463)	loss 1.1235 (1.1540)	grad_norm 0.4149 (0.4055)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:13:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:44 lr 0.000000	 wd 0.0500	time 0.3060 (0.3434)	loss 0.7584 (1.1548)	grad_norm 0.3693 (0.4036)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:14:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:09:08 lr 0.000000	 wd 0.0500	time 0.3180 (0.3422)	loss 1.4928 (1.1561)	grad_norm 0.3903 (0.4057)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:14:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:33 lr 0.000000	 wd 0.0500	time 0.3575 (0.3416)	loss 1.4444 (1.1569)	grad_norm 0.3886 (0.4037)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:15:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:57 lr 0.000000	 wd 0.0500	time 0.3044 (0.3407)	loss 1.2327 (1.1568)	grad_norm 0.3665 (0.4027)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:15:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:21 lr 0.000000	 wd 0.0500	time 0.3014 (0.3392)	loss 1.0222 (1.1545)	grad_norm 0.3610 (0.4011)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:16:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:47 lr 0.000000	 wd 0.0500	time 0.3343 (0.3392)	loss 1.1278 (1.1579)	grad_norm 0.4389 (0.4004)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:17:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:06:13 lr 0.000000	 wd 0.0500	time 0.3247 (0.3389)	loss 1.1175 (1.1529)	grad_norm 0.3992 (0.3997)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:38 lr 0.000000	 wd 0.0500	time 0.3250 (0.3383)	loss 1.0194 (1.1532)	grad_norm 0.3815 (0.3990)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:18:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:05:04 lr 0.000000	 wd 0.0500	time 0.2885 (0.3378)	loss 0.8189 (1.1553)	grad_norm 0.3738 (0.3996)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:18:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:31 lr 0.000000	 wd 0.0500	time 0.3232 (0.3380)	loss 1.3515 (1.1579)	grad_norm 0.3804 (0.3988)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:19:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:57 lr 0.000000	 wd 0.0500	time 0.3481 (0.3379)	loss 1.1697 (1.1564)	grad_norm 0.3722 (0.3987)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:24 lr 0.000000	 wd 0.0500	time 0.3274 (0.3389)	loss 0.7506 (1.1566)	grad_norm 0.3914 (0.3991)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:20:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:50 lr 0.000000	 wd 0.0500	time 0.3446 (0.3387)	loss 0.9056 (1.1584)	grad_norm 0.3878 (0.3988)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:20:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:16 lr 0.000000	 wd 0.0500	time 0.3540 (0.3391)	loss 0.7566 (1.1593)	grad_norm 0.3542 (0.3989)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:21:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:42 lr 0.000000	 wd 0.0500	time 0.3058 (0.3389)	loss 0.8306 (1.1599)	grad_norm 0.4034 (0.3983)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:22:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:08 lr 0.000000	 wd 0.0500	time 0.3622 (0.3387)	loss 0.8510 (1.1570)	grad_norm 0.3953 (0.3980)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:22:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3238 (0.3383)	loss 1.2513 (1.1571)	grad_norm 0.3962 (0.3981)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2930 (0.3374)	loss 1.3153 (1.1564)	grad_norm 0.3677 (0.3978)	loss_scale 2048.0000 (2048.0000)	mem 17019MB
[2024-08-02 15:23:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 249): INFO EPOCH 29 training takes 0:14:09
[2024-08-02 15:23:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_29.pth saving......
[2024-08-02 15:23:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft/diffusion_ft_adapter_smt_l_sequence_stage3/ckpt_epoch_29.pth saved !!!
[2024-08-02 15:23:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.286 (11.286)	Loss 0.4802 (0.4802)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17019MB
[2024-08-02 15:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 296): INFO  * Acc@1 86.206 Acc@5 97.910
[2024-08-02 15:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-02 15:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 182): INFO Max accuracy: 86.27%
[2024-08-02 15:23:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage3-full-ft] (main.py 189): INFO Training time 7:17:37
