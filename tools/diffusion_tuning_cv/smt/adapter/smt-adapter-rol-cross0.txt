[2024-07-29 19:29:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/config.json
[2024-07-29 19:29:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/smt-l/smt_large_224_22k.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_part0
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_sequence_cross0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-29 19:29:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_sequence_cross0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/smt-l/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_sequence_cross0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-29 19:29:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0
[2024-07-29 19:29:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-29 19:29:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 113): INFO number of params: 2466664
[2024-07-29 19:29:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0, ignoring auto resume
[2024-07-29 19:29:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/smt-l/smt_large_224_22k.pth for fine-tuning......
[2024-07-29 19:29:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-29 19:29:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['block1.0.adapter.down.weight', 'block1.0.adapter.down.bias', 'block1.0.adapter.up.weight', 'block1.0.adapter.up.bias', 'block1.1.adapter.down.weight', 'block1.1.adapter.down.bias', 'block1.1.adapter.up.weight', 'block1.1.adapter.up.bias', 'block1.2.adapter.down.weight', 'block1.2.adapter.down.bias', 'block1.2.adapter.up.weight', 'block1.2.adapter.up.bias', 'block1.3.adapter.down.weight', 'block1.3.adapter.down.bias', 'block1.3.adapter.up.weight', 'block1.3.adapter.up.bias', 'block2.0.adapter.down.weight', 'block2.0.adapter.down.bias', 'block2.0.adapter.up.weight', 'block2.0.adapter.up.bias', 'block2.1.adapter.down.weight', 'block2.1.adapter.down.bias', 'block2.1.adapter.up.weight', 'block2.1.adapter.up.bias', 'block2.2.adapter.down.weight', 'block2.2.adapter.down.bias', 'block2.2.adapter.up.weight', 'block2.2.adapter.up.bias', 'block2.3.adapter.down.weight', 'block2.3.adapter.down.bias', 'block2.3.adapter.up.weight', 'block2.3.adapter.up.bias', 'block2.4.adapter.down.weight', 'block2.4.adapter.down.bias', 'block2.4.adapter.up.weight', 'block2.4.adapter.up.bias', 'block2.5.adapter.down.weight', 'block2.5.adapter.down.bias', 'block2.5.adapter.up.weight', 'block2.5.adapter.up.bias', 'block3.0.adapter.down.weight', 'block3.0.adapter.down.bias', 'block3.0.adapter.up.weight', 'block3.0.adapter.up.bias', 'block3.1.adapter.down.weight', 'block3.1.adapter.down.bias', 'block3.1.adapter.up.weight', 'block3.1.adapter.up.bias', 'block3.2.adapter.down.weight', 'block3.2.adapter.down.bias', 'block3.2.adapter.up.weight', 'block3.2.adapter.up.bias', 'block3.3.adapter.down.weight', 'block3.3.adapter.down.bias', 'block3.3.adapter.up.weight', 'block3.3.adapter.up.bias', 'block3.4.adapter.down.weight', 'block3.4.adapter.down.bias', 'block3.4.adapter.up.weight', 'block3.4.adapter.up.bias', 'block3.5.adapter.down.weight', 'block3.5.adapter.down.bias', 'block3.5.adapter.up.weight', 'block3.5.adapter.up.bias', 'block3.6.adapter.down.weight', 'block3.6.adapter.down.bias', 'block3.6.adapter.up.weight', 'block3.6.adapter.up.bias', 'block3.7.adapter.down.weight', 'block3.7.adapter.down.bias', 'block3.7.adapter.up.weight', 'block3.7.adapter.up.bias', 'block3.8.adapter.down.weight', 'block3.8.adapter.down.bias', 'block3.8.adapter.up.weight', 'block3.8.adapter.up.bias', 'block3.9.adapter.down.weight', 'block3.9.adapter.down.bias', 'block3.9.adapter.up.weight', 'block3.9.adapter.up.bias', 'block3.10.adapter.down.weight', 'block3.10.adapter.down.bias', 'block3.10.adapter.up.weight', 'block3.10.adapter.up.bias', 'block3.11.adapter.down.weight', 'block3.11.adapter.down.bias', 'block3.11.adapter.up.weight', 'block3.11.adapter.up.bias', 'block3.12.adapter.down.weight', 'block3.12.adapter.down.bias', 'block3.12.adapter.up.weight', 'block3.12.adapter.up.bias', 'block3.13.adapter.down.weight', 'block3.13.adapter.down.bias', 'block3.13.adapter.up.weight', 'block3.13.adapter.up.bias', 'block3.14.adapter.down.weight', 'block3.14.adapter.down.bias', 'block3.14.adapter.up.weight', 'block3.14.adapter.up.bias', 'block3.15.adapter.down.weight', 'block3.15.adapter.down.bias', 'block3.15.adapter.up.weight', 'block3.15.adapter.up.bias', 'block3.16.adapter.down.weight', 'block3.16.adapter.down.bias', 'block3.16.adapter.up.weight', 'block3.16.adapter.up.bias', 'block3.17.adapter.down.weight', 'block3.17.adapter.down.bias', 'block3.17.adapter.up.weight', 'block3.17.adapter.up.bias', 'block3.18.adapter.down.weight', 'block3.18.adapter.down.bias', 'block3.18.adapter.up.weight', 'block3.18.adapter.up.bias', 'block3.19.adapter.down.weight', 'block3.19.adapter.down.bias', 'block3.19.adapter.up.weight', 'block3.19.adapter.up.bias', 'block3.20.adapter.down.weight', 'block3.20.adapter.down.bias', 'block3.20.adapter.up.weight', 'block3.20.adapter.up.bias', 'block3.21.adapter.down.weight', 'block3.21.adapter.down.bias', 'block3.21.adapter.up.weight', 'block3.21.adapter.up.bias', 'block3.22.adapter.down.weight', 'block3.22.adapter.down.bias', 'block3.22.adapter.up.weight', 'block3.22.adapter.up.bias', 'block3.23.adapter.down.weight', 'block3.23.adapter.down.bias', 'block3.23.adapter.up.weight', 'block3.23.adapter.up.bias', 'block3.24.adapter.down.weight', 'block3.24.adapter.down.bias', 'block3.24.adapter.up.weight', 'block3.24.adapter.up.bias', 'block3.25.adapter.down.weight', 'block3.25.adapter.down.bias', 'block3.25.adapter.up.weight', 'block3.25.adapter.up.bias', 'block3.26.adapter.down.weight', 'block3.26.adapter.down.bias', 'block3.26.adapter.up.weight', 'block3.26.adapter.up.bias', 'block3.27.adapter.down.weight', 'block3.27.adapter.down.bias', 'block3.27.adapter.up.weight', 'block3.27.adapter.up.bias', 'block4.0.adapter.down.weight', 'block4.0.adapter.down.bias', 'block4.0.adapter.up.weight', 'block4.0.adapter.up.bias', 'block4.1.adapter.down.weight', 'block4.1.adapter.down.bias', 'block4.1.adapter.up.weight', 'block4.1.adapter.up.bias', 'block4.2.adapter.down.weight', 'block4.2.adapter.down.bias', 'block4.2.adapter.up.weight', 'block4.2.adapter.up.bias', 'block4.3.adapter.down.weight', 'block4.3.adapter.down.bias', 'block4.3.adapter.up.weight', 'block4.3.adapter.up.bias'], unexpected_keys=[])
[2024-07-29 19:29:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/smt-l/smt_large_224_22k.pth'
[2024-07-29 19:29:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 14.495 (14.495)	Loss 0.4038 (0.4038)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 2331MB
[2024-07-29 19:30:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 84.484 Acc@5 97.124
[2024-07-29 19:30:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-29 19:30:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 168): INFO Start training
[2024-07-29 19:30:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:35:03 lr 0.000000	 wd 0.0500	time 12.3514 (12.3514)	loss 1.6623 (1.6623)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 16682MB
[2024-07-29 19:30:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:18 lr 0.000000	 wd 0.0500	time 0.3494 (0.4323)	loss 1.4311 (1.3244)	grad_norm 0.4052 (nan)	loss_scale 8192.0000 (14112.9505)	mem 16682MB
[2024-07-29 19:31:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:21 lr 0.000000	 wd 0.0500	time 0.2876 (0.3743)	loss 1.1038 (1.3133)	grad_norm 0.4090 (nan)	loss_scale 8192.0000 (11167.2040)	mem 16682MB
[2024-07-29 19:31:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:03 lr 0.000000	 wd 0.0500	time 0.3145 (0.3557)	loss 0.9624 (1.2709)	grad_norm 1.3592 (nan)	loss_scale 8192.0000 (10178.7641)	mem 16682MB
[2024-07-29 19:32:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:12:09 lr 0.000001	 wd 0.0500	time 0.2956 (0.3470)	loss 1.0346 (1.2786)	grad_norm 0.4305 (nan)	loss_scale 4096.0000 (8764.0100)	mem 16682MB
[2024-07-29 19:33:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:24 lr 0.000001	 wd 0.0500	time 0.2975 (0.3419)	loss 1.2567 (1.2836)	grad_norm 0.3760 (nan)	loss_scale 4096.0000 (7832.2715)	mem 16682MB
[2024-07-29 19:33:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:10:43 lr 0.000001	 wd 0.0500	time 0.3256 (0.3385)	loss 1.4442 (1.2831)	grad_norm 0.3912 (nan)	loss_scale 4096.0000 (7210.5957)	mem 16682MB
[2024-07-29 19:34:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:10:04 lr 0.000001	 wd 0.0500	time 0.3045 (0.3356)	loss 1.3699 (1.2853)	grad_norm 0.4388 (nan)	loss_scale 4096.0000 (6766.2882)	mem 16682MB
[2024-07-29 19:34:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:28 lr 0.000001	 wd 0.0500	time 0.3146 (0.3339)	loss 1.1763 (1.2841)	grad_norm 0.4732 (nan)	loss_scale 4096.0000 (6432.9189)	mem 16682MB
[2024-07-29 19:35:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:08:51 lr 0.000001	 wd 0.0500	time 0.3209 (0.3321)	loss 1.4731 (1.2831)	grad_norm 0.4034 (nan)	loss_scale 4096.0000 (6173.5494)	mem 16682MB
[2024-07-29 19:35:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:15 lr 0.000002	 wd 0.0500	time 0.3066 (0.3302)	loss 1.5933 (1.2808)	grad_norm 0.4047 (nan)	loss_scale 2048.0000 (5847.3367)	mem 16682MB
[2024-07-29 19:36:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:41 lr 0.000002	 wd 0.0500	time 0.3340 (0.3292)	loss 1.3682 (1.2818)	grad_norm 0.3979 (nan)	loss_scale 2048.0000 (5502.2561)	mem 16682MB
[2024-07-29 19:36:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:09 lr 0.000002	 wd 0.0500	time 0.3348 (0.3299)	loss 0.9720 (1.2815)	grad_norm 0.3962 (nan)	loss_scale 2048.0000 (5214.6411)	mem 16682MB
[2024-07-29 19:37:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:34 lr 0.000002	 wd 0.0500	time 0.3046 (0.3286)	loss 1.2997 (1.2828)	grad_norm 0.3893 (nan)	loss_scale 2048.0000 (4971.2406)	mem 16682MB
[2024-07-29 19:37:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:01 lr 0.000002	 wd 0.0500	time 0.3195 (0.3284)	loss 1.5656 (1.2846)	grad_norm 0.4357 (nan)	loss_scale 2048.0000 (4762.5867)	mem 16682MB
[2024-07-29 19:38:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:28 lr 0.000002	 wd 0.0500	time 0.3253 (0.3274)	loss 1.3160 (1.2884)	grad_norm 0.4049 (nan)	loss_scale 2048.0000 (4581.7348)	mem 16682MB
[2024-07-29 19:38:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:04:55 lr 0.000003	 wd 0.0500	time 0.2873 (0.3273)	loss 1.3092 (1.2850)	grad_norm 0.3829 (nan)	loss_scale 1024.0000 (4387.6577)	mem 16682MB
[2024-07-29 19:39:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:21 lr 0.000003	 wd 0.0500	time 0.3142 (0.3265)	loss 0.9650 (1.2840)	grad_norm 0.4042 (nan)	loss_scale 1024.0000 (4189.9118)	mem 16682MB
[2024-07-29 19:40:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:49 lr 0.000003	 wd 0.0500	time 0.2917 (0.3264)	loss 1.2141 (1.2835)	grad_norm 0.4010 (nan)	loss_scale 1024.0000 (4014.1255)	mem 16682MB
[2024-07-29 19:40:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:16 lr 0.000003	 wd 0.0500	time 0.3163 (0.3261)	loss 1.5542 (1.2825)	grad_norm 0.3921 (nan)	loss_scale 1024.0000 (3856.8332)	mem 16682MB
[2024-07-29 19:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:43 lr 0.000003	 wd 0.0500	time 0.3225 (0.3261)	loss 0.9114 (1.2807)	grad_norm 0.3934 (nan)	loss_scale 1024.0000 (3715.2624)	mem 16682MB
[2024-07-29 19:41:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:10 lr 0.000003	 wd 0.0500	time 0.3192 (0.3257)	loss 1.1576 (1.2788)	grad_norm 0.4029 (nan)	loss_scale 1024.0000 (3587.1680)	mem 16682MB
[2024-07-29 19:42:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:38 lr 0.000004	 wd 0.0500	time 0.3203 (0.3254)	loss 1.6166 (1.2778)	grad_norm 0.3879 (nan)	loss_scale 1024.0000 (3470.7133)	mem 16682MB
[2024-07-29 19:42:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:05 lr 0.000004	 wd 0.0500	time 0.2949 (0.3254)	loss 1.4166 (1.2777)	grad_norm 0.4008 (nan)	loss_scale 1024.0000 (3364.3807)	mem 16682MB
[2024-07-29 19:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:33 lr 0.000004	 wd 0.0500	time 0.3184 (0.3250)	loss 1.4430 (1.2769)	grad_norm 0.3812 (nan)	loss_scale 1024.0000 (3266.9055)	mem 16682MB
[2024-07-29 19:43:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2848 (0.3244)	loss 0.9567 (1.2743)	grad_norm 0.3973 (nan)	loss_scale 1024.0000 (3177.2251)	mem 16682MB
[2024-07-29 19:43:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 0 training takes 0:13:34
[2024-07-29 19:43:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_0.pth saving......
[2024-07-29 19:43:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_0.pth saved !!!
[2024-07-29 19:43:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.159 (12.159)	Loss 0.4067 (0.4067)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-29 19:44:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 84.702 Acc@5 97.228
[2024-07-29 19:44:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-29 19:44:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 84.70%
[2024-07-29 19:44:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 19:44:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 19:44:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:33:03 lr 0.000004	 wd 0.0500	time 10.8646 (10.8646)	loss 1.2171 (1.2171)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:44:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:17:02 lr 0.000004	 wd 0.0500	time 0.2932 (0.4255)	loss 1.2605 (1.2411)	grad_norm 0.3907 (0.3894)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:45:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:14:17 lr 0.000004	 wd 0.0500	time 0.2988 (0.3726)	loss 0.8082 (1.2551)	grad_norm 0.3971 (0.3954)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:46:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:12:54 lr 0.000004	 wd 0.0500	time 0.2938 (0.3519)	loss 0.9530 (1.2543)	grad_norm 0.3749 (0.3983)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:46:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:00 lr 0.000005	 wd 0.0500	time 0.3118 (0.3427)	loss 1.0378 (1.2688)	grad_norm 0.3822 (0.3955)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:47:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:11:16 lr 0.000005	 wd 0.0500	time 0.2978 (0.3378)	loss 1.5686 (1.2643)	grad_norm 0.3675 (0.3934)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:47:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:10:35 lr 0.000005	 wd 0.0500	time 0.3014 (0.3340)	loss 1.5888 (1.2611)	grad_norm 0.3648 (0.3919)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:48:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:09:56 lr 0.000005	 wd 0.0500	time 0.3513 (0.3312)	loss 0.9086 (1.2557)	grad_norm 0.3574 (0.3896)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:48:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:22 lr 0.000005	 wd 0.0500	time 0.3079 (0.3306)	loss 1.5226 (1.2537)	grad_norm 0.3467 (0.3881)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:49:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:08:45 lr 0.000005	 wd 0.0500	time 0.2865 (0.3283)	loss 1.2151 (1.2511)	grad_norm 0.3921 (0.3862)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:49:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:10 lr 0.000006	 wd 0.0500	time 0.3251 (0.3264)	loss 1.4191 (1.2482)	grad_norm 0.3808 (0.3853)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:50:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:37 lr 0.000006	 wd 0.0500	time 0.3053 (0.3263)	loss 1.4824 (1.2452)	grad_norm 0.3480 (0.3839)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:50:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:03 lr 0.000006	 wd 0.0500	time 0.3334 (0.3249)	loss 1.0261 (1.2413)	grad_norm 0.3507 (0.3819)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:51:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:29 lr 0.000006	 wd 0.0500	time 0.3257 (0.3242)	loss 1.6103 (1.2409)	grad_norm 0.3675 (0.3815)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:51:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:05:56 lr 0.000006	 wd 0.0500	time 0.3128 (0.3234)	loss 0.9734 (1.2394)	grad_norm 0.3620 (0.3795)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:52:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:23 lr 0.000006	 wd 0.0500	time 0.2938 (0.3232)	loss 1.4973 (1.2389)	grad_norm 0.3387 (0.3773)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:52:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:50 lr 0.000007	 wd 0.0500	time 0.3223 (0.3226)	loss 1.3611 (1.2394)	grad_norm 0.3395 (0.3757)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:53:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:18 lr 0.000007	 wd 0.0500	time 0.3005 (0.3218)	loss 1.0317 (1.2385)	grad_norm 0.3611 (0.3742)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:53:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:45 lr 0.000007	 wd 0.0500	time 0.3104 (0.3216)	loss 1.7845 (1.2370)	grad_norm 0.3474 (0.3739)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:54:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:13 lr 0.000007	 wd 0.0500	time 0.3050 (0.3217)	loss 1.4884 (1.2372)	grad_norm 0.3466 (0.3726)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:54:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:41 lr 0.000007	 wd 0.0500	time 0.2940 (0.3216)	loss 1.4860 (1.2369)	grad_norm 0.3462 (0.3710)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:55:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:09 lr 0.000007	 wd 0.0500	time 0.3002 (0.3213)	loss 1.5581 (1.2382)	grad_norm 0.3987 (0.3697)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:56:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:37 lr 0.000008	 wd 0.0500	time 0.3114 (0.3213)	loss 1.2674 (1.2378)	grad_norm 0.3300 (0.3683)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:56:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.3106 (0.3213)	loss 1.0258 (1.2395)	grad_norm 0.3337 (0.3670)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:57:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.2961 (0.3208)	loss 1.2668 (1.2389)	grad_norm 0.3315 (0.3656)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:57:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2843 (0.3200)	loss 0.9222 (1.2384)	grad_norm 0.3359 (0.3646)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:57:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 1 training takes 0:13:23
[2024-07-29 19:57:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.933 (11.933)	Loss 0.4998 (0.4998)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-29 19:58:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 84.974 Acc@5 97.304
[2024-07-29 19:58:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-29 19:58:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 84.97%
[2024-07-29 19:58:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 19:58:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 19:58:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:26:47 lr 0.000008	 wd 0.0500	time 10.7142 (10.7142)	loss 1.3013 (1.3013)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:58:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:17:23 lr 0.000008	 wd 0.0500	time 0.3261 (0.4345)	loss 1.5122 (1.2263)	grad_norm 0.3306 (0.3346)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:59:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:14 lr 0.000008	 wd 0.0500	time 0.3016 (0.3712)	loss 1.4536 (1.2159)	grad_norm 0.3357 (0.3349)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 19:59:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:12:50 lr 0.000008	 wd 0.0500	time 0.2982 (0.3500)	loss 1.2163 (1.2055)	grad_norm 0.3352 (0.3331)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 20:00:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:11:58 lr 0.000009	 wd 0.0500	time 0.3207 (0.3417)	loss 1.4774 (1.2078)	grad_norm 0.3140 (0.3325)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 20:00:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:11:12 lr 0.000009	 wd 0.0500	time 0.3049 (0.3361)	loss 1.4893 (1.2032)	grad_norm 0.3363 (0.3324)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 20:01:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:10:30 lr 0.000009	 wd 0.0500	time 0.3420 (0.3316)	loss 1.3006 (1.2043)	grad_norm 0.3243 (0.3325)	loss_scale 2048.0000 (1126.2296)	mem 16682MB
[2024-07-29 20:01:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:09:51 lr 0.000009	 wd 0.0500	time 0.2859 (0.3284)	loss 1.3718 (1.2092)	grad_norm 0.3359 (0.3318)	loss_scale 2048.0000 (1257.7233)	mem 16682MB
[2024-07-29 20:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:14 lr 0.000009	 wd 0.0500	time 0.2922 (0.3258)	loss 1.4381 (1.2123)	grad_norm 0.3219 (0.3320)	loss_scale 2048.0000 (1356.3845)	mem 16682MB
[2024-07-29 20:02:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:08:39 lr 0.000009	 wd 0.0500	time 0.2931 (0.3240)	loss 0.9604 (1.2172)	grad_norm 0.5009 (0.3327)	loss_scale 2048.0000 (1433.1454)	mem 16682MB
[2024-07-29 20:03:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:08:04 lr 0.000010	 wd 0.0500	time 0.3119 (0.3223)	loss 0.8978 (1.2142)	grad_norm 0.3359 (0.3324)	loss_scale 2048.0000 (1494.5694)	mem 16682MB
[2024-07-29 20:04:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:30 lr 0.000010	 wd 0.0500	time 0.3164 (0.3214)	loss 1.4136 (1.2114)	grad_norm 0.3436 (0.3328)	loss_scale 2048.0000 (1544.8356)	mem 16682MB
[2024-07-29 20:04:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:06:58 lr 0.000010	 wd 0.0500	time 0.3019 (0.3213)	loss 0.9203 (1.2116)	grad_norm 0.3456 (0.3347)	loss_scale 2048.0000 (1586.7311)	mem 16682MB
[2024-07-29 20:05:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:26 lr 0.000010	 wd 0.0500	time 0.3332 (0.3213)	loss 1.4565 (1.2124)	grad_norm 0.3405 (0.3347)	loss_scale 2048.0000 (1622.1860)	mem 16682MB
[2024-07-29 20:05:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:05:53 lr 0.000010	 wd 0.0500	time 0.2864 (0.3209)	loss 1.2663 (1.2140)	grad_norm 0.3149 (0.3342)	loss_scale 2048.0000 (1652.5796)	mem 16682MB
[2024-07-29 20:06:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:20 lr 0.000010	 wd 0.0500	time 0.3204 (0.3204)	loss 1.3920 (1.2144)	grad_norm 0.3410 (0.3344)	loss_scale 2048.0000 (1678.9234)	mem 16682MB
[2024-07-29 20:06:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:04:48 lr 0.000011	 wd 0.0500	time 0.3387 (0.3201)	loss 1.0209 (1.2145)	grad_norm 0.3179 (0.3341)	loss_scale 2048.0000 (1701.9763)	mem 16682MB
[2024-07-29 20:07:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:16 lr 0.000011	 wd 0.0500	time 0.3135 (0.3195)	loss 1.3229 (1.2147)	grad_norm 0.3207 (0.3338)	loss_scale 2048.0000 (1722.3186)	mem 16682MB
[2024-07-29 20:07:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:44 lr 0.000011	 wd 0.0500	time 0.3257 (0.3192)	loss 1.1176 (1.2153)	grad_norm 0.3251 (0.3335)	loss_scale 2048.0000 (1740.4020)	mem 16682MB
[2024-07-29 20:08:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:12 lr 0.000011	 wd 0.0500	time 0.3094 (0.3195)	loss 1.3792 (1.2149)	grad_norm 0.3555 (0.3337)	loss_scale 2048.0000 (1756.5829)	mem 16682MB
[2024-07-29 20:08:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:40 lr 0.000011	 wd 0.0500	time 0.3230 (0.3203)	loss 1.1472 (1.2148)	grad_norm 0.3180 (0.3333)	loss_scale 2048.0000 (1771.1464)	mem 16682MB
[2024-07-29 20:09:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:08 lr 0.000011	 wd 0.0500	time 0.3134 (0.3203)	loss 1.0484 (1.2131)	grad_norm 0.3301 (0.3334)	loss_scale 2048.0000 (1784.3237)	mem 16682MB
[2024-07-29 20:09:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:36 lr 0.000012	 wd 0.0500	time 0.3365 (0.3202)	loss 1.4224 (1.2126)	grad_norm 0.3333 (0.3331)	loss_scale 2048.0000 (1796.3035)	mem 16682MB
[2024-07-29 20:10:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:04 lr 0.000012	 wd 0.0500	time 0.2860 (0.3201)	loss 1.2605 (1.2126)	grad_norm 0.3198 (0.3328)	loss_scale 2048.0000 (1807.2421)	mem 16682MB
[2024-07-29 20:10:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:32 lr 0.000012	 wd 0.0500	time 0.3162 (0.3199)	loss 0.8076 (1.2132)	grad_norm 0.3224 (0.3331)	loss_scale 2048.0000 (1817.2695)	mem 16682MB
[2024-07-29 20:11:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2849 (0.3192)	loss 1.2386 (1.2133)	grad_norm 0.3046 (0.3338)	loss_scale 2048.0000 (1826.4950)	mem 16682MB
[2024-07-29 20:11:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 2 training takes 0:13:21
[2024-07-29 20:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.931 (11.931)	Loss 0.4851 (0.4851)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 20:11:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.300 Acc@5 97.402
[2024-07-29 20:11:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-29 20:11:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.30%
[2024-07-29 20:11:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:11:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:12:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:44:49 lr 0.000012	 wd 0.0500	time 11.1467 (11.1467)	loss 0.7621 (0.7621)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:12:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:16:36 lr 0.000012	 wd 0.0500	time 0.2980 (0.4147)	loss 1.2722 (1.2469)	grad_norm 0.3273 (0.3291)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:13:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:13:53 lr 0.000012	 wd 0.0500	time 0.3290 (0.3622)	loss 1.5570 (1.2195)	grad_norm 0.3277 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:13:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:12:37 lr 0.000012	 wd 0.0500	time 0.2875 (0.3440)	loss 1.4838 (1.2157)	grad_norm 0.3118 (0.3320)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:14:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:11:44 lr 0.000013	 wd 0.0500	time 0.3092 (0.3354)	loss 1.4605 (1.2072)	grad_norm 0.3228 (0.3322)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:14:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:02 lr 0.000013	 wd 0.0500	time 0.3082 (0.3310)	loss 1.0089 (1.2075)	grad_norm 0.3123 (0.3311)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:15:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:10:27 lr 0.000013	 wd 0.0500	time 0.3262 (0.3299)	loss 1.0705 (1.2017)	grad_norm 0.3307 (0.3309)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:15:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:09:50 lr 0.000013	 wd 0.0500	time 0.3024 (0.3279)	loss 1.5781 (1.1990)	grad_norm 0.3193 (0.3308)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:16:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:15 lr 0.000013	 wd 0.0500	time 0.3498 (0.3266)	loss 0.8043 (1.1946)	grad_norm 0.3328 (0.3306)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:16:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:08:40 lr 0.000013	 wd 0.0500	time 0.3178 (0.3251)	loss 1.5910 (1.1967)	grad_norm 0.3302 (0.3296)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:17:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:06 lr 0.000014	 wd 0.0500	time 0.3400 (0.3240)	loss 1.3281 (1.1974)	grad_norm 0.3011 (0.3292)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:17:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:32 lr 0.000014	 wd 0.0500	time 0.3084 (0.3231)	loss 1.0072 (1.1978)	grad_norm 0.3369 (0.3290)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:18:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:07:01 lr 0.000014	 wd 0.0500	time 0.3066 (0.3235)	loss 1.1810 (1.1974)	grad_norm 0.3197 (0.3289)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:18:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:28 lr 0.000014	 wd 0.0500	time 0.3069 (0.3231)	loss 1.3583 (1.1968)	grad_norm 0.3166 (0.3290)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:19:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:05:55 lr 0.000014	 wd 0.0500	time 0.2885 (0.3224)	loss 1.1399 (1.1977)	grad_norm 0.3090 (0.3286)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:20:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:22 lr 0.000014	 wd 0.0500	time 0.3379 (0.3218)	loss 1.4513 (1.1959)	grad_norm 0.3191 (0.3284)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:20:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:49 lr 0.000015	 wd 0.0500	time 0.2918 (0.3214)	loss 0.8101 (1.1942)	grad_norm 0.3266 (0.3282)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:21:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:17 lr 0.000015	 wd 0.0500	time 0.2956 (0.3217)	loss 1.0124 (1.1967)	grad_norm 0.3090 (0.3280)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:21:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:45 lr 0.000015	 wd 0.0500	time 0.3113 (0.3217)	loss 1.1890 (1.1977)	grad_norm 0.3367 (0.3278)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:22:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:13 lr 0.000015	 wd 0.0500	time 0.2966 (0.3212)	loss 1.3089 (1.1968)	grad_norm 0.3168 (0.3278)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:22:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:41 lr 0.000015	 wd 0.0500	time 0.3046 (0.3213)	loss 1.3535 (1.1971)	grad_norm 0.3102 (0.3275)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:23:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:09 lr 0.000015	 wd 0.0500	time 0.3102 (0.3210)	loss 1.0035 (1.1975)	grad_norm 0.3289 (0.3274)	loss_scale 4096.0000 (2108.4360)	mem 16682MB
[2024-07-29 20:23:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:36 lr 0.000016	 wd 0.0500	time 0.2939 (0.3209)	loss 0.8308 (1.1965)	grad_norm 0.3166 (0.3275)	loss_scale 4096.0000 (2198.7388)	mem 16682MB
[2024-07-29 20:24:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:04 lr 0.000016	 wd 0.0500	time 0.3141 (0.3210)	loss 0.9458 (1.1967)	grad_norm 0.3319 (0.3275)	loss_scale 4096.0000 (2281.1925)	mem 16682MB
[2024-07-29 20:24:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:32 lr 0.000016	 wd 0.0500	time 0.2918 (0.3208)	loss 1.4482 (1.1983)	grad_norm 0.3290 (0.3272)	loss_scale 4096.0000 (2356.7780)	mem 16682MB
[2024-07-29 20:25:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2848 (0.3200)	loss 1.3370 (1.1980)	grad_norm 0.3295 (0.3273)	loss_scale 4096.0000 (2426.3191)	mem 16682MB
[2024-07-29 20:25:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 3 training takes 0:13:23
[2024-07-29 20:25:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.166 (11.166)	Loss 0.5059 (0.5059)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 20:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.352 Acc@5 97.532
[2024-07-29 20:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-29 20:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.35%
[2024-07-29 20:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:25:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:25:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:19:45 lr 0.000016	 wd 0.0500	time 10.5458 (10.5458)	loss 1.3005 (1.3005)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:26:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:16:32 lr 0.000016	 wd 0.0500	time 0.3020 (0.4132)	loss 0.9716 (1.2112)	grad_norm 0.3241 (0.3295)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:27:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:14:02 lr 0.000016	 wd 0.0500	time 0.3360 (0.3659)	loss 0.8887 (1.1993)	grad_norm 0.3221 (0.3270)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:27:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:12:48 lr 0.000016	 wd 0.0500	time 0.3185 (0.3490)	loss 0.8124 (1.1952)	grad_norm 0.3191 (0.3255)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:28:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:11:52 lr 0.000017	 wd 0.0500	time 0.3358 (0.3390)	loss 1.3276 (1.1906)	grad_norm 0.3223 (0.3318)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:28:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:11:08 lr 0.000017	 wd 0.0500	time 0.3166 (0.3340)	loss 1.2786 (1.1906)	grad_norm 0.3181 (0.3313)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:29:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:27 lr 0.000017	 wd 0.0500	time 0.2991 (0.3297)	loss 1.1974 (1.1954)	grad_norm 0.3290 (0.3321)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:29:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:09:51 lr 0.000017	 wd 0.0500	time 0.3295 (0.3284)	loss 0.8737 (1.1959)	grad_norm 0.3290 (0.3323)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:30:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:20 lr 0.000017	 wd 0.0500	time 0.3019 (0.3293)	loss 0.8031 (1.1985)	grad_norm 0.3340 (0.3314)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:08:44 lr 0.000017	 wd 0.0500	time 0.2898 (0.3274)	loss 0.8440 (1.1995)	grad_norm 0.3954 (0.3312)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:31:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:08:09 lr 0.000018	 wd 0.0500	time 0.3509 (0.3259)	loss 1.6199 (1.1994)	grad_norm 0.3076 (0.3314)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:31:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:35 lr 0.000018	 wd 0.0500	time 0.3143 (0.3246)	loss 1.4704 (1.2003)	grad_norm 0.3132 (0.3306)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:32:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:01 lr 0.000018	 wd 0.0500	time 0.3307 (0.3241)	loss 1.3068 (1.1989)	grad_norm 0.3282 (0.3300)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:32:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:29 lr 0.000018	 wd 0.0500	time 0.2946 (0.3237)	loss 1.0431 (1.1995)	grad_norm 0.3348 (0.3293)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:05:56 lr 0.000018	 wd 0.0500	time 0.3313 (0.3233)	loss 1.4508 (1.1993)	grad_norm 0.3342 (0.3287)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:23 lr 0.000018	 wd 0.0500	time 0.3430 (0.3226)	loss 1.2277 (1.1996)	grad_norm 0.3170 (0.3286)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:34:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:04:50 lr 0.000019	 wd 0.0500	time 0.3332 (0.3220)	loss 1.5140 (1.1981)	grad_norm 0.3370 (0.3285)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 20:34:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:17 lr 0.000019	 wd 0.0500	time 0.3090 (0.3216)	loss 1.2006 (1.1985)	grad_norm 0.3245 (nan)	loss_scale 2048.0000 (4088.7760)	mem 16682MB
[2024-07-29 20:35:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:45 lr 0.000019	 wd 0.0500	time 0.2869 (0.3218)	loss 1.5666 (1.2006)	grad_norm 0.3266 (nan)	loss_scale 2048.0000 (3975.4625)	mem 16682MB
[2024-07-29 20:36:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:13 lr 0.000019	 wd 0.0500	time 0.3216 (0.3217)	loss 1.4273 (1.2008)	grad_norm 0.3266 (nan)	loss_scale 2048.0000 (3874.0705)	mem 16682MB
[2024-07-29 20:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:41 lr 0.000019	 wd 0.0500	time 0.3170 (0.3215)	loss 0.8711 (1.1985)	grad_norm 0.3259 (nan)	loss_scale 2048.0000 (3782.8126)	mem 16682MB
[2024-07-29 20:37:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:09 lr 0.000019	 wd 0.0500	time 0.2920 (0.3211)	loss 1.0255 (1.1972)	grad_norm 0.3080 (nan)	loss_scale 2048.0000 (3700.2418)	mem 16682MB
[2024-07-29 20:37:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:36 lr 0.000020	 wd 0.0500	time 0.3160 (0.3210)	loss 0.8928 (1.1966)	grad_norm 0.3031 (nan)	loss_scale 2048.0000 (3625.1740)	mem 16682MB
[2024-07-29 20:38:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:04 lr 0.000020	 wd 0.0500	time 0.2819 (0.3207)	loss 0.8153 (1.1959)	grad_norm 0.3239 (nan)	loss_scale 2048.0000 (3556.6310)	mem 16682MB
[2024-07-29 20:38:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.3008 (0.3207)	loss 0.8206 (1.1957)	grad_norm 0.3022 (nan)	loss_scale 2048.0000 (3493.7976)	mem 16682MB
[2024-07-29 20:39:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2849 (0.3200)	loss 1.2699 (1.1962)	grad_norm 0.3319 (nan)	loss_scale 2048.0000 (3435.9888)	mem 16682MB
[2024-07-29 20:39:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 4 training takes 0:13:23
[2024-07-29 20:39:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.896 (11.896)	Loss 0.4983 (0.4983)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 20:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.494 Acc@5 97.608
[2024-07-29 20:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-29 20:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.49%
[2024-07-29 20:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:39:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:39:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:19:23 lr 0.000020	 wd 0.0500	time 10.5369 (10.5369)	loss 1.4089 (1.4089)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:40:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:16:38 lr 0.000020	 wd 0.0500	time 0.3202 (0.4157)	loss 1.0095 (1.2255)	grad_norm 0.3348 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:40:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:14:00 lr 0.000020	 wd 0.0500	time 0.3355 (0.3652)	loss 1.4060 (1.1945)	grad_norm 0.3212 (0.3325)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:41:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:12:46 lr 0.000020	 wd 0.0500	time 0.2864 (0.3479)	loss 1.0300 (1.1838)	grad_norm 0.3124 (0.3293)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:41:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:11:52 lr 0.000020	 wd 0.0500	time 0.3264 (0.3391)	loss 1.2906 (1.1924)	grad_norm 0.3298 (0.3288)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:42:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:08 lr 0.000020	 wd 0.0500	time 0.3062 (0.3337)	loss 1.0473 (1.1908)	grad_norm 0.3189 (0.3291)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:43:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:29 lr 0.000020	 wd 0.0500	time 0.2927 (0.3311)	loss 1.0704 (1.1924)	grad_norm 0.3319 (0.3287)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:43:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:09:52 lr 0.000020	 wd 0.0500	time 0.3359 (0.3286)	loss 0.9333 (1.1906)	grad_norm 0.3090 (0.3290)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:44:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:15 lr 0.000020	 wd 0.0500	time 0.3019 (0.3265)	loss 1.5457 (1.1896)	grad_norm 0.3145 (0.3287)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:44:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:08:40 lr 0.000020	 wd 0.0500	time 0.3226 (0.3252)	loss 1.0877 (1.1882)	grad_norm 0.3148 (0.3284)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:45:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:06 lr 0.000020	 wd 0.0500	time 0.3217 (0.3241)	loss 1.2597 (1.1863)	grad_norm 0.3043 (0.3277)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:45:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:33 lr 0.000020	 wd 0.0500	time 0.3337 (0.3231)	loss 1.1992 (1.1871)	grad_norm 0.3366 (0.3274)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:46:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:06:59 lr 0.000020	 wd 0.0500	time 0.3247 (0.3224)	loss 1.4521 (1.1856)	grad_norm 0.3181 (0.3271)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:46:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:27 lr 0.000020	 wd 0.0500	time 0.3306 (0.3221)	loss 1.0041 (1.1862)	grad_norm 0.3224 (0.3269)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:47:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:05:54 lr 0.000020	 wd 0.0500	time 0.3070 (0.3217)	loss 1.4498 (1.1899)	grad_norm 0.3343 (0.3267)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:47:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:22 lr 0.000020	 wd 0.0500	time 0.3118 (0.3214)	loss 0.8742 (1.1910)	grad_norm 0.3396 (0.3283)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:48:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:50 lr 0.000020	 wd 0.0500	time 0.3300 (0.3218)	loss 1.4518 (1.1935)	grad_norm 0.3276 (0.3282)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:48:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:17 lr 0.000020	 wd 0.0500	time 0.2945 (0.3217)	loss 0.9921 (1.1937)	grad_norm 0.3108 (0.3285)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:49:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:45 lr 0.000020	 wd 0.0500	time 0.3167 (0.3216)	loss 0.7828 (1.1915)	grad_norm 0.3189 (0.3288)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:49:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:13 lr 0.000020	 wd 0.0500	time 0.3237 (0.3215)	loss 1.1447 (1.1910)	grad_norm 0.3202 (0.3284)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:50:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:41 lr 0.000020	 wd 0.0500	time 0.2933 (0.3214)	loss 1.5804 (1.1924)	grad_norm 0.3119 (0.3284)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:50:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:09 lr 0.000020	 wd 0.0500	time 0.3071 (0.3212)	loss 1.5086 (1.1919)	grad_norm 0.3396 (0.3283)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:51:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:36 lr 0.000020	 wd 0.0500	time 0.2954 (0.3211)	loss 1.2465 (1.1930)	grad_norm 0.3189 (0.3283)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:52:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:04 lr 0.000020	 wd 0.0500	time 0.3176 (0.3216)	loss 1.1589 (1.1914)	grad_norm 0.3346 (0.3283)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:52:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.2942 (0.3216)	loss 0.8007 (1.1908)	grad_norm 0.3359 (0.3282)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:53:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2855 (0.3210)	loss 1.3175 (1.1919)	grad_norm 0.3210 (0.3281)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:53:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 5 training takes 0:13:25
[2024-07-29 20:53:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.197 (12.197)	Loss 0.5005 (0.5005)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 20:53:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.626 Acc@5 97.642
[2024-07-29 20:53:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-29 20:53:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.63%
[2024-07-29 20:53:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:53:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:53:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:49:29 lr 0.000020	 wd 0.0500	time 11.2589 (11.2589)	loss 1.2307 (1.2307)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:54:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:16:43 lr 0.000020	 wd 0.0500	time 0.3207 (0.4176)	loss 0.9024 (1.2124)	grad_norm 0.3346 (0.3211)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:54:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:13:56 lr 0.000020	 wd 0.0500	time 0.2873 (0.3632)	loss 0.8769 (1.2021)	grad_norm 0.2944 (0.3244)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:55:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:12:42 lr 0.000020	 wd 0.0500	time 0.2947 (0.3461)	loss 0.9770 (1.1938)	grad_norm 0.3244 (0.3307)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 20:55:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:11:49 lr 0.000020	 wd 0.0500	time 0.2885 (0.3375)	loss 0.8056 (1.1963)	grad_norm 0.3234 (nan)	loss_scale 1024.0000 (1864.1397)	mem 16682MB
[2024-07-29 20:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:07 lr 0.000020	 wd 0.0500	time 0.3062 (0.3332)	loss 1.2366 (1.1965)	grad_norm 0.3153 (nan)	loss_scale 1024.0000 (1696.4471)	mem 16682MB
[2024-07-29 20:56:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:10:30 lr 0.000020	 wd 0.0500	time 0.3689 (0.3313)	loss 0.9081 (1.1878)	grad_norm 0.3212 (nan)	loss_scale 1024.0000 (1584.5591)	mem 16682MB
[2024-07-29 20:57:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:09:53 lr 0.000020	 wd 0.0500	time 0.3437 (0.3294)	loss 1.4942 (1.1904)	grad_norm 0.3008 (nan)	loss_scale 1024.0000 (1504.5934)	mem 16682MB
[2024-07-29 20:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:17 lr 0.000020	 wd 0.0500	time 0.3149 (0.3273)	loss 0.7215 (1.1945)	grad_norm 0.3144 (nan)	loss_scale 1024.0000 (1444.5943)	mem 16682MB
[2024-07-29 20:58:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:08:42 lr 0.000020	 wd 0.0500	time 0.3019 (0.3261)	loss 1.5277 (1.1909)	grad_norm 0.3075 (nan)	loss_scale 1024.0000 (1397.9134)	mem 16682MB
[2024-07-29 20:59:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:08 lr 0.000020	 wd 0.0500	time 0.3665 (0.3255)	loss 1.0531 (1.1905)	grad_norm 0.3103 (nan)	loss_scale 1024.0000 (1360.5594)	mem 16682MB
[2024-07-29 20:59:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:35 lr 0.000020	 wd 0.0500	time 0.3095 (0.3252)	loss 0.8658 (1.1871)	grad_norm 0.3243 (nan)	loss_scale 1024.0000 (1329.9909)	mem 16682MB
[2024-07-29 21:00:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:02 lr 0.000020	 wd 0.0500	time 0.3171 (0.3245)	loss 1.2661 (1.1840)	grad_norm 0.3371 (nan)	loss_scale 1024.0000 (1304.5129)	mem 16682MB
[2024-07-29 21:00:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:28 lr 0.000020	 wd 0.0500	time 0.3270 (0.3236)	loss 1.1218 (1.1826)	grad_norm 0.3145 (nan)	loss_scale 1024.0000 (1282.9516)	mem 16682MB
[2024-07-29 21:01:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:05:56 lr 0.000020	 wd 0.0500	time 0.3279 (0.3231)	loss 0.9650 (1.1828)	grad_norm 0.3274 (nan)	loss_scale 1024.0000 (1264.4682)	mem 16682MB
[2024-07-29 21:01:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:23 lr 0.000020	 wd 0.0500	time 0.3287 (0.3232)	loss 0.7573 (1.1837)	grad_norm 0.3158 (nan)	loss_scale 1024.0000 (1248.4477)	mem 16682MB
[2024-07-29 21:02:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:51 lr 0.000020	 wd 0.0500	time 0.3399 (0.3229)	loss 1.4777 (1.1811)	grad_norm 0.3035 (nan)	loss_scale 1024.0000 (1234.4285)	mem 16682MB
[2024-07-29 21:02:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:18 lr 0.000020	 wd 0.0500	time 0.2852 (0.3226)	loss 1.3938 (1.1804)	grad_norm 0.3180 (nan)	loss_scale 1024.0000 (1222.0576)	mem 16682MB
[2024-07-29 21:03:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:46 lr 0.000020	 wd 0.0500	time 0.3271 (0.3223)	loss 1.2333 (1.1808)	grad_norm 0.3175 (nan)	loss_scale 1024.0000 (1211.0605)	mem 16682MB
[2024-07-29 21:03:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:14 lr 0.000020	 wd 0.0500	time 0.3052 (0.3228)	loss 1.3657 (1.1820)	grad_norm 0.3166 (nan)	loss_scale 1024.0000 (1201.2204)	mem 16682MB
[2024-07-29 21:04:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:41 lr 0.000020	 wd 0.0500	time 0.3021 (0.3225)	loss 0.9933 (1.1828)	grad_norm 0.3248 (nan)	loss_scale 1024.0000 (1192.3638)	mem 16682MB
[2024-07-29 21:04:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:09 lr 0.000020	 wd 0.0500	time 0.2914 (0.3224)	loss 1.0171 (1.1828)	grad_norm 0.3944 (nan)	loss_scale 1024.0000 (1184.3503)	mem 16682MB
[2024-07-29 21:05:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:37 lr 0.000020	 wd 0.0500	time 0.3135 (0.3222)	loss 0.8231 (1.1821)	grad_norm 0.3483 (nan)	loss_scale 1024.0000 (1177.0650)	mem 16682MB
[2024-07-29 21:05:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:05 lr 0.000020	 wd 0.0500	time 0.3148 (0.3222)	loss 1.0471 (1.1847)	grad_norm 0.3402 (nan)	loss_scale 1024.0000 (1170.4129)	mem 16682MB
[2024-07-29 21:06:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.2899 (0.3221)	loss 1.4004 (1.1839)	grad_norm 0.3374 (nan)	loss_scale 1024.0000 (1164.3149)	mem 16682MB
[2024-07-29 21:06:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2846 (0.3214)	loss 1.3605 (1.1835)	grad_norm 0.3160 (nan)	loss_scale 1024.0000 (1158.7045)	mem 16682MB
[2024-07-29 21:07:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 6 training takes 0:13:26
[2024-07-29 21:07:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.216 (12.216)	Loss 0.5127 (0.5127)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 21:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.536 Acc@5 97.662
[2024-07-29 21:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-29 21:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.63%
[2024-07-29 21:07:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:17:05 lr 0.000020	 wd 0.0500	time 11.9205 (11.9205)	loss 0.9598 (0.9598)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:08:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:16:52 lr 0.000020	 wd 0.0500	time 0.3154 (0.4216)	loss 1.2240 (1.1677)	grad_norm 0.3258 (0.3410)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:07 lr 0.000020	 wd 0.0500	time 0.3461 (0.3679)	loss 1.1431 (1.1887)	grad_norm 0.3266 (0.3435)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:09:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:12:48 lr 0.000020	 wd 0.0500	time 0.3286 (0.3491)	loss 1.3109 (1.1836)	grad_norm 0.3231 (0.3377)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:09:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:11:56 lr 0.000020	 wd 0.0500	time 0.3003 (0.3409)	loss 0.8453 (1.1887)	grad_norm 0.3334 (0.3373)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:12 lr 0.000020	 wd 0.0500	time 0.2975 (0.3360)	loss 1.3693 (1.1854)	grad_norm 0.3132 (0.3417)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:10:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:32 lr 0.000020	 wd 0.0500	time 0.2889 (0.3324)	loss 1.3227 (1.1897)	grad_norm 0.3567 (0.3400)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:11:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:09:55 lr 0.000020	 wd 0.0500	time 0.2934 (0.3303)	loss 0.9767 (1.1842)	grad_norm 0.3137 (0.3414)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:11:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:18 lr 0.000020	 wd 0.0500	time 0.2994 (0.3283)	loss 0.8913 (1.1815)	grad_norm 0.3358 (0.3391)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:12:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:08:43 lr 0.000020	 wd 0.0500	time 0.3334 (0.3267)	loss 1.4540 (1.1832)	grad_norm 0.3174 (0.3377)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:12:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:08 lr 0.000020	 wd 0.0500	time 0.2999 (0.3253)	loss 0.8206 (1.1831)	grad_norm 0.3407 (0.3374)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:13:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:34 lr 0.000020	 wd 0.0500	time 0.3047 (0.3244)	loss 1.3796 (1.1822)	grad_norm 0.3206 (0.3368)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:13:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:02 lr 0.000020	 wd 0.0500	time 0.3323 (0.3242)	loss 1.4301 (1.1788)	grad_norm 0.3263 (0.3362)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:14:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:29 lr 0.000020	 wd 0.0500	time 0.3214 (0.3238)	loss 1.3838 (1.1799)	grad_norm 0.3081 (0.3355)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:15:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:05:56 lr 0.000019	 wd 0.0500	time 0.2953 (0.3234)	loss 1.2949 (1.1796)	grad_norm 0.3154 (0.3348)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:15:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:24 lr 0.000019	 wd 0.0500	time 0.3445 (0.3235)	loss 0.8460 (1.1801)	grad_norm 0.3131 (0.3346)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:16:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:51 lr 0.000019	 wd 0.0500	time 0.3170 (0.3232)	loss 1.1259 (1.1807)	grad_norm 0.3444 (0.3339)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:16:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:19 lr 0.000019	 wd 0.0500	time 0.3009 (0.3230)	loss 1.3563 (1.1809)	grad_norm 0.3086 (0.3333)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:17:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:46 lr 0.000019	 wd 0.0500	time 0.3242 (0.3229)	loss 0.7738 (1.1832)	grad_norm 0.3167 (0.3328)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 21:17:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:14 lr 0.000019	 wd 0.0500	time 0.2922 (0.3226)	loss 1.1184 (1.1836)	grad_norm 0.3256 (0.3325)	loss_scale 2048.0000 (1063.8611)	mem 16682MB
[2024-07-29 21:18:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:41 lr 0.000019	 wd 0.0500	time 0.3170 (0.3226)	loss 1.1562 (1.1841)	grad_norm 0.3293 (0.3326)	loss_scale 2048.0000 (1113.0435)	mem 16682MB
[2024-07-29 21:18:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:09 lr 0.000019	 wd 0.0500	time 0.3079 (0.3224)	loss 1.4726 (1.1867)	grad_norm 0.3336 (0.3324)	loss_scale 2048.0000 (1157.5440)	mem 16682MB
[2024-07-29 21:19:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:37 lr 0.000019	 wd 0.0500	time 0.3190 (0.3220)	loss 1.2511 (1.1848)	grad_norm 0.3179 (0.3320)	loss_scale 2048.0000 (1198.0009)	mem 16682MB
[2024-07-29 21:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:05 lr 0.000019	 wd 0.0500	time 0.3299 (0.3218)	loss 1.4158 (1.1839)	grad_norm 0.3316 (0.3318)	loss_scale 2048.0000 (1234.9413)	mem 16682MB
[2024-07-29 21:20:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:32 lr 0.000019	 wd 0.0500	time 0.3335 (0.3216)	loss 1.2681 (1.1843)	grad_norm 0.3195 (0.3316)	loss_scale 2048.0000 (1268.8047)	mem 16682MB
[2024-07-29 21:20:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2847 (0.3208)	loss 1.5985 (1.1844)	grad_norm 0.3040 (0.3314)	loss_scale 2048.0000 (1299.9600)	mem 16682MB
[2024-07-29 21:20:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 7 training takes 0:13:25
[2024-07-29 21:21:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.098 (12.098)	Loss 0.4968 (0.4968)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 21:21:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.624 Acc@5 97.696
[2024-07-29 21:21:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-29 21:21:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.63%
[2024-07-29 21:21:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:36:53 lr 0.000019	 wd 0.0500	time 10.9568 (10.9568)	loss 1.2745 (1.2745)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:22:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:17:12 lr 0.000019	 wd 0.0500	time 0.2870 (0.4297)	loss 0.9895 (1.2261)	grad_norm 0.3086 (0.3351)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:22:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:14:27 lr 0.000019	 wd 0.0500	time 0.2915 (0.3767)	loss 0.8397 (1.2116)	grad_norm 0.8243 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:13:03 lr 0.000019	 wd 0.0500	time 0.3370 (0.3557)	loss 1.4221 (1.2024)	grad_norm 0.3262 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:23:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:07 lr 0.000019	 wd 0.0500	time 0.3220 (0.3462)	loss 0.8501 (1.2010)	grad_norm 0.3300 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:24:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:11:22 lr 0.000019	 wd 0.0500	time 0.2905 (0.3411)	loss 0.8181 (1.1898)	grad_norm 0.3317 (0.3379)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:24:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:10:40 lr 0.000019	 wd 0.0500	time 0.2926 (0.3366)	loss 0.9238 (1.1869)	grad_norm 0.3412 (0.3362)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:25:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:00 lr 0.000019	 wd 0.0500	time 0.3244 (0.3335)	loss 1.3630 (1.1875)	grad_norm 0.3358 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:23 lr 0.000019	 wd 0.0500	time 0.3244 (0.3313)	loss 1.4769 (1.1891)	grad_norm 0.3327 (0.3335)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:26:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:08:47 lr 0.000019	 wd 0.0500	time 0.3297 (0.3295)	loss 0.9595 (1.1841)	grad_norm 0.3256 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:26:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:12 lr 0.000019	 wd 0.0500	time 0.3386 (0.3279)	loss 1.3538 (1.1874)	grad_norm 0.3235 (0.3368)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:27:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:38 lr 0.000019	 wd 0.0500	time 0.2859 (0.3269)	loss 1.2286 (1.1856)	grad_norm 0.3125 (0.3362)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:27:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:04 lr 0.000019	 wd 0.0500	time 0.2992 (0.3258)	loss 1.1366 (1.1851)	grad_norm 0.3370 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:28:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:30 lr 0.000019	 wd 0.0500	time 0.3035 (0.3250)	loss 0.9184 (1.1844)	grad_norm 0.3327 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:28:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:05:57 lr 0.000019	 wd 0.0500	time 0.3398 (0.3245)	loss 1.2891 (1.1853)	grad_norm 0.3266 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:29:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:24 lr 0.000019	 wd 0.0500	time 0.3185 (0.3240)	loss 1.1820 (1.1856)	grad_norm 0.3322 (0.3340)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:30:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:52 lr 0.000019	 wd 0.0500	time 0.3046 (0.3239)	loss 1.3845 (1.1882)	grad_norm 0.3332 (0.3340)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:30:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:19 lr 0.000019	 wd 0.0500	time 0.2950 (0.3238)	loss 0.7806 (1.1907)	grad_norm 0.3192 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:31:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:46 lr 0.000019	 wd 0.0500	time 0.3005 (0.3233)	loss 0.8023 (1.1892)	grad_norm 0.3275 (0.3337)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:14 lr 0.000019	 wd 0.0500	time 0.3148 (0.3230)	loss 1.1929 (1.1910)	grad_norm 0.3180 (0.3336)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:32:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:42 lr 0.000019	 wd 0.0500	time 0.3109 (0.3229)	loss 0.8205 (1.1897)	grad_norm 0.3254 (0.3334)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:32:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:09 lr 0.000019	 wd 0.0500	time 0.2931 (0.3226)	loss 0.8751 (1.1901)	grad_norm 0.3537 (0.3336)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:33:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:37 lr 0.000019	 wd 0.0500	time 0.3156 (0.3223)	loss 1.1317 (1.1885)	grad_norm 0.3417 (0.3332)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:33:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:05 lr 0.000019	 wd 0.0500	time 0.3143 (0.3223)	loss 1.3632 (1.1871)	grad_norm 0.3053 (0.3328)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:34:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:32 lr 0.000019	 wd 0.0500	time 0.3291 (0.3221)	loss 1.1876 (1.1869)	grad_norm 0.3420 (0.3327)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:34:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2849 (0.3214)	loss 0.9124 (1.1867)	grad_norm 0.3085 (0.3325)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:34:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 8 training takes 0:13:26
[2024-07-29 21:35:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.986 (11.986)	Loss 0.4993 (0.4993)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 21:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.650 Acc@5 97.698
[2024-07-29 21:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-29 21:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.65%
[2024-07-29 21:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 21:35:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 21:35:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:06:25 lr 0.000019	 wd 0.0500	time 10.2260 (10.2260)	loss 1.3542 (1.3542)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:36:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:16:39 lr 0.000019	 wd 0.0500	time 0.3061 (0.4161)	loss 1.2889 (1.1643)	grad_norm 0.3266 (0.3293)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:05 lr 0.000019	 wd 0.0500	time 0.3007 (0.3674)	loss 0.9129 (1.1877)	grad_norm 0.3220 (0.3285)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:37:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:12:49 lr 0.000019	 wd 0.0500	time 0.2982 (0.3494)	loss 1.5130 (1.1968)	grad_norm 0.3186 (0.3285)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:37:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:12:01 lr 0.000019	 wd 0.0500	time 0.2992 (0.3434)	loss 0.8498 (1.1796)	grad_norm 0.3319 (0.3282)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:38:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:16 lr 0.000019	 wd 0.0500	time 0.3299 (0.3380)	loss 0.7617 (1.1745)	grad_norm 0.3352 (0.3275)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:38:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:35 lr 0.000019	 wd 0.0500	time 0.2958 (0.3340)	loss 0.8551 (1.1788)	grad_norm 0.3189 (0.3278)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:39:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:09:56 lr 0.000019	 wd 0.0500	time 0.2861 (0.3312)	loss 1.3895 (1.1821)	grad_norm 0.3174 (0.3278)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:39:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:20 lr 0.000019	 wd 0.0500	time 0.3017 (0.3291)	loss 1.3762 (1.1833)	grad_norm 0.3103 (0.3299)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 21:40:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:08:44 lr 0.000019	 wd 0.0500	time 0.2869 (0.3276)	loss 1.3707 (1.1830)	grad_norm 0.3136 (0.3300)	loss_scale 4096.0000 (2225.2963)	mem 16682MB
[2024-07-29 21:40:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:08:09 lr 0.000019	 wd 0.0500	time 0.2881 (0.3260)	loss 1.2010 (1.1838)	grad_norm 0.3134 (0.3295)	loss_scale 4096.0000 (2412.1798)	mem 16682MB
[2024-07-29 21:41:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:35 lr 0.000018	 wd 0.0500	time 0.3054 (0.3249)	loss 1.3406 (1.1796)	grad_norm 0.3281 (0.3294)	loss_scale 4096.0000 (2565.1153)	mem 16682MB
[2024-07-29 21:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:02 lr 0.000018	 wd 0.0500	time 0.3275 (0.3245)	loss 0.8448 (1.1785)	grad_norm 0.3268 (0.3298)	loss_scale 4096.0000 (2692.5828)	mem 16682MB
[2024-07-29 21:42:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:29 lr 0.000018	 wd 0.0500	time 0.3409 (0.3237)	loss 1.2952 (1.1794)	grad_norm 0.3184 (0.3308)	loss_scale 4096.0000 (2800.4550)	mem 16682MB
[2024-07-29 21:42:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:05:56 lr 0.000018	 wd 0.0500	time 0.3063 (0.3232)	loss 1.0830 (1.1786)	grad_norm 0.3192 (0.3310)	loss_scale 4096.0000 (2892.9279)	mem 16682MB
[2024-07-29 21:43:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:23 lr 0.000018	 wd 0.0500	time 0.3266 (0.3232)	loss 0.8799 (1.1818)	grad_norm 0.3305 (0.3309)	loss_scale 4096.0000 (2973.0793)	mem 16682MB
[2024-07-29 21:43:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:04:51 lr 0.000018	 wd 0.0500	time 0.3475 (0.3227)	loss 0.9290 (1.1801)	grad_norm 0.3155 (0.3314)	loss_scale 4096.0000 (3043.2180)	mem 16682MB
[2024-07-29 21:44:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:18 lr 0.000018	 wd 0.0500	time 0.3054 (0.3224)	loss 1.4617 (1.1806)	grad_norm 0.3329 (0.3314)	loss_scale 4096.0000 (3105.1099)	mem 16682MB
[2024-07-29 21:44:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:46 lr 0.000018	 wd 0.0500	time 0.3143 (0.3221)	loss 1.2619 (1.1793)	grad_norm 0.3268 (0.3330)	loss_scale 4096.0000 (3160.1288)	mem 16682MB
[2024-07-29 21:45:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:13 lr 0.000018	 wd 0.0500	time 0.2960 (0.3222)	loss 1.4304 (1.1786)	grad_norm 0.3340 (0.3328)	loss_scale 4096.0000 (3209.3593)	mem 16682MB
[2024-07-29 21:46:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:41 lr 0.000018	 wd 0.0500	time 0.2915 (0.3218)	loss 1.4117 (1.1791)	grad_norm 0.3022 (0.3329)	loss_scale 4096.0000 (3253.6692)	mem 16682MB
[2024-07-29 21:46:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:09 lr 0.000018	 wd 0.0500	time 0.3110 (0.3213)	loss 1.4203 (1.1806)	grad_norm 0.3334 (0.3326)	loss_scale 4096.0000 (3293.7611)	mem 16682MB
[2024-07-29 21:47:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:36 lr 0.000018	 wd 0.0500	time 0.2871 (0.3210)	loss 1.1632 (1.1817)	grad_norm 0.3524 (0.3330)	loss_scale 4096.0000 (3330.2099)	mem 16682MB
[2024-07-29 21:47:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:04 lr 0.000018	 wd 0.0500	time 0.2986 (0.3207)	loss 0.8027 (1.1817)	grad_norm 0.3248 (0.3330)	loss_scale 4096.0000 (3363.4907)	mem 16682MB
[2024-07-29 21:48:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:32 lr 0.000018	 wd 0.0500	time 0.3147 (0.3205)	loss 1.0984 (1.1820)	grad_norm 0.3402 (0.3330)	loss_scale 4096.0000 (3393.9992)	mem 16682MB
[2024-07-29 21:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2846 (0.3197)	loss 1.2676 (1.1814)	grad_norm 0.3422 (0.3329)	loss_scale 4096.0000 (3422.0680)	mem 16682MB
[2024-07-29 21:48:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 9 training takes 0:13:22
[2024-07-29 21:48:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.006 (12.006)	Loss 0.5107 (0.5107)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 21:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.734 Acc@5 97.716
[2024-07-29 21:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-29 21:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.73%
[2024-07-29 21:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 21:49:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 21:49:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:24:44 lr 0.000018	 wd 0.0500	time 10.6651 (10.6651)	loss 1.6402 (1.6402)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:49:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:16:30 lr 0.000018	 wd 0.0500	time 0.3589 (0.4123)	loss 0.9022 (1.1919)	grad_norm 0.3549 (0.3280)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:50:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:13:54 lr 0.000018	 wd 0.0500	time 0.3150 (0.3625)	loss 1.4494 (1.1869)	grad_norm 0.3075 (0.3289)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:50:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:12:38 lr 0.000018	 wd 0.0500	time 0.3009 (0.3443)	loss 1.3318 (1.1818)	grad_norm 0.3410 (0.3284)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:51:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:11:45 lr 0.000018	 wd 0.0500	time 0.3174 (0.3357)	loss 0.9207 (1.1855)	grad_norm 0.3328 (0.3297)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:51:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:04 lr 0.000018	 wd 0.0500	time 0.2826 (0.3319)	loss 0.7177 (1.1810)	grad_norm 0.3151 (0.3300)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:52:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:10:24 lr 0.000018	 wd 0.0500	time 0.3271 (0.3283)	loss 1.1702 (1.1890)	grad_norm 0.3160 (0.3294)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:52:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:09:48 lr 0.000018	 wd 0.0500	time 0.3457 (0.3264)	loss 1.3187 (1.1870)	grad_norm 0.3003 (0.3291)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:53:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:13 lr 0.000018	 wd 0.0500	time 0.3279 (0.3250)	loss 1.2842 (1.1895)	grad_norm 0.3286 (0.3293)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:54:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:08:38 lr 0.000018	 wd 0.0500	time 0.2918 (0.3239)	loss 1.4708 (1.1871)	grad_norm 0.3171 (0.3291)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:54:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:05 lr 0.000018	 wd 0.0500	time 0.3381 (0.3234)	loss 1.2811 (1.1880)	grad_norm 0.3201 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:55:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:32 lr 0.000018	 wd 0.0500	time 0.3059 (0.3228)	loss 1.0130 (1.1939)	grad_norm 0.3420 (0.3339)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:55:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:06:59 lr 0.000018	 wd 0.0500	time 0.2980 (0.3218)	loss 1.4087 (1.1913)	grad_norm 0.3338 (0.3364)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:56:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:26 lr 0.000018	 wd 0.0500	time 0.3016 (0.3214)	loss 1.3869 (1.1888)	grad_norm 0.3205 (0.3357)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:56:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:05:53 lr 0.000018	 wd 0.0500	time 0.3284 (0.3209)	loss 1.3442 (1.1866)	grad_norm 0.3189 (0.3357)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:21 lr 0.000018	 wd 0.0500	time 0.2952 (0.3205)	loss 1.1868 (1.1842)	grad_norm 0.3367 (0.3351)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:57:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:04:48 lr 0.000018	 wd 0.0500	time 0.3401 (0.3202)	loss 1.1163 (1.1829)	grad_norm 0.3080 (0.3350)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:58:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:16 lr 0.000018	 wd 0.0500	time 0.3161 (0.3202)	loss 1.2477 (1.1848)	grad_norm 0.3249 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:58:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:44 lr 0.000018	 wd 0.0500	time 0.3061 (0.3200)	loss 1.1449 (1.1838)	grad_norm 0.3525 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:59:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:12 lr 0.000018	 wd 0.0500	time 0.3104 (0.3202)	loss 1.0921 (1.1827)	grad_norm 0.3204 (0.3349)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 21:59:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:40 lr 0.000017	 wd 0.0500	time 0.2906 (0.3198)	loss 1.3535 (1.1862)	grad_norm 0.3289 (0.3347)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:00:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:08 lr 0.000017	 wd 0.0500	time 0.2939 (0.3197)	loss 1.2272 (1.1875)	grad_norm 0.3054 (0.3346)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:00:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:36 lr 0.000017	 wd 0.0500	time 0.2898 (0.3196)	loss 1.3377 (1.1886)	grad_norm 0.3523 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:01:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:04 lr 0.000017	 wd 0.0500	time 0.3457 (0.3195)	loss 1.2983 (1.1864)	grad_norm 0.3686 (0.3345)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:01:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:32 lr 0.000017	 wd 0.0500	time 0.2917 (0.3195)	loss 1.3803 (1.1854)	grad_norm 0.3143 (0.3348)	loss_scale 8192.0000 (4232.4765)	mem 16682MB
[2024-07-29 22:02:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2825 (0.3188)	loss 0.9084 (1.1849)	grad_norm 0.3221 (nan)	loss_scale 4096.0000 (4285.9784)	mem 16682MB
[2024-07-29 22:02:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 10 training takes 0:13:19
[2024-07-29 22:02:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_10.pth saving......
[2024-07-29 22:02:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_10.pth saved !!!
[2024-07-29 22:02:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.084 (12.084)	Loss 0.5039 (0.5039)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 16682MB
[2024-07-29 22:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.714 Acc@5 97.722
[2024-07-29 22:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-29 22:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.73%
[2024-07-29 22:03:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:03:53 lr 0.000017	 wd 0.0500	time 10.1654 (10.1654)	loss 0.9301 (0.9301)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:03:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:16:50 lr 0.000017	 wd 0.0500	time 0.3173 (0.4207)	loss 1.3987 (1.1688)	grad_norm 0.3317 (0.4078)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:04:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:05 lr 0.000017	 wd 0.0500	time 0.3269 (0.3673)	loss 1.3381 (1.1899)	grad_norm 0.3365 (0.3702)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:04:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:12:49 lr 0.000017	 wd 0.0500	time 0.3286 (0.3494)	loss 0.8759 (1.1802)	grad_norm 0.3164 (0.3554)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:11:56 lr 0.000017	 wd 0.0500	time 0.3058 (0.3407)	loss 1.4138 (1.1785)	grad_norm 0.3433 (0.3494)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:05:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:11 lr 0.000017	 wd 0.0500	time 0.2840 (0.3353)	loss 0.7370 (1.1779)	grad_norm 0.3304 (0.3460)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:10:30 lr 0.000017	 wd 0.0500	time 0.2889 (0.3316)	loss 1.4776 (1.1785)	grad_norm 0.3199 (0.3432)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:06:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:09:53 lr 0.000017	 wd 0.0500	time 0.2960 (0.3292)	loss 1.0139 (1.1740)	grad_norm 0.3723 (0.3451)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:07:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:17 lr 0.000017	 wd 0.0500	time 0.3048 (0.3273)	loss 1.4958 (1.1721)	grad_norm 0.3170 (0.3429)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:07:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:08:41 lr 0.000017	 wd 0.0500	time 0.2905 (0.3256)	loss 1.0207 (1.1750)	grad_norm 0.3395 (0.3411)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:08:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:08:10 lr 0.000017	 wd 0.0500	time 1.5804 (0.3266)	loss 1.1629 (1.1751)	grad_norm 0.3366 (0.3411)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:08:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:38 lr 0.000017	 wd 0.0500	time 0.3054 (0.3269)	loss 0.9160 (1.1735)	grad_norm 0.3362 (0.3400)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:09:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:04 lr 0.000017	 wd 0.0500	time 0.2830 (0.3261)	loss 0.9392 (1.1704)	grad_norm 0.3310 (0.3391)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:10:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:31 lr 0.000017	 wd 0.0500	time 0.2912 (0.3253)	loss 1.4068 (1.1709)	grad_norm 0.3435 (0.3385)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:10:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:05:57 lr 0.000017	 wd 0.0500	time 0.3236 (0.3248)	loss 0.8314 (1.1713)	grad_norm 0.3260 (0.3381)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:11:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:24 lr 0.000017	 wd 0.0500	time 0.2911 (0.3242)	loss 1.4136 (1.1720)	grad_norm 0.3320 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:11:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:04:51 lr 0.000017	 wd 0.0500	time 0.3188 (0.3237)	loss 1.2838 (1.1732)	grad_norm 0.3224 (0.3392)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:12:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:19 lr 0.000017	 wd 0.0500	time 0.3272 (0.3233)	loss 1.0959 (1.1722)	grad_norm 0.3221 (0.3384)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:12:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:46 lr 0.000017	 wd 0.0500	time 0.3449 (0.3232)	loss 0.8634 (1.1722)	grad_norm 0.3226 (0.3381)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:13:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:14 lr 0.000017	 wd 0.0500	time 0.2917 (0.3229)	loss 1.1480 (1.1713)	grad_norm 0.3338 (0.3377)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:13:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:41 lr 0.000017	 wd 0.0500	time 0.2830 (0.3226)	loss 1.0669 (1.1715)	grad_norm 0.3013 (0.3374)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:14:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:09 lr 0.000017	 wd 0.0500	time 0.2933 (0.3224)	loss 1.3729 (1.1707)	grad_norm 0.3398 (0.3368)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:14:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:37 lr 0.000017	 wd 0.0500	time 0.2890 (0.3221)	loss 0.9765 (1.1726)	grad_norm 0.3376 (0.3366)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:15:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:05 lr 0.000016	 wd 0.0500	time 0.2923 (0.3220)	loss 1.1732 (1.1707)	grad_norm 0.3426 (0.3363)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:15:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:32 lr 0.000016	 wd 0.0500	time 0.2891 (0.3217)	loss 1.3959 (1.1719)	grad_norm 0.3200 (0.3366)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:16:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2847 (0.3210)	loss 0.8678 (1.1701)	grad_norm 0.3129 (0.3363)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:16:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 11 training takes 0:13:25
[2024-07-29 22:16:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.876 (11.876)	Loss 0.5054 (0.5054)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 22:16:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.734 Acc@5 97.712
[2024-07-29 22:16:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-29 22:16:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.73%
[2024-07-29 22:16:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 22:16:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:17:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:40:50 lr 0.000016	 wd 0.0500	time 11.0512 (11.0512)	loss 0.9661 (0.9661)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:17:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:16:36 lr 0.000016	 wd 0.0500	time 0.3036 (0.4149)	loss 1.3104 (1.1731)	grad_norm 0.3184 (nan)	loss_scale 2048.0000 (3609.3465)	mem 16682MB
[2024-07-29 22:18:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:13:57 lr 0.000016	 wd 0.0500	time 0.2967 (0.3640)	loss 1.3206 (1.1703)	grad_norm 0.3244 (nan)	loss_scale 2048.0000 (2832.5572)	mem 16682MB
[2024-07-29 22:18:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:12:42 lr 0.000016	 wd 0.0500	time 0.2918 (0.3465)	loss 0.7601 (1.1826)	grad_norm 0.3733 (nan)	loss_scale 2048.0000 (2571.9070)	mem 16682MB
[2024-07-29 22:19:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:11:49 lr 0.000016	 wd 0.0500	time 0.3145 (0.3377)	loss 1.2384 (1.1745)	grad_norm 0.3181 (nan)	loss_scale 2048.0000 (2441.2569)	mem 16682MB
[2024-07-29 22:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:05 lr 0.000016	 wd 0.0500	time 0.3177 (0.3325)	loss 1.4569 (1.1743)	grad_norm 0.3210 (nan)	loss_scale 2048.0000 (2362.7625)	mem 16682MB
[2024-07-29 22:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:10:27 lr 0.000016	 wd 0.0500	time 0.2907 (0.3298)	loss 1.4297 (1.1729)	grad_norm 0.3341 (nan)	loss_scale 2048.0000 (2310.3894)	mem 16682MB
[2024-07-29 22:20:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:09:50 lr 0.000016	 wd 0.0500	time 0.2832 (0.3280)	loss 1.2358 (1.1712)	grad_norm 0.3281 (nan)	loss_scale 2048.0000 (2272.9586)	mem 16682MB
[2024-07-29 22:21:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:16 lr 0.000016	 wd 0.0500	time 0.2895 (0.3267)	loss 1.3759 (1.1722)	grad_norm 0.3284 (nan)	loss_scale 2048.0000 (2244.8739)	mem 16682MB
[2024-07-29 22:21:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:08:41 lr 0.000016	 wd 0.0500	time 0.3004 (0.3255)	loss 1.5498 (1.1697)	grad_norm 0.3257 (nan)	loss_scale 2048.0000 (2223.0233)	mem 16682MB
[2024-07-29 22:22:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:07 lr 0.000016	 wd 0.0500	time 0.3326 (0.3248)	loss 1.4041 (1.1717)	grad_norm 0.3350 (nan)	loss_scale 2048.0000 (2205.5385)	mem 16682MB
[2024-07-29 22:22:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:34 lr 0.000016	 wd 0.0500	time 0.3007 (0.3240)	loss 0.8153 (1.1732)	grad_norm 0.3223 (nan)	loss_scale 2048.0000 (2191.2298)	mem 16682MB
[2024-07-29 22:23:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:01 lr 0.000016	 wd 0.0500	time 0.3250 (0.3234)	loss 1.4475 (1.1713)	grad_norm 0.3283 (nan)	loss_scale 2048.0000 (2179.3039)	mem 16682MB
[2024-07-29 22:23:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:28 lr 0.000016	 wd 0.0500	time 0.2943 (0.3229)	loss 1.0555 (1.1723)	grad_norm 0.3099 (nan)	loss_scale 2048.0000 (2169.2114)	mem 16682MB
[2024-07-29 22:24:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:05:55 lr 0.000016	 wd 0.0500	time 0.3072 (0.3227)	loss 1.3433 (1.1720)	grad_norm 0.3175 (nan)	loss_scale 2048.0000 (2160.5596)	mem 16682MB
[2024-07-29 22:24:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:22 lr 0.000016	 wd 0.0500	time 0.2854 (0.3222)	loss 0.8201 (1.1692)	grad_norm 0.3470 (nan)	loss_scale 2048.0000 (2153.0606)	mem 16682MB
[2024-07-29 22:25:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:04:50 lr 0.000016	 wd 0.0500	time 0.3130 (0.3220)	loss 1.3775 (1.1659)	grad_norm 0.3089 (nan)	loss_scale 2048.0000 (2146.4984)	mem 16682MB
[2024-07-29 22:26:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:18 lr 0.000016	 wd 0.0500	time 0.2927 (0.3219)	loss 1.0761 (1.1681)	grad_norm 0.3345 (nan)	loss_scale 2048.0000 (2140.7078)	mem 16682MB
[2024-07-29 22:26:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:45 lr 0.000016	 wd 0.0500	time 0.3485 (0.3217)	loss 0.8236 (1.1666)	grad_norm 0.3277 (nan)	loss_scale 2048.0000 (2135.5602)	mem 16682MB
[2024-07-29 22:27:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:13 lr 0.000016	 wd 0.0500	time 0.2902 (0.3217)	loss 1.3310 (1.1673)	grad_norm 0.3328 (nan)	loss_scale 2048.0000 (2130.9542)	mem 16682MB
[2024-07-29 22:27:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:41 lr 0.000016	 wd 0.0500	time 0.3245 (0.3216)	loss 1.0454 (1.1681)	grad_norm 0.3241 (nan)	loss_scale 2048.0000 (2126.8086)	mem 16682MB
[2024-07-29 22:28:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:09 lr 0.000016	 wd 0.0500	time 0.2970 (0.3214)	loss 0.8293 (1.1674)	grad_norm 0.2980 (nan)	loss_scale 2048.0000 (2123.0576)	mem 16682MB
[2024-07-29 22:28:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:37 lr 0.000016	 wd 0.0500	time 0.2860 (0.3214)	loss 0.8170 (1.1688)	grad_norm 0.3394 (nan)	loss_scale 2048.0000 (2119.6474)	mem 16682MB
[2024-07-29 22:29:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:04 lr 0.000015	 wd 0.0500	time 0.3309 (0.3213)	loss 1.1981 (1.1694)	grad_norm 0.3190 (nan)	loss_scale 2048.0000 (2116.5337)	mem 16682MB
[2024-07-29 22:29:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:32 lr 0.000015	 wd 0.0500	time 0.2921 (0.3210)	loss 1.3646 (1.1696)	grad_norm 0.3124 (nan)	loss_scale 2048.0000 (2113.6793)	mem 16682MB
[2024-07-29 22:30:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.2852 (0.3202)	loss 0.9474 (1.1689)	grad_norm 0.3328 (nan)	loss_scale 2048.0000 (2111.0532)	mem 16682MB
[2024-07-29 22:30:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 12 training takes 0:13:23
[2024-07-29 22:30:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.218 (12.218)	Loss 0.5166 (0.5166)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 22:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.758 Acc@5 97.744
[2024-07-29 22:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-29 22:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.76%
[2024-07-29 22:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 22:30:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:30:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:30:36 lr 0.000015	 wd 0.0500	time 10.8061 (10.8061)	loss 1.3413 (1.3413)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:31:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:16:38 lr 0.000015	 wd 0.0500	time 0.2869 (0.4157)	loss 1.3699 (1.2334)	grad_norm 0.3271 (0.3310)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:31:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:13:55 lr 0.000015	 wd 0.0500	time 0.3270 (0.3631)	loss 1.3994 (1.2212)	grad_norm 0.3167 (0.3324)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:32:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:12:41 lr 0.000015	 wd 0.0500	time 0.3010 (0.3459)	loss 1.4048 (1.2214)	grad_norm 0.3252 (0.3320)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:33:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:11:49 lr 0.000015	 wd 0.0500	time 0.2936 (0.3374)	loss 1.4712 (1.2020)	grad_norm 0.3439 (0.3318)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:33:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:11:06 lr 0.000015	 wd 0.0500	time 0.3093 (0.3330)	loss 1.3040 (1.1951)	grad_norm 0.3466 (0.3337)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:34:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:10:28 lr 0.000015	 wd 0.0500	time 0.3275 (0.3305)	loss 1.2153 (1.1889)	grad_norm 0.3282 (0.3335)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:34:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:09:51 lr 0.000015	 wd 0.0500	time 0.2963 (0.3284)	loss 1.1837 (1.1798)	grad_norm 0.3429 (0.3333)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:35:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:16 lr 0.000015	 wd 0.0500	time 0.3275 (0.3271)	loss 1.2399 (1.1792)	grad_norm 0.3151 (0.3331)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:35:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:08:41 lr 0.000015	 wd 0.0500	time 0.3344 (0.3257)	loss 1.3399 (1.1797)	grad_norm 0.3370 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:36:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:08 lr 0.000015	 wd 0.0500	time 0.3086 (0.3251)	loss 1.3929 (1.1784)	grad_norm 0.3826 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:36:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:34 lr 0.000015	 wd 0.0500	time 0.2891 (0.3239)	loss 1.4108 (1.1743)	grad_norm 0.3318 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:37:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:01 lr 0.000015	 wd 0.0500	time 0.3303 (0.3237)	loss 1.2027 (1.1783)	grad_norm 0.3364 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:37:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:28 lr 0.000015	 wd 0.0500	time 0.2889 (0.3229)	loss 0.7329 (1.1767)	grad_norm 0.3501 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:38:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:05:54 lr 0.000015	 wd 0.0500	time 0.3038 (0.3220)	loss 1.3704 (1.1754)	grad_norm 0.3305 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:38:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:22 lr 0.000015	 wd 0.0500	time 0.3330 (0.3220)	loss 1.2294 (1.1759)	grad_norm 0.3326 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:39:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:04:50 lr 0.000015	 wd 0.0500	time 0.3268 (0.3220)	loss 0.9131 (1.1749)	grad_norm 0.3265 (0.3359)	loss_scale 4096.0000 (2081.2592)	mem 16682MB
[2024-07-29 22:39:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:17 lr 0.000015	 wd 0.0500	time 0.3173 (0.3216)	loss 1.0449 (1.1761)	grad_norm 0.3279 (0.3357)	loss_scale 4096.0000 (2199.7037)	mem 16682MB
[2024-07-29 22:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:45 lr 0.000015	 wd 0.0500	time 0.3367 (0.3213)	loss 1.2518 (1.1765)	grad_norm 0.3237 (0.3353)	loss_scale 4096.0000 (2304.9950)	mem 16682MB
[2024-07-29 22:40:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:13 lr 0.000015	 wd 0.0500	time 0.2942 (0.3209)	loss 1.0876 (1.1766)	grad_norm 0.3256 (0.3353)	loss_scale 4096.0000 (2399.2088)	mem 16682MB
[2024-07-29 22:41:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:40 lr 0.000015	 wd 0.0500	time 0.3009 (0.3207)	loss 1.4057 (1.1764)	grad_norm 0.3090 (0.3357)	loss_scale 4096.0000 (2484.0060)	mem 16682MB
[2024-07-29 22:41:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:08 lr 0.000014	 wd 0.0500	time 0.3265 (0.3205)	loss 1.3840 (1.1763)	grad_norm 0.3333 (0.3357)	loss_scale 4096.0000 (2560.7311)	mem 16682MB
[2024-07-29 22:42:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:36 lr 0.000014	 wd 0.0500	time 0.3429 (0.3204)	loss 1.2103 (1.1749)	grad_norm 0.3345 (0.3360)	loss_scale 4096.0000 (2630.4843)	mem 16682MB
[2024-07-29 22:43:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:04 lr 0.000014	 wd 0.0500	time 0.2932 (0.3204)	loss 1.3530 (1.1739)	grad_norm 0.3591 (0.3359)	loss_scale 4096.0000 (2694.1747)	mem 16682MB
[2024-07-29 22:43:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:32 lr 0.000014	 wd 0.0500	time 0.3171 (0.3205)	loss 0.9612 (1.1766)	grad_norm 0.3120 (0.3358)	loss_scale 4096.0000 (2752.5598)	mem 16682MB
[2024-07-29 22:44:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.3033 (0.3199)	loss 0.7917 (1.1753)	grad_norm 0.3292 (0.3356)	loss_scale 4096.0000 (2806.2759)	mem 16682MB
[2024-07-29 22:44:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 13 training takes 0:13:22
[2024-07-29 22:44:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.793 (11.793)	Loss 0.4912 (0.4912)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 22:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.820 Acc@5 97.788
[2024-07-29 22:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-29 22:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.82%
[2024-07-29 22:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 22:44:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:44:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:03:47 lr 0.000014	 wd 0.0500	time 10.1628 (10.1628)	loss 1.4328 (1.4328)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:45:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:16:54 lr 0.000014	 wd 0.0500	time 0.2892 (0.4223)	loss 1.2000 (1.1929)	grad_norm 0.3286 (0.3344)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:45:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:04 lr 0.000014	 wd 0.0500	time 0.2868 (0.3669)	loss 1.4634 (1.1911)	grad_norm 0.3153 (0.3319)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:46:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:12:43 lr 0.000014	 wd 0.0500	time 0.2890 (0.3469)	loss 0.9430 (1.1938)	grad_norm 0.3385 (0.3318)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:46:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:11:49 lr 0.000014	 wd 0.0500	time 0.3204 (0.3374)	loss 1.6217 (1.1929)	grad_norm 0.3760 (0.3397)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:04 lr 0.000014	 wd 0.0500	time 0.2927 (0.3320)	loss 0.9816 (1.1882)	grad_norm 0.3210 (0.3383)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:47:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:24 lr 0.000014	 wd 0.0500	time 0.3329 (0.3282)	loss 0.9719 (1.1872)	grad_norm 0.3389 (0.3371)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:48:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:09:47 lr 0.000014	 wd 0.0500	time 0.2975 (0.3261)	loss 1.1447 (1.1878)	grad_norm 0.3367 (0.3367)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:48:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:13 lr 0.000014	 wd 0.0500	time 0.3426 (0.3255)	loss 0.8199 (1.1818)	grad_norm 0.3226 (0.3360)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:08:40 lr 0.000014	 wd 0.0500	time 0.3059 (0.3248)	loss 0.8734 (1.1808)	grad_norm 0.3428 (0.3369)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:50:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:05 lr 0.000014	 wd 0.0500	time 0.3069 (0.3235)	loss 1.0060 (1.1781)	grad_norm 0.3346 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:50:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:31 lr 0.000014	 wd 0.0500	time 0.3255 (0.3223)	loss 1.3996 (1.1776)	grad_norm 0.3480 (0.3408)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:51:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:06:58 lr 0.000014	 wd 0.0500	time 0.3048 (0.3215)	loss 1.4847 (1.1772)	grad_norm 0.3377 (0.3406)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:51:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:25 lr 0.000014	 wd 0.0500	time 0.2947 (0.3207)	loss 1.2525 (1.1804)	grad_norm 0.3272 (0.3402)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:52:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:05:53 lr 0.000014	 wd 0.0500	time 0.3022 (0.3208)	loss 1.2546 (1.1800)	grad_norm 0.3291 (0.3401)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:52:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:21 lr 0.000014	 wd 0.0500	time 0.3273 (0.3206)	loss 1.4048 (1.1791)	grad_norm 0.3263 (0.3399)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:53:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:04:48 lr 0.000014	 wd 0.0500	time 0.3261 (0.3204)	loss 0.7829 (1.1779)	grad_norm 0.3415 (0.3418)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:53:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:16 lr 0.000014	 wd 0.0500	time 0.3196 (0.3203)	loss 0.9583 (1.1768)	grad_norm 0.3186 (0.3412)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:44 lr 0.000013	 wd 0.0500	time 0.3077 (0.3202)	loss 0.9109 (1.1773)	grad_norm 0.3383 (0.3410)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:54:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:12 lr 0.000013	 wd 0.0500	time 0.3268 (0.3203)	loss 1.1809 (1.1777)	grad_norm 0.3465 (0.3409)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:55:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:40 lr 0.000013	 wd 0.0500	time 0.3752 (0.3203)	loss 1.2287 (1.1770)	grad_norm 0.3285 (0.3404)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:55:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:08 lr 0.000013	 wd 0.0500	time 0.3214 (0.3202)	loss 1.3652 (1.1764)	grad_norm 0.3290 (0.3400)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:36 lr 0.000013	 wd 0.0500	time 0.2971 (0.3203)	loss 1.3227 (1.1755)	grad_norm 0.3332 (0.3398)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:56:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:04 lr 0.000013	 wd 0.0500	time 0.3297 (0.3203)	loss 0.8571 (1.1743)	grad_norm 0.3420 (0.3396)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 22:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:32 lr 0.000013	 wd 0.0500	time 0.3032 (0.3202)	loss 0.8747 (1.1735)	grad_norm 0.3497 (nan)	loss_scale 2048.0000 (4049.9392)	mem 16682MB
[2024-07-29 22:57:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2851 (0.3195)	loss 1.2630 (1.1724)	grad_norm 0.3270 (nan)	loss_scale 2048.0000 (3969.8936)	mem 16682MB
[2024-07-29 22:57:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 14 training takes 0:13:21
[2024-07-29 22:58:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.961 (11.961)	Loss 0.4819 (0.4819)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 22:58:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.840 Acc@5 97.794
[2024-07-29 22:58:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-29 22:58:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.84%
[2024-07-29 22:58:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 22:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:58:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:04:20 lr 0.000013	 wd 0.0500	time 11.6149 (11.6149)	loss 1.2736 (1.2736)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:59:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:17:03 lr 0.000013	 wd 0.0500	time 0.3061 (0.4260)	loss 0.7851 (1.1786)	grad_norm 0.3365 (0.3320)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 22:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:13 lr 0.000013	 wd 0.0500	time 0.3282 (0.3710)	loss 1.4654 (1.1645)	grad_norm 0.3363 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:00:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:12:53 lr 0.000013	 wd 0.0500	time 0.2864 (0.3515)	loss 0.9426 (1.1639)	grad_norm 0.3344 (nan)	loss_scale 1024.0000 (1973.1561)	mem 16682MB
[2024-07-29 23:00:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:12:02 lr 0.000013	 wd 0.0500	time 0.2985 (0.3436)	loss 0.8198 (1.1625)	grad_norm 0.3531 (nan)	loss_scale 1024.0000 (1736.4589)	mem 16682MB
[2024-07-29 23:01:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:11:16 lr 0.000013	 wd 0.0500	time 0.3386 (0.3380)	loss 0.9521 (1.1738)	grad_norm 0.3776 (nan)	loss_scale 1024.0000 (1594.2515)	mem 16682MB
[2024-07-29 23:01:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:10:35 lr 0.000013	 wd 0.0500	time 0.2981 (0.3343)	loss 1.4007 (1.1766)	grad_norm 0.3310 (nan)	loss_scale 1024.0000 (1499.3677)	mem 16682MB
[2024-07-29 23:02:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:09:57 lr 0.000013	 wd 0.0500	time 0.3305 (0.3315)	loss 1.2449 (1.1765)	grad_norm 0.3331 (nan)	loss_scale 1024.0000 (1431.5549)	mem 16682MB
[2024-07-29 23:02:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:21 lr 0.000013	 wd 0.0500	time 0.3198 (0.3298)	loss 1.4093 (1.1782)	grad_norm 0.3156 (nan)	loss_scale 1024.0000 (1380.6742)	mem 16682MB
[2024-07-29 23:03:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:08:45 lr 0.000013	 wd 0.0500	time 0.2892 (0.3283)	loss 1.2834 (1.1777)	grad_norm 0.3356 (nan)	loss_scale 1024.0000 (1341.0877)	mem 16682MB
[2024-07-29 23:03:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:08:11 lr 0.000013	 wd 0.0500	time 0.3031 (0.3269)	loss 0.8588 (1.1776)	grad_norm 0.3234 (nan)	loss_scale 1024.0000 (1309.4106)	mem 16682MB
[2024-07-29 23:04:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:37 lr 0.000013	 wd 0.0500	time 0.2971 (0.3261)	loss 0.8686 (1.1784)	grad_norm 0.3353 (nan)	loss_scale 1024.0000 (1283.4877)	mem 16682MB
[2024-07-29 23:04:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:07:03 lr 0.000013	 wd 0.0500	time 0.2929 (0.3254)	loss 0.8428 (1.1785)	grad_norm 0.3658 (nan)	loss_scale 1024.0000 (1261.8818)	mem 16682MB
[2024-07-29 23:05:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:30 lr 0.000013	 wd 0.0500	time 0.3193 (0.3247)	loss 0.8870 (1.1796)	grad_norm 0.3297 (nan)	loss_scale 1024.0000 (1243.5972)	mem 16682MB
[2024-07-29 23:06:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:05:57 lr 0.000012	 wd 0.0500	time 0.3151 (0.3240)	loss 0.9599 (1.1797)	grad_norm 0.3335 (nan)	loss_scale 1024.0000 (1227.9229)	mem 16682MB
[2024-07-29 23:06:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:24 lr 0.000012	 wd 0.0500	time 0.3109 (0.3235)	loss 1.0369 (1.1758)	grad_norm 0.3349 (nan)	loss_scale 1024.0000 (1214.3371)	mem 16682MB
[2024-07-29 23:07:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:04:51 lr 0.000012	 wd 0.0500	time 0.2951 (0.3232)	loss 1.0249 (1.1765)	grad_norm 0.3432 (nan)	loss_scale 1024.0000 (1202.4485)	mem 16682MB
[2024-07-29 23:07:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:18 lr 0.000012	 wd 0.0500	time 0.2947 (0.3228)	loss 0.8313 (1.1766)	grad_norm 0.3396 (nan)	loss_scale 1024.0000 (1191.9577)	mem 16682MB
[2024-07-29 23:08:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:46 lr 0.000012	 wd 0.0500	time 0.3194 (0.3226)	loss 1.6819 (1.1746)	grad_norm 0.3566 (nan)	loss_scale 1024.0000 (1182.6319)	mem 16682MB
[2024-07-29 23:08:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:13 lr 0.000012	 wd 0.0500	time 0.2981 (0.3222)	loss 1.0006 (1.1728)	grad_norm 0.3492 (nan)	loss_scale 1024.0000 (1174.2872)	mem 16682MB
[2024-07-29 23:09:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:41 lr 0.000012	 wd 0.0500	time 0.3005 (0.3220)	loss 0.8604 (1.1741)	grad_norm 0.3329 (nan)	loss_scale 1024.0000 (1166.7766)	mem 16682MB
[2024-07-29 23:09:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:09 lr 0.000012	 wd 0.0500	time 0.3286 (0.3217)	loss 0.8035 (1.1728)	grad_norm 0.3312 (nan)	loss_scale 1024.0000 (1159.9810)	mem 16682MB
[2024-07-29 23:10:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:37 lr 0.000012	 wd 0.0500	time 0.2853 (0.3217)	loss 1.0997 (1.1724)	grad_norm 0.3189 (nan)	loss_scale 1024.0000 (1153.8028)	mem 16682MB
[2024-07-29 23:10:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:04 lr 0.000012	 wd 0.0500	time 0.3160 (0.3216)	loss 1.0332 (1.1727)	grad_norm 0.3409 (nan)	loss_scale 1024.0000 (1148.1617)	mem 16682MB
[2024-07-29 23:11:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:32 lr 0.000012	 wd 0.0500	time 0.3111 (0.3213)	loss 1.4854 (1.1731)	grad_norm 0.3361 (nan)	loss_scale 1024.0000 (1142.9904)	mem 16682MB
[2024-07-29 23:11:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2840 (0.3207)	loss 1.5320 (1.1736)	grad_norm 0.3361 (nan)	loss_scale 1024.0000 (1138.2327)	mem 16682MB
[2024-07-29 23:11:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 15 training takes 0:13:24
[2024-07-29 23:12:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.590 (11.590)	Loss 0.5361 (0.5361)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 23:12:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.778 Acc@5 97.768
[2024-07-29 23:12:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-29 23:12:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.84%
[2024-07-29 23:12:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:37:10 lr 0.000012	 wd 0.0500	time 10.9635 (10.9635)	loss 1.2197 (1.2197)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:13:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:16:53 lr 0.000012	 wd 0.0500	time 0.2873 (0.4218)	loss 1.0619 (1.1849)	grad_norm 0.3359 (0.3421)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:13:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:14:00 lr 0.000012	 wd 0.0500	time 0.2976 (0.3652)	loss 1.2719 (1.1741)	grad_norm 0.3281 (0.3382)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:14:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:12:43 lr 0.000012	 wd 0.0500	time 0.3251 (0.3467)	loss 1.0172 (1.1671)	grad_norm 0.3248 (0.3443)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:14:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:11:51 lr 0.000012	 wd 0.0500	time 0.2869 (0.3383)	loss 0.9363 (1.1555)	grad_norm 0.3287 (0.3437)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:15:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:07 lr 0.000012	 wd 0.0500	time 0.3120 (0.3334)	loss 1.5463 (1.1584)	grad_norm 0.3418 (0.3421)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:15:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:10:27 lr 0.000012	 wd 0.0500	time 0.2909 (0.3301)	loss 1.4472 (1.1590)	grad_norm 0.3093 (0.3407)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:16:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:09:50 lr 0.000012	 wd 0.0500	time 0.3293 (0.3277)	loss 1.0802 (1.1614)	grad_norm 0.3405 (0.3403)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:16:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:14 lr 0.000012	 wd 0.0500	time 0.3481 (0.3259)	loss 1.3285 (1.1629)	grad_norm 0.3412 (0.3393)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:08:39 lr 0.000012	 wd 0.0500	time 0.3160 (0.3245)	loss 1.1499 (1.1639)	grad_norm 0.3267 (0.3386)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:17:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:08 lr 0.000011	 wd 0.0500	time 0.2948 (0.3250)	loss 1.3575 (1.1637)	grad_norm 0.3227 (0.3383)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:18:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:34 lr 0.000011	 wd 0.0500	time 0.3308 (0.3241)	loss 0.8493 (1.1678)	grad_norm 0.3406 (0.3381)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:18:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:01 lr 0.000011	 wd 0.0500	time 0.3111 (0.3235)	loss 0.8799 (1.1656)	grad_norm 0.3301 (0.3383)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:19:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:28 lr 0.000011	 wd 0.0500	time 0.3287 (0.3231)	loss 1.3147 (1.1685)	grad_norm 0.3281 (0.3381)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:19:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:05:55 lr 0.000011	 wd 0.0500	time 0.2777 (0.3227)	loss 1.2422 (1.1685)	grad_norm 0.3452 (0.3383)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:20:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:23 lr 0.000011	 wd 0.0500	time 0.3239 (0.3225)	loss 1.3005 (1.1709)	grad_norm 0.3233 (0.3382)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:20:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:04:50 lr 0.000011	 wd 0.0500	time 0.2971 (0.3223)	loss 1.1376 (1.1714)	grad_norm 0.4553 (0.3387)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:18 lr 0.000011	 wd 0.0500	time 0.2891 (0.3220)	loss 1.0888 (1.1705)	grad_norm 0.3382 (0.3386)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-29 23:21:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:45 lr 0.000011	 wd 0.0500	time 0.3422 (0.3219)	loss 0.8773 (1.1693)	grad_norm 0.3142 (0.3390)	loss_scale 2048.0000 (1037.6458)	mem 16682MB
[2024-07-29 23:22:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:13 lr 0.000011	 wd 0.0500	time 0.2853 (0.3216)	loss 1.0456 (1.1701)	grad_norm 0.3222 (0.3387)	loss_scale 2048.0000 (1090.7943)	mem 16682MB
[2024-07-29 23:23:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:41 lr 0.000011	 wd 0.0500	time 0.3064 (0.3215)	loss 1.3688 (1.1708)	grad_norm 0.3126 (0.3385)	loss_scale 2048.0000 (1138.6307)	mem 16682MB
[2024-07-29 23:23:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:09 lr 0.000011	 wd 0.0500	time 0.3145 (0.3214)	loss 1.3723 (1.1716)	grad_norm 0.3174 (0.3382)	loss_scale 2048.0000 (1181.9134)	mem 16682MB
[2024-07-29 23:24:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:37 lr 0.000011	 wd 0.0500	time 0.3274 (0.3213)	loss 1.0194 (1.1711)	grad_norm 0.3363 (0.3382)	loss_scale 2048.0000 (1221.2631)	mem 16682MB
[2024-07-29 23:24:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:04 lr 0.000011	 wd 0.0500	time 0.3051 (0.3211)	loss 0.8987 (1.1693)	grad_norm 0.3507 (0.3380)	loss_scale 2048.0000 (1257.1925)	mem 16682MB
[2024-07-29 23:25:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:32 lr 0.000011	 wd 0.0500	time 0.3197 (0.3210)	loss 1.2328 (1.1683)	grad_norm 0.3208 (0.3378)	loss_scale 2048.0000 (1290.1291)	mem 16682MB
[2024-07-29 23:25:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2849 (0.3204)	loss 0.8542 (1.1686)	grad_norm 0.3544 (0.3420)	loss_scale 2048.0000 (1320.4318)	mem 16682MB
[2024-07-29 23:25:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 16 training takes 0:13:24
[2024-07-29 23:25:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.986 (11.986)	Loss 0.5054 (0.5054)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 23:26:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.792 Acc@5 97.794
[2024-07-29 23:26:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-29 23:26:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.84%
[2024-07-29 23:26:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:13:47 lr 0.000011	 wd 0.0500	time 10.4027 (10.4027)	loss 1.4299 (1.4299)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:26:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:16:46 lr 0.000011	 wd 0.0500	time 0.3266 (0.4189)	loss 1.2430 (1.1518)	grad_norm 0.3215 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:27:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:01 lr 0.000011	 wd 0.0500	time 0.3217 (0.3657)	loss 1.2934 (1.1512)	grad_norm 0.3305 (0.3522)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:27:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:12:48 lr 0.000011	 wd 0.0500	time 0.3155 (0.3489)	loss 1.2548 (1.1668)	grad_norm 0.3376 (0.3496)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:28:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:11:55 lr 0.000011	 wd 0.0500	time 0.3315 (0.3404)	loss 1.4382 (1.1649)	grad_norm 0.3316 (0.3468)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:29:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:11 lr 0.000010	 wd 0.0500	time 0.2893 (0.3352)	loss 1.3709 (1.1692)	grad_norm 0.3408 (0.3448)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:29:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:30 lr 0.000010	 wd 0.0500	time 0.3118 (0.3315)	loss 0.8091 (1.1704)	grad_norm 0.3729 (0.3431)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:30:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:09:52 lr 0.000010	 wd 0.0500	time 0.2913 (0.3286)	loss 1.2959 (1.1682)	grad_norm 0.3400 (0.3431)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:30:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:16 lr 0.000010	 wd 0.0500	time 0.3289 (0.3268)	loss 1.2008 (1.1664)	grad_norm 0.3401 (0.3427)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:31:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:08:40 lr 0.000010	 wd 0.0500	time 0.2884 (0.3252)	loss 1.3077 (1.1682)	grad_norm 0.3384 (0.3424)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:31:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:07 lr 0.000010	 wd 0.0500	time 0.3014 (0.3244)	loss 1.0374 (1.1674)	grad_norm 0.3491 (0.3436)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:32:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:33 lr 0.000010	 wd 0.0500	time 0.2991 (0.3235)	loss 1.0438 (1.1720)	grad_norm 0.3363 (0.3435)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:32:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:00 lr 0.000010	 wd 0.0500	time 0.2996 (0.3230)	loss 0.7717 (1.1718)	grad_norm 0.3244 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:33:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:27 lr 0.000010	 wd 0.0500	time 0.3100 (0.3224)	loss 1.4302 (1.1690)	grad_norm 0.3441 (0.3425)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:33:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:54 lr 0.000010	 wd 0.0500	time 0.2909 (0.3220)	loss 1.3316 (1.1691)	grad_norm 0.3131 (0.3434)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:34:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:22 lr 0.000010	 wd 0.0500	time 0.2975 (0.3217)	loss 1.4661 (1.1696)	grad_norm 0.3391 (0.3440)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:34:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:49 lr 0.000010	 wd 0.0500	time 0.2862 (0.3214)	loss 1.3973 (1.1695)	grad_norm 0.3256 (0.3435)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:17 lr 0.000010	 wd 0.0500	time 0.3004 (0.3211)	loss 1.5423 (1.1713)	grad_norm 0.3212 (0.3433)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:35:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:45 lr 0.000010	 wd 0.0500	time 0.3237 (0.3208)	loss 0.9765 (1.1707)	grad_norm 0.3451 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:36:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:13 lr 0.000010	 wd 0.0500	time 0.2967 (0.3208)	loss 1.2581 (1.1697)	grad_norm 0.3265 (0.3423)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:36:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:41 lr 0.000010	 wd 0.0500	time 0.2989 (0.3209)	loss 1.2062 (1.1699)	grad_norm 0.3295 (0.3421)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:37:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:08 lr 0.000010	 wd 0.0500	time 0.3288 (0.3208)	loss 1.4817 (1.1698)	grad_norm 0.3532 (0.3427)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:37:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:36 lr 0.000010	 wd 0.0500	time 0.3054 (0.3206)	loss 1.3831 (1.1696)	grad_norm 0.3373 (0.3423)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:38:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:04 lr 0.000010	 wd 0.0500	time 0.3319 (0.3204)	loss 0.8163 (1.1690)	grad_norm 0.3364 (0.3422)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:39:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:32 lr 0.000010	 wd 0.0500	time 0.3322 (0.3203)	loss 0.9490 (1.1677)	grad_norm 0.3436 (0.3432)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:39:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2851 (0.3197)	loss 1.4626 (1.1674)	grad_norm 0.3227 (0.3432)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:39:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 17 training takes 0:13:22
[2024-07-29 23:39:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.184 (12.184)	Loss 0.4963 (0.4963)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 23:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.878 Acc@5 97.768
[2024-07-29 23:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-29 23:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.88%
[2024-07-29 23:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 23:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 23:40:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:35:17 lr 0.000009	 wd 0.0500	time 10.9183 (10.9183)	loss 1.4631 (1.4631)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:40:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:16:39 lr 0.000009	 wd 0.0500	time 0.2975 (0.4162)	loss 1.4962 (1.2020)	grad_norm 0.3406 (0.3372)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:41:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:13:59 lr 0.000009	 wd 0.0500	time 0.3231 (0.3649)	loss 1.0389 (1.1928)	grad_norm 0.3613 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:41:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:12:40 lr 0.000009	 wd 0.0500	time 0.3240 (0.3454)	loss 1.5222 (1.1921)	grad_norm 0.3459 (0.3373)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:42:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:11:50 lr 0.000009	 wd 0.0500	time 0.3017 (0.3378)	loss 0.7671 (1.1907)	grad_norm 0.3629 (0.3372)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:42:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:11:04 lr 0.000009	 wd 0.0500	time 0.3245 (0.3320)	loss 1.2908 (1.1825)	grad_norm 0.3118 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:43:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:10:26 lr 0.000009	 wd 0.0500	time 0.2901 (0.3295)	loss 1.5009 (1.1747)	grad_norm 0.6024 (0.3376)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:43:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:09:49 lr 0.000009	 wd 0.0500	time 0.2892 (0.3271)	loss 1.4710 (1.1797)	grad_norm 0.3437 (0.3415)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-29 23:44:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:16 lr 0.000009	 wd 0.0500	time 0.2949 (0.3267)	loss 1.3326 (1.1828)	grad_norm 0.3271 (0.3422)	loss_scale 4096.0000 (2119.5905)	mem 16682MB
[2024-07-29 23:44:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:08:40 lr 0.000009	 wd 0.0500	time 0.2909 (0.3250)	loss 0.9277 (1.1825)	grad_norm 0.3404 (0.3424)	loss_scale 4096.0000 (2338.9478)	mem 16682MB
[2024-07-29 23:45:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:07 lr 0.000009	 wd 0.0500	time 0.3162 (0.3248)	loss 1.4442 (1.1783)	grad_norm 0.3283 (0.3417)	loss_scale 4096.0000 (2514.4775)	mem 16682MB
[2024-07-29 23:45:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:34 lr 0.000009	 wd 0.0500	time 0.2992 (0.3241)	loss 1.0109 (1.1803)	grad_norm 0.3215 (0.3421)	loss_scale 4096.0000 (2658.1217)	mem 16682MB
[2024-07-29 23:46:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:00 lr 0.000009	 wd 0.0500	time 0.3294 (0.3233)	loss 1.0680 (1.1735)	grad_norm 0.3334 (0.3421)	loss_scale 4096.0000 (2777.8451)	mem 16682MB
[2024-07-29 23:47:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:28 lr 0.000009	 wd 0.0500	time 0.3276 (0.3230)	loss 1.4905 (1.1738)	grad_norm 0.3771 (0.3428)	loss_scale 4096.0000 (2879.1637)	mem 16682MB
[2024-07-29 23:47:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:05:55 lr 0.000009	 wd 0.0500	time 0.3300 (0.3223)	loss 1.1661 (1.1742)	grad_norm 0.3200 (0.3428)	loss_scale 4096.0000 (2966.0186)	mem 16682MB
[2024-07-29 23:48:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:22 lr 0.000009	 wd 0.0500	time 0.3421 (0.3218)	loss 1.3301 (1.1738)	grad_norm 0.3201 (0.3421)	loss_scale 4096.0000 (3041.3005)	mem 16682MB
[2024-07-29 23:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:04:50 lr 0.000009	 wd 0.0500	time 0.3369 (0.3219)	loss 1.4590 (1.1758)	grad_norm 0.3784 (0.3419)	loss_scale 4096.0000 (3107.1780)	mem 16682MB
[2024-07-29 23:49:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:17 lr 0.000009	 wd 0.0500	time 0.2998 (0.3216)	loss 1.3408 (1.1775)	grad_norm 0.3395 (0.3415)	loss_scale 4096.0000 (3165.3098)	mem 16682MB
[2024-07-29 23:49:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:45 lr 0.000009	 wd 0.0500	time 0.3634 (0.3211)	loss 0.7284 (1.1778)	grad_norm 0.3380 (0.3413)	loss_scale 4096.0000 (3216.9861)	mem 16682MB
[2024-07-29 23:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:13 lr 0.000009	 wd 0.0500	time 0.3090 (0.3209)	loss 1.4429 (1.1775)	grad_norm 0.3347 (0.3421)	loss_scale 4096.0000 (3263.2257)	mem 16682MB
[2024-07-29 23:50:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:41 lr 0.000008	 wd 0.0500	time 0.2932 (0.3211)	loss 1.0795 (1.1771)	grad_norm 0.3321 (0.3429)	loss_scale 4096.0000 (3304.8436)	mem 16682MB
[2024-07-29 23:51:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:09 lr 0.000008	 wd 0.0500	time 0.2862 (0.3209)	loss 1.3497 (1.1761)	grad_norm 0.3333 (0.3431)	loss_scale 4096.0000 (3342.4998)	mem 16682MB
[2024-07-29 23:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:36 lr 0.000008	 wd 0.0500	time 0.2928 (0.3206)	loss 1.0315 (1.1762)	grad_norm 0.3199 (0.3428)	loss_scale 4096.0000 (3376.7342)	mem 16682MB
[2024-07-29 23:52:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.2867 (0.3204)	loss 1.3295 (1.1768)	grad_norm 0.3198 (0.3427)	loss_scale 4096.0000 (3407.9930)	mem 16682MB
[2024-07-29 23:52:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.3211 (0.3203)	loss 1.4375 (1.1774)	grad_norm 0.3480 (0.3424)	loss_scale 4096.0000 (3436.6481)	mem 16682MB
[2024-07-29 23:53:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2855 (0.3198)	loss 1.2781 (1.1776)	grad_norm 0.3282 (0.3421)	loss_scale 4096.0000 (3463.0116)	mem 16682MB
[2024-07-29 23:53:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 18 training takes 0:13:22
[2024-07-29 23:53:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.387 (11.387)	Loss 0.5068 (0.5068)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-29 23:53:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.868 Acc@5 97.784
[2024-07-29 23:53:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-29 23:53:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.88%
[2024-07-29 23:54:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:09:46 lr 0.000008	 wd 0.0500	time 11.7454 (11.7454)	loss 0.8034 (0.8034)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:54:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:01 lr 0.000008	 wd 0.0500	time 0.2880 (0.4252)	loss 1.6113 (1.2240)	grad_norm 0.3255 (0.3356)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:55:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:04 lr 0.000008	 wd 0.0500	time 0.3117 (0.3668)	loss 1.1849 (1.1933)	grad_norm 0.3476 (0.3381)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:55:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:12:46 lr 0.000008	 wd 0.0500	time 0.3176 (0.3479)	loss 1.4111 (1.1798)	grad_norm 0.3378 (0.3374)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:56:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:11:51 lr 0.000008	 wd 0.0500	time 0.3315 (0.3385)	loss 0.9748 (1.1730)	grad_norm 0.3706 (0.3388)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:09 lr 0.000008	 wd 0.0500	time 0.3361 (0.3342)	loss 1.4644 (1.1772)	grad_norm 0.3354 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:57:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:31 lr 0.000008	 wd 0.0500	time 0.2906 (0.3322)	loss 0.8552 (1.1666)	grad_norm 0.3439 (0.3383)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:57:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:09:55 lr 0.000008	 wd 0.0500	time 0.3360 (0.3303)	loss 1.2484 (1.1648)	grad_norm 0.3209 (0.3384)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:58:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:19 lr 0.000008	 wd 0.0500	time 0.3164 (0.3290)	loss 1.5239 (1.1666)	grad_norm 0.3420 (0.3385)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:58:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:08:44 lr 0.000008	 wd 0.0500	time 0.3095 (0.3273)	loss 1.2954 (1.1671)	grad_norm 0.3516 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:59:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:09 lr 0.000008	 wd 0.0500	time 0.2801 (0.3258)	loss 1.4626 (1.1685)	grad_norm 0.3209 (0.3403)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-29 23:59:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:35 lr 0.000008	 wd 0.0500	time 0.2897 (0.3246)	loss 1.1923 (1.1675)	grad_norm 0.3366 (0.3401)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:00:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:01 lr 0.000008	 wd 0.0500	time 0.3145 (0.3239)	loss 1.3397 (1.1671)	grad_norm 0.3403 (0.3399)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:00:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:28 lr 0.000008	 wd 0.0500	time 0.2979 (0.3232)	loss 0.7811 (1.1660)	grad_norm 0.3475 (0.3396)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:01:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:05:55 lr 0.000008	 wd 0.0500	time 0.3236 (0.3229)	loss 0.8200 (1.1653)	grad_norm 0.3389 (0.3395)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:01:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:23 lr 0.000008	 wd 0.0500	time 0.3308 (0.3226)	loss 1.3698 (1.1665)	grad_norm 0.3203 (0.3395)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:50 lr 0.000007	 wd 0.0500	time 0.3046 (0.3225)	loss 1.4315 (1.1682)	grad_norm 0.3307 (0.3394)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:03:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:18 lr 0.000007	 wd 0.0500	time 0.3310 (0.3223)	loss 0.8993 (1.1688)	grad_norm 0.3440 (0.3393)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:03:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:46 lr 0.000007	 wd 0.0500	time 0.3079 (0.3223)	loss 1.3379 (1.1712)	grad_norm 0.3291 (0.3390)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:04:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:14 lr 0.000007	 wd 0.0500	time 0.2977 (0.3224)	loss 1.1120 (1.1702)	grad_norm 0.3606 (0.3391)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 00:04:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:41 lr 0.000007	 wd 0.0500	time 0.2932 (0.3223)	loss 1.4868 (1.1713)	grad_norm 0.3519 (nan)	loss_scale 2048.0000 (3999.7921)	mem 16682MB
[2024-07-30 00:05:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:09 lr 0.000007	 wd 0.0500	time 0.2945 (0.3223)	loss 1.5931 (1.1728)	grad_norm 0.5002 (nan)	loss_scale 2048.0000 (3906.8939)	mem 16682MB
[2024-07-30 00:05:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:37 lr 0.000007	 wd 0.0500	time 0.3244 (0.3221)	loss 1.1545 (1.1720)	grad_norm 0.3493 (nan)	loss_scale 2048.0000 (3822.4371)	mem 16682MB
[2024-07-30 00:06:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:05 lr 0.000007	 wd 0.0500	time 0.3319 (0.3219)	loss 1.0223 (1.1721)	grad_norm 0.3227 (nan)	loss_scale 2048.0000 (3745.3212)	mem 16682MB
[2024-07-30 00:06:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:32 lr 0.000007	 wd 0.0500	time 0.3106 (0.3217)	loss 1.1223 (1.1713)	grad_norm 0.3397 (nan)	loss_scale 2048.0000 (3674.6289)	mem 16682MB
[2024-07-30 00:07:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2972 (0.3209)	loss 0.8117 (1.1715)	grad_norm 1.6265 (nan)	loss_scale 2048.0000 (3609.5898)	mem 16682MB
[2024-07-30 00:07:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 19 training takes 0:13:25
[2024-07-30 00:07:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.041 (12.041)	Loss 0.4915 (0.4915)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-30 00:07:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.870 Acc@5 97.798
[2024-07-30 00:07:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 00:07:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.88%
[2024-07-30 00:07:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:26:46 lr 0.000007	 wd 0.0500	time 10.7139 (10.7139)	loss 1.5484 (1.5484)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:08:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:17:33 lr 0.000007	 wd 0.0500	time 0.3038 (0.4387)	loss 1.0736 (1.1811)	grad_norm 0.3407 (0.3408)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:09:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:42 lr 0.000007	 wd 0.0500	time 0.3276 (0.3833)	loss 1.3273 (1.1645)	grad_norm 0.3506 (0.3386)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:09:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:13:13 lr 0.000007	 wd 0.0500	time 0.3394 (0.3602)	loss 0.7072 (1.1770)	grad_norm 0.3542 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:10:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:12:15 lr 0.000007	 wd 0.0500	time 0.3341 (0.3497)	loss 1.3298 (1.1762)	grad_norm 0.3606 (0.3379)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:10:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:11:28 lr 0.000007	 wd 0.0500	time 0.3191 (0.3438)	loss 1.2876 (1.1742)	grad_norm 0.3360 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:11:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:10:45 lr 0.000007	 wd 0.0500	time 0.2850 (0.3394)	loss 1.2669 (1.1791)	grad_norm 0.4024 (0.3389)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:11:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:10:04 lr 0.000007	 wd 0.0500	time 0.2889 (0.3357)	loss 1.2830 (1.1773)	grad_norm 0.3317 (0.3387)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:12:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:26 lr 0.000007	 wd 0.0500	time 0.2964 (0.3330)	loss 1.1814 (1.1777)	grad_norm 0.3349 (0.3385)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:12:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:08:50 lr 0.000007	 wd 0.0500	time 0.2999 (0.3309)	loss 1.2708 (1.1771)	grad_norm 0.3407 (nan)	loss_scale 1024.0000 (1988.9012)	mem 16682MB
[2024-07-30 00:13:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:08:15 lr 0.000007	 wd 0.0500	time 0.2864 (0.3297)	loss 1.4115 (1.1790)	grad_norm 0.3667 (nan)	loss_scale 1024.0000 (1892.5075)	mem 16682MB
[2024-07-30 00:13:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:40 lr 0.000007	 wd 0.0500	time 0.3119 (0.3286)	loss 1.0895 (1.1769)	grad_norm 0.3438 (nan)	loss_scale 1024.0000 (1813.6240)	mem 16682MB
[2024-07-30 00:14:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:06 lr 0.000006	 wd 0.0500	time 0.3049 (0.3276)	loss 1.2296 (1.1729)	grad_norm 0.3252 (nan)	loss_scale 1024.0000 (1747.8768)	mem 16682MB
[2024-07-30 00:14:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:32 lr 0.000006	 wd 0.0500	time 0.3128 (0.3267)	loss 1.3489 (1.1723)	grad_norm 0.3317 (nan)	loss_scale 1024.0000 (1692.2367)	mem 16682MB
[2024-07-30 00:15:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:05:59 lr 0.000006	 wd 0.0500	time 0.3340 (0.3262)	loss 1.0785 (1.1737)	grad_norm 0.3539 (nan)	loss_scale 1024.0000 (1644.5396)	mem 16682MB
[2024-07-30 00:15:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:26 lr 0.000006	 wd 0.0500	time 0.2986 (0.3254)	loss 1.3327 (1.1764)	grad_norm 0.4046 (nan)	loss_scale 1024.0000 (1603.1979)	mem 16682MB
[2024-07-30 00:16:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:04:53 lr 0.000006	 wd 0.0500	time 0.3408 (0.3251)	loss 1.3094 (1.1785)	grad_norm 0.3340 (nan)	loss_scale 1024.0000 (1567.0206)	mem 16682MB
[2024-07-30 00:16:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:20 lr 0.000006	 wd 0.0500	time 0.2884 (0.3248)	loss 1.6184 (1.1798)	grad_norm 0.3198 (nan)	loss_scale 1024.0000 (1535.0970)	mem 16682MB
[2024-07-30 00:17:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:47 lr 0.000006	 wd 0.0500	time 0.3010 (0.3243)	loss 0.7687 (1.1775)	grad_norm 0.3312 (nan)	loss_scale 1024.0000 (1506.7185)	mem 16682MB
[2024-07-30 00:18:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:15 lr 0.000006	 wd 0.0500	time 0.3031 (0.3241)	loss 0.8416 (1.1787)	grad_norm 0.3315 (nan)	loss_scale 1024.0000 (1481.3256)	mem 16682MB
[2024-07-30 00:18:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:42 lr 0.000006	 wd 0.0500	time 0.2879 (0.3238)	loss 1.1003 (1.1778)	grad_norm 0.3542 (nan)	loss_scale 1024.0000 (1458.4708)	mem 16682MB
[2024-07-30 00:19:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:10 lr 0.000006	 wd 0.0500	time 0.2940 (0.3234)	loss 1.2600 (1.1778)	grad_norm 0.6296 (nan)	loss_scale 1024.0000 (1437.7915)	mem 16682MB
[2024-07-30 00:19:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:37 lr 0.000006	 wd 0.0500	time 0.3393 (0.3233)	loss 1.3952 (1.1789)	grad_norm 0.3402 (nan)	loss_scale 1024.0000 (1418.9914)	mem 16682MB
[2024-07-30 00:20:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:05 lr 0.000006	 wd 0.0500	time 0.3191 (0.3229)	loss 0.8757 (1.1800)	grad_norm 0.3372 (nan)	loss_scale 1024.0000 (1401.8253)	mem 16682MB
[2024-07-30 00:20:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:32 lr 0.000006	 wd 0.0500	time 0.2999 (0.3226)	loss 0.8013 (1.1794)	grad_norm 0.3471 (nan)	loss_scale 1024.0000 (1386.0891)	mem 16682MB
[2024-07-30 00:21:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.3024 (0.3218)	loss 1.0625 (1.1813)	grad_norm 0.3280 (nan)	loss_scale 1024.0000 (1371.6114)	mem 16682MB
[2024-07-30 00:21:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 20 training takes 0:13:27
[2024-07-30 00:21:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_20.pth saving......
[2024-07-30 00:21:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_20.pth saved !!!
[2024-07-30 00:21:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.184 (11.184)	Loss 0.5010 (0.5010)	Acc@1 92.188 (92.188)	Acc@5 98.047 (98.047)	Mem 16682MB
[2024-07-30 00:21:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.830 Acc@5 97.814
[2024-07-30 00:21:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-30 00:21:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.88%
[2024-07-30 00:21:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:55:04 lr 0.000006	 wd 0.0500	time 11.3928 (11.3928)	loss 1.0063 (1.0063)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:22:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:16:59 lr 0.000006	 wd 0.0500	time 0.3087 (0.4246)	loss 1.3753 (1.2019)	grad_norm 0.3495 (0.3536)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:14:03 lr 0.000006	 wd 0.0500	time 0.3011 (0.3664)	loss 1.3347 (1.1965)	grad_norm 0.3359 (0.3457)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:23:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:12:43 lr 0.000006	 wd 0.0500	time 0.2871 (0.3468)	loss 1.1957 (1.1857)	grad_norm 0.3526 (0.3466)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:23:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:11:51 lr 0.000006	 wd 0.0500	time 0.3096 (0.3386)	loss 1.3749 (1.1774)	grad_norm 0.3305 (0.3478)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:07 lr 0.000006	 wd 0.0500	time 0.3071 (0.3333)	loss 0.9165 (1.1724)	grad_norm 0.3322 (0.3452)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:25:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:28 lr 0.000006	 wd 0.0500	time 0.3201 (0.3304)	loss 1.3104 (1.1676)	grad_norm 0.3275 (0.3441)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:25:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:09:51 lr 0.000006	 wd 0.0500	time 0.2920 (0.3281)	loss 1.2537 (1.1653)	grad_norm 0.3295 (0.3437)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:26:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:15 lr 0.000006	 wd 0.0500	time 0.2915 (0.3261)	loss 1.2258 (1.1637)	grad_norm 0.3391 (0.3438)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:08:41 lr 0.000005	 wd 0.0500	time 0.3152 (0.3254)	loss 1.4510 (1.1669)	grad_norm 0.3429 (0.3430)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:07 lr 0.000005	 wd 0.0500	time 0.2984 (0.3247)	loss 1.5109 (1.1704)	grad_norm 0.3344 (0.3431)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:27:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:33 lr 0.000005	 wd 0.0500	time 0.3315 (0.3238)	loss 0.8139 (1.1736)	grad_norm 0.3278 (0.3432)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:28:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:01 lr 0.000005	 wd 0.0500	time 0.3043 (0.3239)	loss 1.4090 (1.1706)	grad_norm 0.3364 (0.3430)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:28:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:28 lr 0.000005	 wd 0.0500	time 0.3164 (0.3230)	loss 1.2126 (1.1710)	grad_norm 0.3319 (0.3425)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:29:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:05:55 lr 0.000005	 wd 0.0500	time 0.2867 (0.3223)	loss 0.8370 (1.1683)	grad_norm 0.3970 (0.3422)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:29:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:22 lr 0.000005	 wd 0.0500	time 0.3579 (0.3220)	loss 0.8515 (1.1700)	grad_norm 0.3361 (0.3423)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:30:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:04:50 lr 0.000005	 wd 0.0500	time 0.3110 (0.3217)	loss 1.0605 (1.1665)	grad_norm 0.3598 (0.3425)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:30:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:17 lr 0.000005	 wd 0.0500	time 0.2973 (0.3211)	loss 1.3608 (1.1673)	grad_norm 0.3192 (0.3430)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:31:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:45 lr 0.000005	 wd 0.0500	time 0.3091 (0.3211)	loss 0.7807 (1.1677)	grad_norm 0.3450 (0.3425)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:31:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:13 lr 0.000005	 wd 0.0500	time 0.3048 (0.3209)	loss 0.8548 (1.1684)	grad_norm 0.3325 (0.3423)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:32:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:41 lr 0.000005	 wd 0.0500	time 0.2909 (0.3209)	loss 1.0718 (1.1675)	grad_norm 0.3604 (0.3422)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:32:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:08 lr 0.000005	 wd 0.0500	time 0.3186 (0.3208)	loss 1.1558 (1.1672)	grad_norm 0.3347 (0.3418)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:33:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:36 lr 0.000005	 wd 0.0500	time 0.3146 (0.3207)	loss 1.4568 (1.1658)	grad_norm 0.4908 (0.3438)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:33:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:04 lr 0.000005	 wd 0.0500	time 0.3007 (0.3205)	loss 1.5552 (1.1674)	grad_norm 0.3631 (0.3437)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 00:34:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:32 lr 0.000005	 wd 0.0500	time 0.3115 (0.3209)	loss 1.2297 (1.1692)	grad_norm 0.3409 (0.3437)	loss_scale 2048.0000 (1047.0304)	mem 16682MB
[2024-07-30 00:35:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2847 (0.3202)	loss 1.5009 (1.1697)	grad_norm 0.3368 (0.3435)	loss_scale 2048.0000 (1087.0532)	mem 16682MB
[2024-07-30 00:35:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 21 training takes 0:13:23
[2024-07-30 00:35:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.918 (11.918)	Loss 0.5205 (0.5205)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-30 00:35:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.822 Acc@5 97.788
[2024-07-30 00:35:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-30 00:35:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.88%
[2024-07-30 00:35:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:13:27 lr 0.000005	 wd 0.0500	time 11.8335 (11.8335)	loss 0.9924 (0.9924)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:36:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:16:54 lr 0.000005	 wd 0.0500	time 0.3308 (0.4225)	loss 1.4835 (1.1853)	grad_norm 0.3564 (0.3672)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:36:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:13:59 lr 0.000005	 wd 0.0500	time 0.3172 (0.3645)	loss 0.7619 (1.1785)	grad_norm 0.3302 (0.3569)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:37:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:12:42 lr 0.000005	 wd 0.0500	time 0.2985 (0.3463)	loss 1.3811 (1.1841)	grad_norm 0.3358 (0.3520)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:37:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:11:59 lr 0.000005	 wd 0.0500	time 0.2909 (0.3422)	loss 0.7657 (1.1901)	grad_norm 0.3273 (0.3491)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:38:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:12 lr 0.000005	 wd 0.0500	time 0.3121 (0.3359)	loss 1.2073 (1.1874)	grad_norm 0.4142 (0.3472)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:38:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:10:32 lr 0.000005	 wd 0.0500	time 0.2900 (0.3325)	loss 0.8922 (1.1890)	grad_norm 0.3227 (0.3464)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:39:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:09:55 lr 0.000005	 wd 0.0500	time 0.3194 (0.3303)	loss 1.3631 (1.1836)	grad_norm 0.3459 (0.3451)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:39:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:18 lr 0.000004	 wd 0.0500	time 0.3226 (0.3283)	loss 0.9334 (1.1824)	grad_norm 0.4123 (0.3446)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:40:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:08:44 lr 0.000004	 wd 0.0500	time 0.2898 (0.3272)	loss 1.1203 (1.1812)	grad_norm 0.3449 (0.3460)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:41:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:09 lr 0.000004	 wd 0.0500	time 0.3220 (0.3262)	loss 1.2837 (1.1815)	grad_norm 0.3258 (0.3454)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:07:36 lr 0.000004	 wd 0.0500	time 0.3212 (0.3253)	loss 0.8178 (1.1788)	grad_norm 0.3345 (0.3499)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:42:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:02 lr 0.000004	 wd 0.0500	time 0.2944 (0.3243)	loss 1.3949 (1.1797)	grad_norm 0.3289 (0.3497)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:42:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:28 lr 0.000004	 wd 0.0500	time 0.3210 (0.3236)	loss 0.8595 (1.1734)	grad_norm 0.3187 (0.3496)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:43:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:05:56 lr 0.000004	 wd 0.0500	time 0.3005 (0.3234)	loss 1.0156 (1.1679)	grad_norm 0.3353 (0.3488)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:23 lr 0.000004	 wd 0.0500	time 0.3027 (0.3231)	loss 1.2173 (1.1674)	grad_norm 0.3127 (0.3481)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:44:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:04:51 lr 0.000004	 wd 0.0500	time 0.3060 (0.3226)	loss 1.5345 (1.1675)	grad_norm 0.3333 (0.3478)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:44:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:18 lr 0.000004	 wd 0.0500	time 0.3172 (0.3225)	loss 0.9137 (1.1657)	grad_norm 0.3390 (0.3472)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:45:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:46 lr 0.000004	 wd 0.0500	time 0.2968 (0.3223)	loss 1.4412 (1.1642)	grad_norm 0.3140 (0.3470)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:45:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:14 lr 0.000004	 wd 0.0500	time 0.3248 (0.3224)	loss 0.9502 (1.1651)	grad_norm 0.3284 (0.3472)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:46:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:41 lr 0.000004	 wd 0.0500	time 0.3060 (0.3224)	loss 0.9287 (1.1661)	grad_norm 0.3305 (0.3477)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:46:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:09 lr 0.000004	 wd 0.0500	time 0.2882 (0.3222)	loss 1.3058 (1.1676)	grad_norm 0.3272 (0.3476)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:37 lr 0.000004	 wd 0.0500	time 0.2956 (0.3220)	loss 1.1045 (1.1675)	grad_norm 0.3403 (0.3471)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:47:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:05 lr 0.000004	 wd 0.0500	time 0.3016 (0.3218)	loss 1.1447 (1.1676)	grad_norm 0.3219 (0.3468)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:48:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:32 lr 0.000004	 wd 0.0500	time 0.3203 (0.3216)	loss 1.3565 (1.1686)	grad_norm 0.3493 (0.3464)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:48:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2855 (0.3210)	loss 1.1196 (1.1682)	grad_norm 0.3284 (0.3462)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:49:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 22 training takes 0:13:25
[2024-07-30 00:49:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.695 (12.695)	Loss 0.5049 (0.5049)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-30 00:49:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.886 Acc@5 97.778
[2024-07-30 00:49:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 00:49:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.89%
[2024-07-30 00:49:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 00:49:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 00:49:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:29:00 lr 0.000004	 wd 0.0500	time 10.7674 (10.7674)	loss 0.7692 (0.7692)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:16:39 lr 0.000004	 wd 0.0500	time 0.2989 (0.4163)	loss 1.4866 (1.1576)	grad_norm 0.3546 (0.3445)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:50:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:13:57 lr 0.000004	 wd 0.0500	time 0.3353 (0.3636)	loss 0.8584 (1.1630)	grad_norm 0.3424 (0.3411)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:12:43 lr 0.000004	 wd 0.0500	time 0.2887 (0.3468)	loss 1.4249 (1.1539)	grad_norm 0.3425 (0.3412)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:51:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:11:56 lr 0.000004	 wd 0.0500	time 0.3485 (0.3408)	loss 0.9161 (1.1594)	grad_norm 0.3361 (0.3409)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:52:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:13 lr 0.000004	 wd 0.0500	time 0.3201 (0.3362)	loss 0.8238 (1.1626)	grad_norm 0.3397 (0.3405)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:52:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:10:32 lr 0.000004	 wd 0.0500	time 0.3000 (0.3327)	loss 1.4508 (1.1618)	grad_norm 0.3605 (0.3403)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:53:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:09:55 lr 0.000004	 wd 0.0500	time 0.3070 (0.3305)	loss 1.1384 (1.1651)	grad_norm 0.3664 (0.3402)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:53:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:19 lr 0.000003	 wd 0.0500	time 0.2942 (0.3288)	loss 1.4482 (1.1668)	grad_norm 0.3241 (0.3401)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:54:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:08:44 lr 0.000003	 wd 0.0500	time 0.3387 (0.3273)	loss 1.1458 (1.1673)	grad_norm 0.3370 (0.3397)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:54:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:09 lr 0.000003	 wd 0.0500	time 0.3405 (0.3259)	loss 1.1440 (1.1628)	grad_norm 0.3034 (0.3415)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:55:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:36 lr 0.000003	 wd 0.0500	time 0.3606 (0.3253)	loss 1.0446 (1.1656)	grad_norm 0.3466 (0.3422)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:56:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:02 lr 0.000003	 wd 0.0500	time 0.3140 (0.3243)	loss 1.4258 (1.1686)	grad_norm 0.3294 (0.3417)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:56:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:28 lr 0.000003	 wd 0.0500	time 0.3073 (0.3234)	loss 1.2677 (1.1693)	grad_norm 0.3230 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 00:57:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:05:55 lr 0.000003	 wd 0.0500	time 0.2982 (0.3229)	loss 1.2864 (1.1695)	grad_norm 0.3530 (0.3427)	loss_scale 4096.0000 (2132.7852)	mem 16682MB
[2024-07-30 00:57:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:23 lr 0.000003	 wd 0.0500	time 0.2971 (0.3224)	loss 1.3064 (1.1679)	grad_norm 0.3353 (0.3431)	loss_scale 4096.0000 (2263.5789)	mem 16682MB
[2024-07-30 00:58:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:04:50 lr 0.000003	 wd 0.0500	time 0.2996 (0.3219)	loss 1.1517 (1.1711)	grad_norm 0.3212 (0.3435)	loss_scale 4096.0000 (2378.0337)	mem 16682MB
[2024-07-30 00:58:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:18 lr 0.000003	 wd 0.0500	time 0.3387 (0.3220)	loss 0.7260 (1.1685)	grad_norm 0.3390 (0.3434)	loss_scale 4096.0000 (2479.0312)	mem 16682MB
[2024-07-30 00:59:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:45 lr 0.000003	 wd 0.0500	time 0.3369 (0.3219)	loss 1.3173 (1.1677)	grad_norm 0.3531 (0.3436)	loss_scale 4096.0000 (2568.8129)	mem 16682MB
[2024-07-30 00:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:13 lr 0.000003	 wd 0.0500	time 0.2948 (0.3217)	loss 1.3055 (1.1670)	grad_norm 0.3266 (0.3436)	loss_scale 4096.0000 (2649.1489)	mem 16682MB
[2024-07-30 01:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:41 lr 0.000003	 wd 0.0500	time 0.3337 (0.3215)	loss 0.9369 (1.1667)	grad_norm 0.3517 (0.3433)	loss_scale 4096.0000 (2721.4553)	mem 16682MB
[2024-07-30 01:00:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:09 lr 0.000003	 wd 0.0500	time 0.3162 (0.3214)	loss 1.3921 (1.1671)	grad_norm 0.3378 (0.3431)	loss_scale 4096.0000 (2786.8786)	mem 16682MB
[2024-07-30 01:01:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:37 lr 0.000003	 wd 0.0500	time 0.2936 (0.3213)	loss 1.3445 (1.1656)	grad_norm 0.3598 (0.3429)	loss_scale 4096.0000 (2846.3571)	mem 16682MB
[2024-07-30 01:01:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:04 lr 0.000003	 wd 0.0500	time 0.2930 (0.3213)	loss 1.3708 (1.1657)	grad_norm 0.3234 (0.3429)	loss_scale 4096.0000 (2900.6658)	mem 16682MB
[2024-07-30 01:02:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:32 lr 0.000003	 wd 0.0500	time 0.3129 (0.3212)	loss 0.8273 (1.1655)	grad_norm 0.3329 (0.3427)	loss_scale 4096.0000 (2950.4506)	mem 16682MB
[2024-07-30 01:02:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2853 (0.3204)	loss 0.8748 (1.1648)	grad_norm 0.3334 (0.3431)	loss_scale 4096.0000 (2996.2543)	mem 16682MB
[2024-07-30 01:02:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 23 training takes 0:13:24
[2024-07-30 01:03:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.277 (12.277)	Loss 0.5220 (0.5220)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-30 01:03:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.872 Acc@5 97.800
[2024-07-30 01:03:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 01:03:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.89%
[2024-07-30 01:03:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:24:34 lr 0.000003	 wd 0.0500	time 12.1000 (12.1000)	loss 1.2322 (1.2322)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:04:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:17:12 lr 0.000003	 wd 0.0500	time 0.2827 (0.4298)	loss 0.9534 (1.1720)	grad_norm 0.3222 (0.3534)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:04:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:14:44 lr 0.000003	 wd 0.0500	time 0.3360 (0.3842)	loss 1.2346 (1.1611)	grad_norm 0.3180 (0.3518)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:05:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:13:13 lr 0.000003	 wd 0.0500	time 0.3207 (0.3605)	loss 1.3948 (1.1755)	grad_norm 0.3267 (0.3482)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:05:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:12:14 lr 0.000003	 wd 0.0500	time 0.3218 (0.3494)	loss 0.7334 (1.1753)	grad_norm 0.3577 (0.3530)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:06:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:11:26 lr 0.000003	 wd 0.0500	time 0.3292 (0.3427)	loss 1.4388 (1.1755)	grad_norm 0.3556 (0.3501)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:06:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:10:43 lr 0.000003	 wd 0.0500	time 0.2865 (0.3386)	loss 0.7000 (1.1704)	grad_norm 0.3295 (0.3480)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:07:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:10:04 lr 0.000003	 wd 0.0500	time 0.3077 (0.3355)	loss 1.2068 (1.1688)	grad_norm 0.3290 (0.3471)	loss_scale 4096.0000 (4096.0000)	mem 16682MB
[2024-07-30 01:07:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:26 lr 0.000003	 wd 0.0500	time 0.2966 (0.3330)	loss 1.2107 (1.1709)	grad_norm 0.3414 (nan)	loss_scale 2048.0000 (3860.7740)	mem 16682MB
[2024-07-30 01:08:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:08:50 lr 0.000003	 wd 0.0500	time 0.2955 (0.3311)	loss 0.9653 (1.1746)	grad_norm 0.3251 (nan)	loss_scale 2048.0000 (3659.5782)	mem 16682MB
[2024-07-30 01:08:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:14 lr 0.000003	 wd 0.0500	time 0.3151 (0.3292)	loss 0.8091 (1.1717)	grad_norm 0.3388 (nan)	loss_scale 2048.0000 (3498.5814)	mem 16682MB
[2024-07-30 01:09:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:07:39 lr 0.000003	 wd 0.0500	time 0.3184 (0.3280)	loss 0.8629 (1.1710)	grad_norm 0.3456 (nan)	loss_scale 2048.0000 (3366.8302)	mem 16682MB
[2024-07-30 01:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:05 lr 0.000002	 wd 0.0500	time 0.2956 (0.3269)	loss 0.9117 (1.1690)	grad_norm 0.3473 (nan)	loss_scale 2048.0000 (3257.0192)	mem 16682MB
[2024-07-30 01:10:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:32 lr 0.000002	 wd 0.0500	time 0.3368 (0.3264)	loss 0.7809 (1.1660)	grad_norm 0.3503 (nan)	loss_scale 2048.0000 (3164.0892)	mem 16682MB
[2024-07-30 01:11:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:05:59 lr 0.000002	 wd 0.0500	time 0.2982 (0.3259)	loss 1.4597 (1.1670)	grad_norm 0.3279 (nan)	loss_scale 2048.0000 (3084.4254)	mem 16682MB
[2024-07-30 01:11:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:26 lr 0.000002	 wd 0.0500	time 0.3012 (0.3254)	loss 1.2569 (1.1632)	grad_norm 0.3559 (nan)	loss_scale 2048.0000 (3015.3764)	mem 16682MB
[2024-07-30 01:12:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:04:53 lr 0.000002	 wd 0.0500	time 0.2923 (0.3251)	loss 1.2995 (1.1621)	grad_norm 0.3416 (nan)	loss_scale 2048.0000 (2954.9532)	mem 16682MB
[2024-07-30 01:12:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:20 lr 0.000002	 wd 0.0500	time 0.3162 (0.3248)	loss 1.5204 (1.1636)	grad_norm 0.3421 (nan)	loss_scale 2048.0000 (2901.6343)	mem 16682MB
[2024-07-30 01:13:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:47 lr 0.000002	 wd 0.0500	time 0.2914 (0.3247)	loss 1.0971 (1.1635)	grad_norm 0.3373 (nan)	loss_scale 2048.0000 (2854.2365)	mem 16682MB
[2024-07-30 01:13:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:15 lr 0.000002	 wd 0.0500	time 0.3296 (0.3244)	loss 0.9178 (1.1647)	grad_norm 0.3244 (nan)	loss_scale 2048.0000 (2811.8254)	mem 16682MB
[2024-07-30 01:14:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:42 lr 0.000002	 wd 0.0500	time 0.2913 (0.3244)	loss 1.2899 (1.1647)	grad_norm 0.3648 (nan)	loss_scale 2048.0000 (2773.6532)	mem 16682MB
[2024-07-30 01:14:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:10 lr 0.000002	 wd 0.0500	time 0.3067 (0.3239)	loss 1.0133 (1.1650)	grad_norm 0.3399 (nan)	loss_scale 2048.0000 (2739.1147)	mem 16682MB
[2024-07-30 01:15:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:37 lr 0.000002	 wd 0.0500	time 0.2870 (0.3235)	loss 1.0209 (1.1666)	grad_norm 0.3674 (nan)	loss_scale 2048.0000 (2707.7147)	mem 16682MB
[2024-07-30 01:15:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:05 lr 0.000002	 wd 0.0500	time 0.3115 (0.3234)	loss 1.3842 (1.1689)	grad_norm 0.3677 (nan)	loss_scale 2048.0000 (2679.0439)	mem 16682MB
[2024-07-30 01:16:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:32 lr 0.000002	 wd 0.0500	time 0.3022 (0.3234)	loss 1.2341 (1.1668)	grad_norm 0.3209 (nan)	loss_scale 2048.0000 (2652.7613)	mem 16682MB
[2024-07-30 01:16:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2845 (0.3226)	loss 0.9547 (1.1673)	grad_norm 0.3218 (nan)	loss_scale 2048.0000 (2628.5806)	mem 16682MB
[2024-07-30 01:16:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 24 training takes 0:13:31
[2024-07-30 01:17:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.926 (11.926)	Loss 0.5020 (0.5020)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-30 01:17:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.902 Acc@5 97.798
[2024-07-30 01:17:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 01:17:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.90%
[2024-07-30 01:17:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 01:17:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 01:17:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:16:10 lr 0.000002	 wd 0.0500	time 11.8986 (11.8986)	loss 1.2880 (1.2880)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:16:56 lr 0.000002	 wd 0.0500	time 0.3192 (0.4232)	loss 1.2075 (1.2264)	grad_norm 0.3320 (0.3654)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:18:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:14:01 lr 0.000002	 wd 0.0500	time 0.3189 (0.3657)	loss 1.4426 (1.1951)	grad_norm 0.3361 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:19:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:12:47 lr 0.000002	 wd 0.0500	time 0.3269 (0.3483)	loss 1.1472 (1.1785)	grad_norm 0.3351 (0.3487)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:19:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:11:53 lr 0.000002	 wd 0.0500	time 0.3146 (0.3395)	loss 1.3174 (1.1794)	grad_norm 0.3209 (0.3474)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:20:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:11:08 lr 0.000002	 wd 0.0500	time 0.3461 (0.3337)	loss 1.4254 (1.1775)	grad_norm 0.3459 (0.3456)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:20:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:10:30 lr 0.000002	 wd 0.0500	time 0.3204 (0.3316)	loss 0.8304 (1.1675)	grad_norm 0.3366 (0.3443)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:21:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:09:55 lr 0.000002	 wd 0.0500	time 0.3286 (0.3304)	loss 0.8037 (1.1692)	grad_norm 0.3468 (0.3441)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:21:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:17 lr 0.000002	 wd 0.0500	time 0.2859 (0.3276)	loss 1.3728 (1.1681)	grad_norm 0.3669 (0.3439)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:22:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:08:41 lr 0.000002	 wd 0.0500	time 0.3008 (0.3255)	loss 1.4563 (1.1679)	grad_norm 0.3313 (0.3432)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:22:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:07 lr 0.000002	 wd 0.0500	time 0.3344 (0.3244)	loss 1.5212 (1.1693)	grad_norm 0.3378 (0.3428)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:23:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:07:34 lr 0.000002	 wd 0.0500	time 0.3135 (0.3239)	loss 1.5576 (1.1701)	grad_norm 0.3436 (0.3427)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:23:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:00 lr 0.000002	 wd 0.0500	time 0.3256 (0.3232)	loss 0.9824 (1.1665)	grad_norm 0.3285 (0.3428)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:24:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:27 lr 0.000002	 wd 0.0500	time 0.3184 (0.3222)	loss 0.8352 (1.1654)	grad_norm 0.3536 (0.3431)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:25:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:05:55 lr 0.000002	 wd 0.0500	time 0.3387 (0.3223)	loss 0.8162 (1.1649)	grad_norm 0.3481 (0.3427)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:25:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:22 lr 0.000002	 wd 0.0500	time 0.3090 (0.3216)	loss 0.8672 (1.1651)	grad_norm 0.3456 (0.3431)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:26:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:04:49 lr 0.000002	 wd 0.0500	time 0.3094 (0.3214)	loss 0.8093 (1.1643)	grad_norm 0.3229 (0.3431)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:17 lr 0.000002	 wd 0.0500	time 0.3419 (0.3213)	loss 0.7578 (1.1620)	grad_norm 0.3420 (0.3433)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:27:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:45 lr 0.000002	 wd 0.0500	time 0.3189 (0.3210)	loss 0.8889 (1.1639)	grad_norm 0.3335 (0.3430)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:27:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:14 lr 0.000002	 wd 0.0500	time 0.3114 (0.3223)	loss 1.6686 (1.1663)	grad_norm 0.3290 (0.3430)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:28:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:41 lr 0.000002	 wd 0.0500	time 0.3162 (0.3221)	loss 0.9938 (1.1668)	grad_norm 0.3352 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 01:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:09 lr 0.000002	 wd 0.0500	time 0.2956 (0.3220)	loss 1.2888 (1.1680)	grad_norm 0.3367 (nan)	loss_scale 1024.0000 (2036.3027)	mem 16682MB
[2024-07-30 01:29:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:37 lr 0.000001	 wd 0.0500	time 0.3306 (0.3222)	loss 1.1166 (1.1683)	grad_norm 0.3287 (nan)	loss_scale 1024.0000 (1990.3099)	mem 16682MB
[2024-07-30 01:29:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:05 lr 0.000001	 wd 0.0500	time 0.2936 (0.3220)	loss 1.3510 (1.1677)	grad_norm 0.3362 (nan)	loss_scale 1024.0000 (1948.3146)	mem 16682MB
[2024-07-30 01:30:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:32 lr 0.000001	 wd 0.0500	time 0.3333 (0.3220)	loss 1.3726 (1.1668)	grad_norm 0.3124 (nan)	loss_scale 1024.0000 (1909.8176)	mem 16682MB
[2024-07-30 01:30:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2849 (0.3213)	loss 0.7525 (1.1656)	grad_norm 0.3744 (nan)	loss_scale 1024.0000 (1874.3990)	mem 16682MB
[2024-07-30 01:30:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 25 training takes 0:13:28
[2024-07-30 01:31:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.015 (12.015)	Loss 0.5283 (0.5283)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-30 01:31:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.890 Acc@5 97.804
[2024-07-30 01:31:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 01:31:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.90%
[2024-07-30 01:31:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:00:48 lr 0.000001	 wd 0.0500	time 11.5301 (11.5301)	loss 1.1837 (1.1837)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:32:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:16:59 lr 0.000001	 wd 0.0500	time 0.2872 (0.4246)	loss 1.3190 (1.1785)	grad_norm 0.5039 (0.3451)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:32:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:14:07 lr 0.000001	 wd 0.0500	time 0.2864 (0.3682)	loss 1.0941 (1.1777)	grad_norm 0.3569 (0.3434)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:33:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:12:48 lr 0.000001	 wd 0.0500	time 0.2941 (0.3491)	loss 0.9157 (1.1597)	grad_norm 0.3302 (0.3454)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:33:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:11:54 lr 0.000001	 wd 0.0500	time 0.2878 (0.3401)	loss 1.3643 (1.1581)	grad_norm 0.3475 (0.3447)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:34:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:10 lr 0.000001	 wd 0.0500	time 0.3617 (0.3351)	loss 1.3280 (1.1669)	grad_norm 0.3453 (0.3445)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:34:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:10:32 lr 0.000001	 wd 0.0500	time 0.2903 (0.3326)	loss 0.7955 (1.1709)	grad_norm 0.3422 (0.3437)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:35:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:09:55 lr 0.000001	 wd 0.0500	time 0.2885 (0.3303)	loss 1.4185 (1.1671)	grad_norm 0.3171 (0.3438)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:35:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:18 lr 0.000001	 wd 0.0500	time 0.2912 (0.3281)	loss 1.2571 (1.1710)	grad_norm 0.3465 (0.3438)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:36:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:08:43 lr 0.000001	 wd 0.0500	time 0.3002 (0.3266)	loss 1.2801 (1.1702)	grad_norm 0.3278 (0.3431)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:36:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:08 lr 0.000001	 wd 0.0500	time 0.2938 (0.3255)	loss 1.1179 (1.1690)	grad_norm 0.3284 (0.3429)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:37:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:35 lr 0.000001	 wd 0.0500	time 0.2949 (0.3250)	loss 0.9483 (1.1690)	grad_norm 0.3297 (0.3446)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:37:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:01 lr 0.000001	 wd 0.0500	time 0.3073 (0.3236)	loss 1.0884 (1.1707)	grad_norm 0.3359 (0.3447)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:38:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:28 lr 0.000001	 wd 0.0500	time 0.2913 (0.3229)	loss 1.2913 (1.1723)	grad_norm 0.3525 (0.3445)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:39:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:05:55 lr 0.000001	 wd 0.0500	time 0.3111 (0.3228)	loss 1.3912 (1.1733)	grad_norm 0.3363 (0.3441)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:39:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:23 lr 0.000001	 wd 0.0500	time 0.2873 (0.3226)	loss 1.4241 (1.1750)	grad_norm 0.3359 (0.3444)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:40:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:04:50 lr 0.000001	 wd 0.0500	time 0.2926 (0.3223)	loss 1.2122 (1.1724)	grad_norm 0.3314 (0.3450)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:40:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:18 lr 0.000001	 wd 0.0500	time 0.3037 (0.3221)	loss 1.3083 (1.1705)	grad_norm 0.3399 (0.3446)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:41:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:46 lr 0.000001	 wd 0.0500	time 0.2859 (0.3219)	loss 0.8962 (1.1705)	grad_norm 0.3519 (0.3452)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:41:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:13 lr 0.000001	 wd 0.0500	time 0.3179 (0.3219)	loss 1.3104 (1.1716)	grad_norm 0.3481 (0.3451)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:42:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:41 lr 0.000001	 wd 0.0500	time 0.2996 (0.3217)	loss 1.1886 (1.1700)	grad_norm 0.3374 (0.3448)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:42:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:09 lr 0.000001	 wd 0.0500	time 0.3010 (0.3216)	loss 1.3796 (1.1690)	grad_norm 0.3505 (0.3446)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:43:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:37 lr 0.000001	 wd 0.0500	time 0.3052 (0.3214)	loss 1.2924 (1.1674)	grad_norm 0.3357 (0.3445)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:43:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:04 lr 0.000001	 wd 0.0500	time 0.3339 (0.3214)	loss 1.2919 (1.1675)	grad_norm 0.3179 (0.3443)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:32 lr 0.000001	 wd 0.0500	time 0.3019 (0.3213)	loss 0.7233 (1.1680)	grad_norm 0.3584 (0.3442)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2844 (0.3206)	loss 0.9443 (1.1685)	grad_norm 0.3500 (0.3440)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:44:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 26 training takes 0:13:27
[2024-07-30 01:45:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 12.285 (12.285)	Loss 0.5098 (0.5098)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-30 01:45:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.866 Acc@5 97.796
[2024-07-30 01:45:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 01:45:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.90%
[2024-07-30 01:45:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:14:27 lr 0.000001	 wd 0.0500	time 10.4185 (10.4185)	loss 1.4200 (1.4200)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:46:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:16:43 lr 0.000001	 wd 0.0500	time 0.2832 (0.4176)	loss 1.2891 (1.1589)	grad_norm 0.3095 (0.3388)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:46:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:13:56 lr 0.000001	 wd 0.0500	time 0.3164 (0.3632)	loss 1.3919 (1.1713)	grad_norm 0.3500 (0.3459)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:47:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:12:39 lr 0.000001	 wd 0.0500	time 0.3274 (0.3450)	loss 1.3612 (1.1707)	grad_norm 0.3347 (0.3439)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:47:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:11:46 lr 0.000001	 wd 0.0500	time 0.2990 (0.3360)	loss 1.0042 (1.1672)	grad_norm 0.3445 (0.3425)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:48:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:04 lr 0.000001	 wd 0.0500	time 0.3026 (0.3317)	loss 0.6814 (1.1683)	grad_norm 0.3381 (0.3435)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:48:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:10:25 lr 0.000001	 wd 0.0500	time 0.3247 (0.3291)	loss 1.3261 (1.1681)	grad_norm 0.3422 (0.3426)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:49:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:09:48 lr 0.000001	 wd 0.0500	time 0.3219 (0.3266)	loss 0.9847 (1.1699)	grad_norm 0.3446 (0.3421)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:49:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:13 lr 0.000001	 wd 0.0500	time 0.3090 (0.3253)	loss 0.9387 (1.1716)	grad_norm 0.3463 (0.3428)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:50:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:08:39 lr 0.000001	 wd 0.0500	time 0.3037 (0.3240)	loss 1.2627 (1.1696)	grad_norm 0.3272 (0.3437)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:50:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:08 lr 0.000001	 wd 0.0500	time 0.3056 (0.3255)	loss 1.1324 (1.1662)	grad_norm 0.3040 (0.3435)	loss_scale 1024.0000 (1024.0000)	mem 16682MB
[2024-07-30 01:51:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:35 lr 0.000001	 wd 0.0500	time 0.2994 (0.3246)	loss 1.1542 (1.1683)	grad_norm 0.3341 (0.3433)	loss_scale 2048.0000 (1050.0418)	mem 16682MB
[2024-07-30 01:51:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:01 lr 0.000001	 wd 0.0500	time 0.3126 (0.3237)	loss 0.7245 (1.1689)	grad_norm 0.3282 (0.3432)	loss_scale 2048.0000 (1133.1357)	mem 16682MB
[2024-07-30 01:52:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:28 lr 0.000001	 wd 0.0500	time 0.3031 (0.3232)	loss 0.8089 (1.1657)	grad_norm 0.3397 (0.3435)	loss_scale 2048.0000 (1203.4558)	mem 16682MB
[2024-07-30 01:53:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:05:55 lr 0.000001	 wd 0.0500	time 0.3304 (0.3226)	loss 0.8321 (1.1669)	grad_norm 0.3234 (0.3433)	loss_scale 2048.0000 (1263.7373)	mem 16682MB
[2024-07-30 01:53:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:22 lr 0.000001	 wd 0.0500	time 0.3023 (0.3222)	loss 1.4705 (1.1678)	grad_norm 0.3300 (0.3432)	loss_scale 2048.0000 (1315.9867)	mem 16682MB
[2024-07-30 01:54:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:04:50 lr 0.000001	 wd 0.0500	time 0.2990 (0.3219)	loss 1.3258 (1.1696)	grad_norm 0.3344 (0.3435)	loss_scale 2048.0000 (1361.7089)	mem 16682MB
[2024-07-30 01:54:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:17 lr 0.000001	 wd 0.0500	time 0.2932 (0.3215)	loss 1.3591 (1.1712)	grad_norm 0.3391 (0.3441)	loss_scale 2048.0000 (1402.0553)	mem 16682MB
[2024-07-30 01:55:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:45 lr 0.000001	 wd 0.0500	time 0.3235 (0.3214)	loss 1.4180 (1.1708)	grad_norm 0.3302 (0.3438)	loss_scale 2048.0000 (1437.9212)	mem 16682MB
[2024-07-30 01:55:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:13 lr 0.000001	 wd 0.0500	time 0.2945 (0.3211)	loss 1.3852 (1.1717)	grad_norm 0.3326 (0.3436)	loss_scale 2048.0000 (1470.0137)	mem 16682MB
[2024-07-30 01:56:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:41 lr 0.000001	 wd 0.0500	time 0.3197 (0.3210)	loss 1.4220 (1.1724)	grad_norm 0.3239 (0.3432)	loss_scale 2048.0000 (1498.8986)	mem 16682MB
[2024-07-30 01:56:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:08 lr 0.000001	 wd 0.0500	time 0.3385 (0.3209)	loss 1.5171 (1.1715)	grad_norm 0.3451 (0.3432)	loss_scale 2048.0000 (1525.0338)	mem 16682MB
[2024-07-30 01:57:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:36 lr 0.000001	 wd 0.0500	time 0.2931 (0.3206)	loss 1.1935 (1.1728)	grad_norm 0.3472 (0.3451)	loss_scale 2048.0000 (1548.7942)	mem 16682MB
[2024-07-30 01:57:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:04 lr 0.000001	 wd 0.0500	time 0.3194 (0.3208)	loss 1.3131 (1.1726)	grad_norm 0.3371 (0.3453)	loss_scale 2048.0000 (1570.4894)	mem 16682MB
[2024-07-30 01:58:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:32 lr 0.000001	 wd 0.0500	time 0.3314 (0.3205)	loss 1.3602 (1.1743)	grad_norm 0.3561 (0.3451)	loss_scale 2048.0000 (1590.3773)	mem 16682MB
[2024-07-30 01:58:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2848 (0.3198)	loss 1.2851 (1.1734)	grad_norm 0.3441 (0.3449)	loss_scale 2048.0000 (1608.6749)	mem 16682MB
[2024-07-30 01:58:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 27 training takes 0:13:25
[2024-07-30 01:59:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.906 (11.906)	Loss 0.4971 (0.4971)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-30 01:59:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.902 Acc@5 97.804
[2024-07-30 01:59:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 01:59:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.90%
[2024-07-30 01:59:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:26:11 lr 0.000001	 wd 0.0500	time 10.6999 (10.6999)	loss 0.9605 (0.9605)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:00:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:16:47 lr 0.000000	 wd 0.0500	time 0.3101 (0.4195)	loss 0.9100 (1.2085)	grad_norm 0.3302 (0.3445)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:00:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:14:31 lr 0.000000	 wd 0.0500	time 0.3196 (0.3787)	loss 1.5474 (1.1952)	grad_norm 0.3276 (0.3406)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:01:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:13:02 lr 0.000000	 wd 0.0500	time 0.2927 (0.3554)	loss 1.4461 (1.1890)	grad_norm 0.3623 (0.3411)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:01:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:12:05 lr 0.000000	 wd 0.0500	time 0.2984 (0.3453)	loss 0.7418 (1.1805)	grad_norm 0.3228 (0.3404)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:02:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:16 lr 0.000000	 wd 0.0500	time 0.2990 (0.3380)	loss 0.8100 (1.1856)	grad_norm 0.3240 (0.3416)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:02:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:10:34 lr 0.000000	 wd 0.0500	time 0.3185 (0.3334)	loss 0.8918 (1.1782)	grad_norm 0.3521 (0.3408)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:03:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:09:57 lr 0.000000	 wd 0.0500	time 0.3240 (0.3318)	loss 1.3017 (1.1751)	grad_norm 0.3468 (0.3422)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:03:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:20 lr 0.000000	 wd 0.0500	time 0.3582 (0.3293)	loss 0.8511 (1.1754)	grad_norm 0.3789 (0.3417)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:04:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.2881 (0.3274)	loss 0.8614 (1.1720)	grad_norm 0.3505 (0.3414)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:04:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:11 lr 0.000000	 wd 0.0500	time 0.3117 (0.3273)	loss 1.0161 (1.1736)	grad_norm 0.3297 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:05:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:37 lr 0.000000	 wd 0.0500	time 0.2965 (0.3260)	loss 1.2763 (1.1738)	grad_norm 0.3398 (0.3430)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:05:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:02 lr 0.000000	 wd 0.0500	time 0.2867 (0.3248)	loss 1.4512 (1.1738)	grad_norm 0.3540 (0.3427)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:06:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:29 lr 0.000000	 wd 0.0500	time 0.2914 (0.3239)	loss 1.5265 (1.1752)	grad_norm 0.3439 (0.3423)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:06:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:05:56 lr 0.000000	 wd 0.0500	time 0.3379 (0.3233)	loss 0.9605 (1.1746)	grad_norm 0.3372 (0.3427)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:07:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:23 lr 0.000000	 wd 0.0500	time 0.2874 (0.3232)	loss 1.1896 (1.1730)	grad_norm 0.3320 (0.3425)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:08:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:04:51 lr 0.000000	 wd 0.0500	time 0.3078 (0.3227)	loss 1.4059 (1.1757)	grad_norm 0.3592 (0.3444)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:08:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:18 lr 0.000000	 wd 0.0500	time 0.3102 (0.3221)	loss 1.2513 (1.1752)	grad_norm 0.3418 (0.3441)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:09:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:46 lr 0.000000	 wd 0.0500	time 0.3104 (0.3219)	loss 0.7991 (1.1747)	grad_norm 0.3375 (0.3437)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:09:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:13 lr 0.000000	 wd 0.0500	time 0.3154 (0.3216)	loss 1.0997 (1.1723)	grad_norm 0.3380 (0.3433)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:10:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:41 lr 0.000000	 wd 0.0500	time 0.3304 (0.3215)	loss 1.1218 (1.1724)	grad_norm 0.3927 (0.3444)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:09 lr 0.000000	 wd 0.0500	time 0.3106 (0.3218)	loss 0.9785 (1.1723)	grad_norm 0.3413 (0.3446)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:37 lr 0.000000	 wd 0.0500	time 0.3277 (0.3220)	loss 1.2851 (1.1714)	grad_norm 0.3391 (0.3444)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:11:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:05 lr 0.000000	 wd 0.0500	time 0.3371 (0.3219)	loss 1.2503 (1.1714)	grad_norm 0.3404 (0.3454)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:12:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:32 lr 0.000000	 wd 0.0500	time 0.3011 (0.3217)	loss 1.5198 (1.1700)	grad_norm 0.3191 (0.3450)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:12:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.3087 (0.3209)	loss 1.0887 (1.1693)	grad_norm 0.3170 (0.3455)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:12:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 28 training takes 0:13:27
[2024-07-30 02:13:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.343 (11.343)	Loss 0.5054 (0.5054)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 16682MB
[2024-07-30 02:13:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.898 Acc@5 97.816
[2024-07-30 02:13:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 02:13:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.90%
[2024-07-30 02:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:50:26 lr 0.000000	 wd 0.0500	time 11.2817 (11.2817)	loss 1.2406 (1.2406)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 16682MB
[2024-07-30 02:14:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:16:48 lr 0.000000	 wd 0.0500	time 0.2923 (0.4200)	loss 1.4299 (1.1810)	grad_norm 0.3406 (0.3528)	loss_scale 4096.0000 (2696.8713)	mem 16682MB
[2024-07-30 02:14:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:13:58 lr 0.000000	 wd 0.0500	time 0.2952 (0.3643)	loss 0.8248 (1.1794)	grad_norm 0.3574 (0.3456)	loss_scale 4096.0000 (3392.9552)	mem 16682MB
[2024-07-30 02:15:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:12:40 lr 0.000000	 wd 0.0500	time 0.3153 (0.3455)	loss 0.8871 (1.1757)	grad_norm 0.3479 (0.3466)	loss_scale 4096.0000 (3626.5249)	mem 16682MB
[2024-07-30 02:15:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:59 lr 0.000000	 wd 0.0500	time 0.3223 (0.3421)	loss 1.1293 (1.1694)	grad_norm 0.3385 (0.3460)	loss_scale 4096.0000 (3743.6010)	mem 16682MB
[2024-07-30 02:16:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:11:12 lr 0.000000	 wd 0.0500	time 0.3068 (0.3361)	loss 1.3148 (1.1650)	grad_norm 0.3452 (0.3472)	loss_scale 4096.0000 (3813.9401)	mem 16682MB
[2024-07-30 02:16:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:10:30 lr 0.000000	 wd 0.0500	time 0.2999 (0.3315)	loss 0.7312 (1.1682)	grad_norm 0.3350 (0.3461)	loss_scale 4096.0000 (3860.8719)	mem 16682MB
[2024-07-30 02:17:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:09:52 lr 0.000000	 wd 0.0500	time 0.3231 (0.3285)	loss 1.1259 (1.1689)	grad_norm 0.3384 (0.3491)	loss_scale 4096.0000 (3894.4137)	mem 16682MB
[2024-07-30 02:17:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:15 lr 0.000000	 wd 0.0500	time 0.2948 (0.3264)	loss 0.7751 (1.1696)	grad_norm 0.3239 (0.3480)	loss_scale 4096.0000 (3919.5805)	mem 16682MB
[2024-07-30 02:18:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:08:40 lr 0.000000	 wd 0.0500	time 0.3111 (0.3248)	loss 1.4978 (1.1709)	grad_norm 0.3453 (0.3472)	loss_scale 4096.0000 (3939.1609)	mem 16682MB
[2024-07-30 02:18:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:06 lr 0.000000	 wd 0.0500	time 0.3455 (0.3240)	loss 1.4413 (1.1716)	grad_norm 0.3377 (0.3470)	loss_scale 4096.0000 (3954.8292)	mem 16682MB
[2024-07-30 02:19:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:33 lr 0.000000	 wd 0.0500	time 0.3129 (0.3237)	loss 1.2548 (1.1714)	grad_norm 0.3299 (0.3464)	loss_scale 4096.0000 (3967.6512)	mem 16682MB
[2024-07-30 02:19:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:00 lr 0.000000	 wd 0.0500	time 0.3015 (0.3229)	loss 1.0237 (1.1693)	grad_norm 0.3160 (0.3458)	loss_scale 4096.0000 (3978.3381)	mem 16682MB
[2024-07-30 02:20:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:27 lr 0.000000	 wd 0.0500	time 0.3343 (0.3225)	loss 1.1508 (1.1727)	grad_norm 0.3577 (0.3456)	loss_scale 4096.0000 (3987.3820)	mem 16682MB
[2024-07-30 02:20:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:05:54 lr 0.000000	 wd 0.0500	time 0.2945 (0.3220)	loss 1.1272 (1.1677)	grad_norm 0.3484 (0.3451)	loss_scale 4096.0000 (3995.1349)	mem 16682MB
[2024-07-30 02:21:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:21 lr 0.000000	 wd 0.0500	time 0.2925 (0.3213)	loss 1.0294 (1.1679)	grad_norm 0.3345 (0.3461)	loss_scale 4096.0000 (4001.8548)	mem 16682MB
[2024-07-30 02:21:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:04:49 lr 0.000000	 wd 0.0500	time 0.3014 (0.3209)	loss 0.8206 (1.1700)	grad_norm 0.3287 (0.3463)	loss_scale 4096.0000 (4007.7352)	mem 16682MB
[2024-07-30 02:22:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:17 lr 0.000000	 wd 0.0500	time 0.2990 (0.3207)	loss 1.3675 (1.1726)	grad_norm 0.3419 (0.3469)	loss_scale 4096.0000 (4012.9242)	mem 16682MB
[2024-07-30 02:23:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:44 lr 0.000000	 wd 0.0500	time 0.3355 (0.3205)	loss 1.1883 (1.1710)	grad_norm 0.3174 (0.3464)	loss_scale 4096.0000 (4017.5369)	mem 16682MB
[2024-07-30 02:23:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:12 lr 0.000000	 wd 0.0500	time 0.3325 (0.3205)	loss 0.7680 (1.1714)	grad_norm 0.3483 (0.3471)	loss_scale 4096.0000 (4021.6644)	mem 16682MB
[2024-07-30 02:24:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:40 lr 0.000000	 wd 0.0500	time 0.3173 (0.3204)	loss 0.9273 (1.1731)	grad_norm 0.3383 (0.3468)	loss_scale 4096.0000 (4025.3793)	mem 16682MB
[2024-07-30 02:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:08 lr 0.000000	 wd 0.0500	time 0.3227 (0.3203)	loss 0.7580 (1.1740)	grad_norm 0.3097 (0.3467)	loss_scale 4096.0000 (4028.7406)	mem 16682MB
[2024-07-30 02:25:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:36 lr 0.000000	 wd 0.0500	time 0.3093 (0.3203)	loss 0.8466 (1.1746)	grad_norm 0.3432 (0.3464)	loss_scale 4096.0000 (4031.7965)	mem 16682MB
[2024-07-30 02:25:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:04 lr 0.000000	 wd 0.0500	time 0.3197 (0.3203)	loss 0.8611 (1.1716)	grad_norm 0.3479 (0.3464)	loss_scale 4096.0000 (4034.5867)	mem 16682MB
[2024-07-30 02:26:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:32 lr 0.000000	 wd 0.0500	time 0.2966 (0.3203)	loss 1.2719 (1.1717)	grad_norm 0.3396 (0.3464)	loss_scale 4096.0000 (4037.1445)	mem 16682MB
[2024-07-30 02:26:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2850 (0.3196)	loss 1.3266 (1.1710)	grad_norm 0.3435 (0.3463)	loss_scale 4096.0000 (4039.4978)	mem 16682MB
[2024-07-30 02:26:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 249): INFO EPOCH 29 training takes 0:13:23
[2024-07-30 02:26:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_29.pth saving......
[2024-07-30 02:26:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_29.pth saved !!!
[2024-07-30 02:27:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 289): INFO Test: [0/98]	Time 11.984 (11.984)	Loss 0.4949 (0.4949)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 16682MB
[2024-07-30 02:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 296): INFO  * Acc@1 85.936 Acc@5 97.816
[2024-07-30 02:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-30 02:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 182): INFO Max accuracy: 85.94%
[2024-07-30 02:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 02:27:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 02:27:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0] (main.py 189): INFO Training time 6:57:08
