# Diffusion-Tuning
This is the code base for **`"Step-by-step Fine-Tuning Strategy for Deep Neural Networks"`**, and the article is submitted to **`IEEE Transactions on Neural Networks and Learning Systems`**.

## Abstract
As data size and computational power increase, the pre-training + fine-tuning paradigm emerges in deep learning, where fine-tuning determines the model's performance on a new target task. Currently, mainstream methods, such as full fine-tuning (FFT) or parameter-efficient fine-tuning (PEFT), uniformly optimize all training parameters within a specified epoch, collectively referred to as one-step tuning (OST). However, OST encounters the challenge of model performance failing to improve with increased training time, tending to saturate or even decline at some point. Consequently, we introduce the concept of step-by-step tuning (SST), which refers to the training method of fine-tuning a model by progressively optimizing the parameters of different parts of the model in a specific order or strategy, allowing the model to be better adapted to new tasks. Inspired by the physical diffusion process, we propose a novel SST method, diffusion tuning, which divides the neural network parameter space into multiple subspaces and progressively optimizes each using a staged training strategy. Experiments conducted on three visual models and two language models demonstrate that diffusion tuning effectively reduces the difficulty of finding the global optimum during training and enhances the baseline model's performance across various tasks with good generalization. Finally, we develop AgriMaTech-LLaMA, a large-scale language model for agricultural mechanization education based on diffusion tuning, and verified its practical value.

![cvresult](figures/fig5.png)
![nlpresult1](figures/fig6.png)
![nlpresult2](figures/fig7.png)
